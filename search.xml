<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[elasticsearch学习第一弹：集群搭建]]></title>
    <url>%2F2019%2F04%2F25%2Felasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%BC%B9%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[集群搭建下载资源包到官方网站 https://www.elastic.co/downloads/ 分别下载需要安装的组件 集群规划 主机名 ip 组件 节点类型 hadoop01 192.168.8.81 elasticsearch、kibana 主节点、非数据节点 hadoop02 192.168.8.82 elasticsearch 主节点、非数据节点 hadoop03 192.168.8.83 elasticsearch 非主节点、数据节点 配置文件elasticsearch.yml hadoop01123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: cluster## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-1node.master: truenode.data: false## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /home/hadoop/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /home/hadoop/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.8.81network.bind_host: 0.0.0.0## Set a custom port for HTTP:##http.port: 9200http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]##discovery.seed_hosts: ["host1", "host2"]#discovery.seed_hosts: ["192.168.8.81","192.168.8.82","192.168.8.83"]discovery.zen.ping.unicast.hosts: ["192.168.8.81", "192.168.8.82", "192.168.8.83"]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: ["node-1", "node-2"]#可能成为主的节点cluster.initial_master_nodes: ["node-1", "node-2"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3#当集群内达到3个时开始恢复数据（防止集群启动时部分节点自动恢复数据）gateway.recover_after_nodes: 3 ## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true hadoop04123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: cluster## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-2node.name: node-1node.master: truenode.data: false## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /home/hadoop/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /home/hadoop/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.8.82network.bind_host: 0.0.0.0## Set a custom port for HTTP:##http.port: 9200http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]##discovery.seed_hosts: ["host1", "host2"]#discovery.seed_hosts: ["192.168.8.81","192.168.8.82","192.168.8.83"]discovery.zen.ping.unicast.hosts: ["192.168.8.81", "192.168.8.82", "192.168.8.83"]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: ["node-1", "node-2"]#可能成为主的节点cluster.initial_master_nodes: ["node-1", "node-2"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3#当集群内达到3个时开始恢复数据（防止集群启动时部分节点自动恢复数据）gateway.recover_after_nodes: 3 ## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true hadoop03123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: cluster## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-3node.master: falsenode.data: true## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /home/hadoop/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /home/hadoop/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.8.83network.bind_host: 0.0.0.0## Set a custom port for HTTP:##http.port: 9200http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]##discovery.seed_hosts: ["host1", "host2"]#discovery.seed_hosts: ["192.168.8.81","192.168.8.82","192.168.8.83"]discovery.zen.ping.unicast.hosts: ["192.168.8.81", "192.168.8.82", "192.168.8.83"]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: ["node-1", "node-2"]#可能成为主的节点cluster.initial_master_nodes: ["node-1", "node-2"]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3#当集群内达到3个时开始恢复数据（防止集群启动时部分节点自动恢复数据）gateway.recover_after_nodes: 3 ## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true kibana配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# Kibana is served by a back end server. This setting specifies the port to use.#server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is 'localhost', which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: "hadoop01"# Enables you to specify a path to mount Kibana at if you are running behind a proxy.# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath# from requests it receives, and to prevent a deprecation warning at startup.# This setting cannot end in a slash.#server.basePath: ""# Specifies whether Kibana should rewrite requests that are prefixed with# `server.basePath` or require that they are rewritten by your reverse proxy.# This setting was effectively always `false` before Kibana 6.3 and will# default to `true` starting in Kibana 7.0.#server.rewriteBasePath: false# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server's name. This is used for display purposes.server.name: "kibana"# The URLs of the Elasticsearch instances to use for all your queries.elasticsearch.hosts: ["http://hadoop01:9200"]# When this setting's value is true Kibana uses the hostname specified in the server.host# setting. When the value of this setting is false, Kibana uses the hostname of the host# that connects to this Kibana instance.#elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations and# dashboards. Kibana creates a new index if the index doesn't already exist.#kibana.index: ".kibana"# The default application to load.#kibana.defaultAppId: "home"# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which# is proxied through the Kibana server.#elasticsearch.username: "user"#elasticsearch.password: "pass"# Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.# These settings enable SSL for outgoing requests from the Kibana server to the browser.#server.ssl.enabled: false#server.ssl.certificate: /path/to/your/server.crt#server.ssl.key: /path/to/your/server.key# Optional settings that provide the paths to the PEM-format SSL certificate and key files.# These files validate that your Elasticsearch backend uses the same key files.#elasticsearch.ssl.certificate: /path/to/your/client.crt#elasticsearch.ssl.key: /path/to/your/client.key# Optional setting that enables you to specify a path to the PEM file for the certificate# authority for your Elasticsearch instance.#elasticsearch.ssl.certificateAuthorities: [ "/path/to/your/CA.pem" ]# To disregard the validity of SSL certificates, change this setting's value to 'none'.#elasticsearch.ssl.verificationMode: full# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of# the elasticsearch.requestTimeout setting.#elasticsearch.pingTimeout: 1500# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value# must be a positive integer.#elasticsearch.requestTimeout: 30000# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side# headers, set this value to [] (an empty list).#elasticsearch.requestHeadersWhitelist: [ authorization ]# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.#elasticsearch.customHeaders: &#123;&#125;# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.#elasticsearch.shardTimeout: 30000# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.#elasticsearch.startupTimeout: 5000# Logs queries sent to Elasticsearch. Requires logging.verbose set to true.#elasticsearch.logQueries: false# Specifies the path where Kibana creates the process ID file.#pid.file: /var/run/kibana.pid# Enables you specify a file where Kibana stores log output.#logging.dest: stdout# Set the value of this setting to true to suppress all logging output.#logging.silent: false# Set the value of this setting to true to suppress all logging output other than error messages.#logging.quiet: false# Set the value of this setting to true to log all events, including system usage information# and all requests.#logging.verbose: false# Set the interval in milliseconds to sample system and process performance# metrics. Minimum is 100ms. Defaults to 5000.#ops.interval: 5000# Specifies locale to be used for all localizable strings, dates and number formats.#i18n.locale: "en" 启动分别进入每一台机器,运行下面命令，启动es集群：1bin/elasticsearch -d 启动kibana：1nohup ./kibana &amp; 测试在浏览器输入如下地址：1http://192.168.8.81:9200/_cluster/health?pretty=true 结果:1234567891011121314151617&#123; "cluster_name" : "cluster", "status" : "green", "timed_out" : false, "number_of_nodes" : 3, "number_of_data_nodes" : 2, "active_primary_shards" : 4, "active_shards" : 8, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 0, "delayed_unassigned_shards" : 0, "number_of_pending_tasks" : 0, "number_of_in_flight_fetch" : 0, "task_max_waiting_in_queue_millis" : 0, "active_shards_percent_as_number" : 100.0&#125; green表示集群正确启动完成。 在浏览器输入如下地址：1http://192.168.8.81:5601 在左边工具栏找到Dev Tools,在开发窗口输入一下命令,查看集群健康状态：1GET /_cat/health?v 结果12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1556175058 06:50:58 cluster green 3 2 8 4 0 0 0 0 - 100.0% 集群的健康状况green：每个索引的primary shard和replica shard都是active状态的。yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态 。red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了。]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive自定义UDTF函数]]></title>
    <url>%2F2019%2F04%2F24%2Fhive%20%E8%87%AA%E5%AE%9A%E4%B9%89UDTF%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[自定义UDTF函数概述用户自定义表生成函数（UDTF）接受零个或多个输入，然后产生多列或多行的输出。要实现UDTF，需要继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF，同时实现三个方法。 参考文章https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDFhttps://javinjunfeng.top/technicalstack/hive/77 案例原始数据和转换后的数据原始数据： sku,category_name,busi_time,shop_id,flag,sale_num,income 21A5+32B2,烟灶,2018-06-29 00:00:00,21,0,6,16056.0 转换后的数据： rela_sku,rule_id,sku,category_name,busi_time,shop_id,sale_num,income 21A5,0,21A5+32B2,烟机,2018-06-29 00:00:00,21,6,16056.0 32B2,1,21A5+32B2,灶具,2018-06-29 00:00:00,21,6,16056.0 创建自定义函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.xh.spark.sql.hive.udtf;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;import java.util.ArrayList;import java.util.List;public class JDQuotaReduction extends GenericUDTF &#123; @Override public StructObjectInspector initialize(StructObjectInspector args) throws UDFArgumentException &#123; ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;&gt;(); ArrayList&lt;ObjectInspector&gt; resType = new ArrayList&lt;&gt;(); fieldNames.add("rela_sku"); resType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add("rule_id"); resType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, resType); &#125; @Override public void process(Object[] record) throws HiveException &#123; List&lt;Object&gt; allQuota = (ArrayList&lt;Object&gt;) record[0]; List&lt;Object&gt; firstQuotaValueOrder = (ArrayList&lt;Object&gt;) allQuota.get(0); String[] sku = firstQuotaValueOrder.get(0).toString().split("\\+"); String cate = firstQuotaValueOrder.get(1).toString(); String flag = firstQuotaValueOrder.get(2).toString(); // 烟灶套餐 if (cate != null &amp;&amp; "烟灶".equals(cate)) &#123; Object[] result = new Object[2]; //第一件单品 result[0] = sku[0]; result[1] = "0"; forward(result); //第二件的单品 result[0] = sku[1]; result[1] = "1"; forward(result); return; &#125; // 嵌入式套餐 if (cate != null &amp;&amp; "嵌入式".equals(cate)) &#123; Object[] result = new Object[2]; // 第一件单品 result[0] = sku[0]; result[1] = "2"; forward(result); // 第二件及之后的单品 for (int i = 1; i &lt; sku.length; i++) &#123; result[0] = sku[i]; result[1] = "1"; forward(result); &#125; return; &#125; // 烟灶消套餐及大套系 if (cate != null &amp;&amp; ("烟灶消".equals(cate) || "大套系".equals(cate))) &#123; Object[] result = new Object[2]; // 第二件及之后的单品 for (int i = 1; i &lt; sku.length; i++) &#123; result[0] = sku[i]; result[1] = "1"; forward(result); &#125; if (flag != null &amp;&amp; "1".equals(flag)) &#123; // 第一件+第二件的组合是烟灶套餐 result[0] = sku[0] + "+" + sku[1]; result[1] = 3; forward(result); &#125; else &#123; result[0] = sku[0]; result[1] = "2"; forward(result); &#125; &#125; &#125; @Override public void close() throws HiveException &#123; &#125;&#125; 创建表 12345678910111213create table if not exists transct_quota(sku string,category_name string,busi_time string,shop_id string,flag string,sale_num int,income double)row format delimited fields terminated by ','stored as textfile; 插入数据1insert into transct_quota values ("21A5+32B2","烟灶","2018-06-29 00:00:00","21","0",6,16056.0); 在hive命令窗口添加函数jar包1add jar /home/hadoop/sparklearning-1.0-SNAPSHOT.jar; 在hive命令窗口创建临时函数1create temporary function combine_sku_reduction_rule as 'com.xh.spark.sql.hive.udtf.JDQuotaReduction'; 使用自定义函数1select c1,c2,jd_quota.* from transct_quota LATERAL VIEW combine_sku_reduction_rule(array(named_struct('sku', goods_sku_id, 'cate', category_name,'flag',flag))) sku_tab as c1,c2;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中调用scala]]></title>
    <url>%2F2019%2F04%2F20%2Fjava%E4%B8%AD%E8%B0%83%E7%94%A8scala%2F</url>
    <content type="text"><![CDATA[案例说明：主要参照spark源码 编写scala类及半生对象123456789101112131415161718192021222324package org.apache.spark.sql.typesimport org.apache.spark.annotation.Stable/** * The data type representing `NULL` values. Please use the singleton `DataTypes.NullType`. * * @since 1.3.0 */@Stableclass NullType private() extends DataType &#123; // The companion object and this class is separated so the companion object also subclasses // this type. Otherwise, the companion object would be of type "NullType$" in byte code. // Defined with a private constructor so the companion object is the only possible instantiation. override def defaultSize: Int = 1 private[spark] override def asNullable: NullType = this&#125;/** * @since 1.3.0 */@Stablecase object NullType extends NullType 在java中调用scala123456789101112131415161718package org.apache.spark.sql.types;import java.util.*;import org.apache.spark.annotation.Stable;/** * To get/create specific data type, users should use singleton objects and factory methods * provided by this class. * * @since 1.3.0 */@Stablepublic class DataTypes &#123; /** * Gets the NullType object. */ public static final DataType NullType = NullType$.MODULE$;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql DataSet算子实战]]></title>
    <url>%2F2019%2F04%2F19%2Fsparksql%20DataSet%E7%AE%97%E5%AD%90%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[基本环境scala版本：scala2.12.8jdk版本：1.8spark版本：2.4.1 准备数据sku_sale_amount.csv123456789101112131415161718192010001,36B0,烟机 ,2015,10010001,27A3,烟机 ,2016,10010001,27A3,烟机 ,2017,20010002,36B0,烟机 ,2015,10010002,36B0,烟机 ,2016,10010002,36B0,烟机 ,2017,20010002,58B5,灶具 ,2014,20010002,58B5,灶具 ,2015,20010002,58B5,灶具 ,2016,20010002,58B5,灶具 ,2017,20010003,64B8,洗碗机 ,2014,20010003,727T,智能消毒柜,2014,20010004,64B8,净水器 ,2014,15010004,64B8,净水器 ,2015,5010004,64B8,净水器 ,2016,5010004,45A8,净水器 ,2017,5010004,64B8,嵌入式蒸箱,2014,15010004,64B8,嵌入式蒸箱,2015,5010004,64B8,嵌入式蒸箱,2016,5010004,45A8,嵌入式蒸箱,2017,50 sku_product.csv1234567891011121310001,100010001,200010001,400010001,600010002,800010002,1000010002,900010002,600010003,500010004,400010004,300010004,4400010004,11000 准备基本环境123456789101112131415161718192021222324252627282930object DataSetSingleOperating &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[*]") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import org.apache.spark.sql.functions._ import spark.implicits._ val skuIncomeDF = spark.read.format("csv") .option("sep", ",") .option("inferSchema", "true") .option("header", "false") .load("src/main/resources/data/sku_sale_amount.csv") .toDF("goods_id", "sku", "category", "year", "amount") val skuProductDF = spark.read.format("csv") .option("sep", ",") .option("inferSchema", "true") .option("header", "false") .load("src/main/resources/data/sku_product.csv") .toDF("goods_id1", "volume") skuIncomeDF.createTempView("sku_sale_amount") skuIncomeDF.createTempView("sku_product") &#125;&#125; select 操作12345678skuIncomeDF.select($"goods_id", $"sku", $"category", $"year".as("date"), $"amount") skuIncomeDF.select('goods_id, 'sku, 'category, 'year.as("date"), 'amount) skuIncomeDF.select(col("goods_id"), col("sku"), col("category"), col("year").as("date"), 'amount) skuIncomeDF.select(expr("goods_id"), expr("sku"), expr("category"), expr("year as date"), expr("amount+1")) spark.sql( """ |select sku,category,year,amount from sku_sale_amount """.stripMargin) selectExpr操作1skuIncomeDF.selectExpr("goods_id", "sku", "category", "year as date", "amount+1 as amount") filter 操作12345skuIncomeDF.filter($"amount" &gt; 150)skuIncomeDF.filter(col("amount") &gt; 150)skuIncomeDF.filter('amount &gt; 150)skuIncomeDF.filter("amount &gt; 150")skuIncomeDF.filter(row =&gt; row.getInt(3) &gt; 150) where操作12345678skuIncomeDF.where($"amount" &gt; 150)skuIncomeDF.where(col("amount") &gt; 150)skuIncomeDF.where('amount &gt; 150)skuIncomeDF.where("amount &gt; 150")spark.sql( """ |select sku,category,year,amount from sku_sale_amount where amount &gt; 150 """.stripMargin) union 操作12345678910111213141516171819skuIncomeDF.union(skuIncomeDF).groupBy("sku").count()spark.sql( """ |select sku,count(1) from ( |select sku,category,year,amount from sku_sale_amount | union all |select sku,category,year,amount from sku_sale_amount |)a group by sku """.stripMargin)skuIncomeDF.union(skuIncomeDF).distinct().groupBy("sku").count()spark.sql( """ |select sku,count(1) from ( |select sku,category,year,amount from sku_sale_amount | union |select sku,category,year,amount from sku_sale_amount |)a group by sku """.stripMargin) group by操作1234skuIncomeDF.groupBy("sku").count()skuIncomeDF.groupBy($"sku").count()skuIncomeDF.groupBy('sku).count()skuIncomeDF.groupBy(col("sku")).count() join操作12345678skuIncomeDF.join(skuIncomeDF, "sku")skuIncomeDF.join(skuIncomeDF, Seq("sku", "category"))skuIncomeDF.join(skuIncomeDF, Seq("sku"), "inner")skuIncomeDF.join(skuProductDF, $"goods_id" === $"goods_id1")skuIncomeDF.join(skuProductDF, col("goods_id") === col("goods_id1"))skuIncomeDF.join(skuProductDF, col("goods_id").equalTo(col("goods_id1")))skuIncomeDF.joinWith(skuProductDF, skuIncomeDF("goods_id") === skuProductDF("goods_id1"), "inner") order by底层用的还是sort。123skuIncomeDF.orderBy("sku", "category", "amount")skuIncomeDF.orderBy($"sku", $"category", $"amount".desc)skuIncomeDF.orderBy(col("sku"), col("category"), col("amount").desc) sort 操作123skuIncomeDF.sort("sku", "category", "amount")skuIncomeDF.sort($"sku", $"category", $"amount".desc)skuIncomeDF.sort(col("sku"), col("category"), col("amount").desc) sortwithinpartition操作分区内部进行排序，局部排序。123skuIncomeDF.sortWithinPartitions("sku", "category")skuIncomeDF.sortWithinPartitions($"sku", $"category", $"amount".desc)skuIncomeDF.sortWithinPartitions(col("sku"), col("category").desc) withColumn 操作作用:假如列，存在就替换，不存在新增;对已有的列进行重命名。123skuIncomeDF.withColumn("amount", $"amount" + 1)skuIncomeDF.withColumn("amount", 'amount + 1)skuIncomeDF.withColumnRenamed("amount", "amount1") foreach操作1skuIncomeDF.foreach(row =&gt; println(row.get(0))) foreachPartition操作123skuIncomeDF.foreachPartition((it: Iterator[Row]) =&gt; &#123; it.foreach(row =&gt; println(row.get(0)))&#125;) distinct操作1skuIncomeDF.distinct() dropDuplicates操作1skuIncomeDF.dropDuplicates("sku") drop操作删除一列，或者多列。1skuIncomeDF.drop("sku", "category") cube操作说明：相当于(category,year),(year),(category),() 分别分组然后对amount求sum参考：https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html 123skuIncomeDF.cube($"category", $"year".cast("string").as("year")) .agg(sum("amount")) .sort(col("category").desc_nulls_first, col("year").desc_nulls_first) rollup操作说明：等价于分别对(category,year),(year),()进行 groupby 对amount求sum。参考：https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html123skuIncomeDF.rollup($"category", $"year".cast("string").as("year")) .agg(sum("amount") as "amount", grouping_id() as "gid") .sort($"category".desc_nulls_last, $"year".asc_nulls_last) pivot操作123skuIncomeDF.groupBy("category") .pivot("year", Seq("2014", "2015", "2016", "2017")) .agg(sum("amount")) 转置操作文章来自：http://bailiwick.io/2017/10/21/transpose-data-with-spark/1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Import the requisite methodsimport org.apache.spark.sql.DataFrameimport org.apache.spark.sql.functions.&#123;array, col, explode, lit, struct&#125;// Create a dataframeval df = spark.createDataFrame(Seq( (1, 1, 2, 3, 8, 4, 5), (2, 4, 3, 8, 7, 9, 8), (3, 6, 1, 9, 2, 3, 6), (4, 7, 8, 6, 9, 4, 5), (5, 9, 2, 7, 8, 7, 3), (6, 1, 1, 4, 2, 8, 4))).toDF("uid", "col1", "col2", "col3", "col4", "col5", "col6")df.show(10,false)// Create the transpose user defined function.// Imputs:// transDF: The dataframe which will be transposed// transBy: The column that the dataframe will be transposed by// Outputs:// Dataframe datatype consisting of three columns:// transBy// column_name// column_valuedef transposeUDF(transDF: DataFrame, transBy: Seq[String]): DataFrame = &#123; val (cols, types) = transDF.dtypes.filter&#123; case (c, _) =&gt; !transBy.contains(c)&#125;.unzip require(types.distinct.size == 1) val kvs = explode(array( cols.map(c =&gt; struct(lit(c).alias("column_name"), col(c).alias("column_value"))): _* )) val byExprs = transBy.map(col(_)) transDF .select(byExprs :+ kvs.alias("_kvs"): _*) .select(byExprs ++ Seq($"_kvs.column_name", $"_kvs.column_value"): _*)&#125;transposeUDF(df, Seq("uid")).show(12,false)Output:df.show(10,false)+---+----+----+----+----+----+----+|uid|col1|col2|col3|col4|col5|col6|+---+----+----+----+----+----+----+|1 |1 |2 |3 |8 |4 |5 ||2 |4 |3 |8 |7 |9 |8 ||3 |6 |1 |9 |2 |3 |6 ||4 |7 |8 |6 |9 |4 |5 ||5 |9 |2 |7 |8 |7 |3 ||6 |1 |1 |4 |2 |8 |4 |+---+----+----+----+----+----+----+transposeUDF(df, Seq("uid")).show(12,false)+---+-----------+------------+|uid|column_name|column_value|+---+-----------+------------+|1 |col1 |1 ||1 |col2 |2 ||1 |col3 |3 ||1 |col4 |8 ||1 |col5 |4 ||1 |col6 |5 ||2 |col1 |4 ||2 |col2 |3 ||2 |col3 |8 ||2 |col4 |7 ||2 |col5 |9 ||2 |col6 |8 |+---+-----------+------------+only showing top 12 rows 参考文章https://legacy.gitbook.com/book/jaceklaskowski/mastering-spark-sql/detailshttps://www.toutiao.com/i6631318012546793992/]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql读取数据库问题]]></title>
    <url>%2F2019%2F04%2F18%2Fsparksql%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%BA%93%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[spark jdbc方式读取数据库，每个excutor都会去拉数据？说明：该问题来源spark技术分享朋友群12345678910spark.read("jdbc") .option("url", url) .option("dbtable", "pets") .option("user", user) .option("password", password) .option("numPartitions", 10) .option("partitionColumn", "owner_id") .option("lowerBound", 1) .option("upperBound", 10000) .load() 这种一般会转换为1234SELECT * FROM pets WHERE owner_id &gt;= 1 and owner_id &lt; 1000SELECT * FROM pets WHERE owner_id &gt;= 1000 and owner_id &lt; 2000SELECT * FROM pets WHERE owner_id &gt;= 2000 and owner_id &lt; 3000...]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antlr4学习笔记]]></title>
    <url>%2F2019%2F04%2F18%2Fantlr4%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[antlr介绍注意： 文字来自《antlr4权威指南》ANTLR 语言识别的一个工具 (ANother Tool for Language Recognition ) 是一种语言工具，它提供了一个框架，可以通过包含 Java, C++, 或 C# 动作（action）的语法描述来构造语言识别器，编译器和解释器。 计算机语言的解析已经变成了一种非常普遍的工作，在这方面的理论和工具经过近 40 年的发展已经相当成熟，使用 Antlr 等识别工具来识别，解析，构造编译器比手工编程更加容易，同时开发的程序也更易于维护。语言识别的工具有很多种，比如大名鼎鼎的 Lex 和 YACC，Linux 中有他们的开源版本，分别是 Flex 和 Bison。在 Java 社区里，除了 Antlr 外，语言识别工具还有 JavaCC 和 SableCC 等。和大多数语言识别工具一样，Antlr 使用上下文无关文法描述语言。最新的 Antlr 是一个基于 LL(*) 的语言识别器。在 Antlr 中通过解析用户自定义的上下文无关文法，自动生成词法分析器 (Lexer)、语法分析器 (Parser) 和树分析器 (Tree Parser)。 antlr 的应用编程语言处理识别和处理编程语言是 Antlr 的首要任务，编程语言的处理是一项繁重复杂的任务，为了简化处理，一般的编译技术都将语言处理工作分为前端和后端两个部分。其中前端包括词法分析、语法分析、语义分析、中间代码生成等若干步骤，后端包括目标代码生成和代码优化等步骤。 Antlr 致力于解决编译前端的所有工作。使用 Anltr 的语法可以定义目标语言的词法记号和语法规则，Antlr 自动生成目标语言的词法分析器和语法分析器；此外，如果在语法规则中指定抽象语法树的规则，在生成语法分析器的同时，Antlr 还能够生成抽象语法树；最终使用树分析器遍历抽象语法树，完成语义分析和中间代码生成。整个工作在 Anltr 强大的支持下，将变得非常轻松和愉快。 文本处理当需要文本处理时，首先想到的是正则表达式，使用 Anltr 的词法分析器生成器，可以很容易的完成正则表达式能够完成的所有工作；除此之外使用 Anltr 还可以完成一些正则表达式难以完成的工作，比如识别左括号和右括号的成对匹配等。 基本环境配置命令方式生成文件下载antlrhttps://www.antlr.org/download/antlr-4.7.2-complete.jar 启动antlr12345alias antlr4='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.Tool'alias grun='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.gui.TestRig'antlr4 测试案例定义antlr Hello.g4文件注意： 逗号必须对齐1234grammar Hello; //定义一个名为Hello的语法r : &apos;hello&apos; ID ; // 匹配一个关键字hello和一个紧随其后的标识符ID : [a-z]+ ; // 匹配小写字母组成的标识符WS : [ \t\r\n]+ -&gt; skip; // 忽略空格 Tab 换行 运行生成对应的文件和代码antlr4 Hello.g4javac Hello*.java 运行生成树图grun Hello r -gui IDEA中antlr4环境配置参考文章https://www.baeldung.com/java-antlr 在maven 项目src/main下创建一个目录mkdir -p antlr4/cn/xh/parse 编写MyParse.g4文件12345678910111213141516171819202122grammar MyParse;//parserprog:stat;stat:expr|NEWLINE;expr:multExpr((&apos;+&apos;|&apos;-&apos;)multExpr)*;multExpr:atom((&apos;*&apos;|&apos;/&apos;)atom)*;atom:&apos;(&apos;expr&apos;)&apos; |INT |ID;//lexerID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;INT:&apos;0&apos;..&apos;9&apos;+;NEWLINE:&apos;\r&apos;?&apos;\n&apos;;WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;; 在pom.xml文件中添加如下内容1234567891011121314151617181920212223&lt;antlr4.version&gt;4.7.2&lt;/antlr4.version&gt;&lt;dependency&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4-runtime&lt;/artifactId&gt; &lt;version&gt;$&#123;antlr4.version&#125;&lt;/version&gt;&lt;/dependency&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.7.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;antlr4&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 输入maven命令打包1mvn package 结果在target/generated-sources/antlr4/cn/xh/parse 中产生对应的文件 antlr基本语法详解参考《antlr4权威指南》 定义文件头使用Antlr 的语法规则来定义算术表达式文法，文件头部是 grammar 关键字，定义文法的名字，必须与文法文件文件的名字相同。1grammar MyParse; 自定义一个算术表达式表达式规则： 算法的优先级需要通过文法规则的嵌套定义来体现，加减法的优先级低于乘除法，表达式 expr 的定义由乘除法表达式 multExpr 和加减法算符 (‘+’|’-‘) 构成；同理，括号的优先级高于乘除法，乘除法表达式 multExpr 通过原子操作数 atom 和乘除法算符 (‘*’|’/’) 构成。 程序有一个语句构成，语句有表达式或者换行符构成。12345prog: stat ; stat: expr |NEWLINE ; 在 Antlr 中语法定义和词法定义通过规则的第一个字符来区别， 规定语法定义符号的第一个字母小写，而词法定义符号的第一个字母大写。算术表达式中用到了 4 类记号 ( 在 Antlr 中被称为 Token)，分别是标识符 ID，表示一个变量；常量 INT，表示一个常数；换行符 NEWLINE 和空格 WS，空格字符在语言处理时将被跳过，skip() 是词法分析器类的一个方法1234ID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;INT:&apos;0&apos;..&apos;9&apos;+;NEWLINE:&apos;\r&apos;?&apos;\n&apos;;WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;; 案例antlr文件编写MyParse.g4文件12345678910111213141516171819202122grammar MyParse;//parserprog:stat;stat:expr|NEWLINE;expr:multExpr((&apos;+&apos;|&apos;-&apos;)multExpr)*;multExpr:atom((&apos;*&apos;|&apos;/&apos;)atom)*;atom:&apos;(&apos;expr&apos;)&apos; |INT |ID;//lexerID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;INT:&apos;0&apos;..&apos;9&apos;+;NEWLINE:&apos;\r&apos;?&apos;\n&apos;;WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;; 使用java调用分析器参考spark源码中ParseDriver.scala文件中AbstractSqlParser类parse方法123456789101112131415161718192021222324252627282930313233343536373839404142package com.xh.antlr4;import cn.xh.parse.MyParseLexer;import cn.xh.parse.MyParseParser;import org.antlr.v4.runtime.CharStreams;import org.antlr.v4.runtime.CommonTokenStream;import org.apache.spark.sql.catalyst.parser.UpperCaseCharStream;public class UserMyParse &#123; public static void run(String expr) throws Exception &#123; //对每一个输入的字符串，构造一个 CodePointCharStream 流 in UpperCaseCharStream in = new UpperCaseCharStream(CharStreams.fromString(expr)); //用 in 构造词法分析器 lexer，词法分析的作用是产生记号 MyParseLexer lexer = new MyParseLexer(in); //用词法分析器 lexer 构造一个记号流 tokens CommonTokenStream tokens = new CommonTokenStream(lexer); //再使用 tokens 构造语法分析器 parser,至此已经完成词法分析和语法分析的准备工作 MyParseParser parser = new MyParseParser(tokens); //最终调用语法分析器的规则 prog，完成对表达式的验证 parser.prog(); &#125; public static void main(String[] args) throws Exception &#123; String[] testStr = &#123; "2", "a+b+3", "(a-b)+3", "a+(b*3)" &#125;; for (String s : testStr) &#123; System.out.println("Input expr:" + s); run(s); &#125; &#125;&#125;]]></content>
      <categories>
        <category>antlr</category>
      </categories>
      <tags>
        <tag>antlr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark sql窗口函数实战]]></title>
    <url>%2F2019%2F04%2F15%2Fspark%20sql%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[参考文章主要以一些官方文档为参考。https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics https://help.aliyun.com/document_detail/34994.html?spm=a2c4g.11174283.6.650.6f02590e0d209m#h2-url-1 https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html http://cdn2.hubspot.net/hubfs/438089/notebooks/eBook/Introducing_Window_Functions_in_Spark_SQL_Notebook.html http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/ https://www.cnblogs.com/piaolingzxh/p/5538783.html 准备数据123456789101112131415161718192021222324object WindowFunctionTest extends BaseSparkSession &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() .setAppName("WindowFunctionTest") .set("spark.master", "local[*]") val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ val df = List( ("浙江", "2018-01-01", 500), ("浙江", "2018-01-02", 450), ("浙江", "2018-01-03", 550), ("湖北", "2018-01-01", 250), ("湖北", "2018-01-02", 290), ("湖北", "2018-01-03", 270) ).toDF("site", "date", "user_cnt") &#125;&#125; 平均移动值DataFrame API方式实现方式一：123// 窗口定义从 -1(前一行)到 1(后一行)，每一个滑动的窗口总用有3行 val movinAvgSpec = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("MovingAvg", avg(df("user_cnt")).over(movinAvgSpec)).show() 方式二：1234567 val movinAvgSpec = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1)df.select( $"site", $"date", $"amount", avg($"user_cnt").over(movinAvgSpec).as("moving_avg_user_cnt")).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg |from site_info """.stripMargin).show() lag函数说明：取当前记录的前x条数据的指定列，如果没有返回null，有就返回真实值。 DataFrame API方式实现方式一：12val lagwSpec = Window.partitionBy("site").orderBy("date")df.withColumn("prevUserCnt", lag(df("user_cnt"), 1).over(lagwSpec)).show() 方式二：1234567 val lagwSpec = Window.partitionBy("site").orderBy("date")df.select( $"site", $"date", $"amount", lag($"user_cnt").over(movinAvgSpec).as("lag_user_cnt")).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | lag(user_cnt,1) over(partition by site order by date asc ) as prevUserCnt |from site_info """.stripMargin).show() lead函数说明：取当前记录的后x条数据的指定列，如果没有返回null，有就返回真实值。 DataFrame API方式实现方式一：12val leadwSpec = Window.partitionBy("site").orderBy("date")df.withColumn("lead_user_cnt", lead(df("user_cnt"), 1).over(leadwSpec)).show() 方式二：1234567val leadwSpec = Window.partitionBy("site").orderBy("date") df.select( $"site", $"date", $"user_cnt", lead($"user_cnt", 1).over(leadwSpec).as("lead_user_cnt") ).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | lead(user_cnt,1) over(partition by site order by date asc ) as lead_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+-------------+|site| date|user_cnt|lead_user_cnt|+----+----------+--------+-------------+|湖北|2018-01-01| 250| 290||湖北|2018-01-02| 290| 270||湖北|2018-01-03| 270| null||浙江|2018-01-01| 500| 450||浙江|2018-01-02| 450| 550||浙江|2018-01-03| 550| null|+----+----------+--------+-------------+ FIRST_VALUE函数说明：该函数用于获取分组排序后最第一条记录的字段值。 DataFrame API方式实现方式一：12val firstValuewSpec = Window.partitionBy("site").orderBy("date")df.withColumn("first_value_user_cnt", first("user_cnt").over(firstValuewSpec)).show() 方式二：123456val firstValuewSpec = Window.partitionBy("site").orderBy("date")df.select( $"site", $"date", $"user_cnt", first($"user_cnt").over(firstValuewSpec).as("first_value_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | first_value(user_cnt) over(partition by site order by date asc ) as first_value_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+--------------------+|site| date|user_cnt|first_value_user_cnt|+----+----------+--------+--------------------+|湖北|2018-01-01| 250| 250||湖北|2018-01-02| 290| 250||湖北|2018-01-03| 270| 250||浙江|2018-01-01| 500| 500||浙江|2018-01-02| 450| 500||浙江|2018-01-03| 550| 500|+----+----------+--------+--------------------+ LAST_VALUE函数说明：该函数用于获取分组排序后最后一条记录的字段值。 DataFrame API方式实现方式一：12val lastValuewSpec = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, Long.MaxValue)df.withColumn("last_value_user_cnt", last("user_cnt").over(lastValuewSpec)).show() 方式二：123456val lastValuewSpec = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, Long.MaxValue)df.select( $"site", $"date", $"user_cnt", last($"user_cnt").over(lastValuewSpec).as("last_value_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | last_value(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following ) as last_value_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+-------------------+|site| date|user_cnt|last_value_user_cnt|+----+----------+--------+-------------------+|湖北|2018-01-01| 250| 270||湖北|2018-01-02| 290| 270||湖北|2018-01-03| 270| 270||浙江|2018-01-01| 500| 550||浙江|2018-01-02| 450| 550||浙江|2018-01-03| 550| 550|+----+----------+--------+-------------------+ COUNT说明：该函数用于计算计数值。 不指定order by###方式一：12val counWSpec = Window.partitionBy("site")df.withColumn("count", count("user_cnt").over(counWSpec)).show() 方式二：123456val counWSpec = Window.partitionBy("site")df.select( $"site", $"date", $"user_cnt", count($"user_cnt").over(counWSpec).as("count")).show() sql方式实现12345678spark.sql( """ | select site, | date, | user_cnt, | count(user_cnt) over(partition by site) as count |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+-----+|site| date|user_cnt|count|+----+----------+--------+-----+|湖北|2018-01-01| 250| 3||湖北|2018-01-02| 290| 3||湖北|2018-01-03| 270| 3||浙江|2018-01-01| 500| 3||浙江|2018-01-02| 450| 3||浙江|2018-01-03| 550| 3|+----+----------+--------+-----+ 指定order by指定order by时，返回当前窗口内从开始行到当前行的累计计数值。 DataFrame API方式实现方式一：12val counWSpec = Window.partitionBy("site").orderBy('date.asc)df.withColumn("count", count("user_cnt").over(counWSpec)).show() 方式二：123456val counWSpec = Window.partitionBy("site").orderBy("date")df.select( $"site", $"date", $"user_cnt", count($"user_cnt").over(counWSpec).as("count")) sql方式实现12345678spark.sql( """ | select site, | date, | user_cnt, | count(user_cnt) over(partition by site order by date) as count |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+-----+|site| date|user_cnt|count|+----+----------+--------+-----+|湖北|2018-01-01| 250| 1||湖北|2018-01-02| 290| 2||湖北|2018-01-03| 270| 3||浙江|2018-01-01| 500| 1||浙江|2018-01-02| 450| 2||浙江|2018-01-03| 550| 3|+----+----------+--------+-----+ sum函数说明：该函数用于计算汇总值。 不指定order byDataFrame API方式实现方式一：12val sumWSpec = Window.partitionBy("site")df.withColumn("sum_user_cnt", sum("user_cnt").over(sumWSpec)).show() 方式二：1234567val sumWSpec = Window.partitionBy("site")df.select( $"site", $"date", $"user_cnt", sum($"user_cnt").over(sumWSpec).as("sum_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | sum(user_cnt) over(partition by site ) as sum_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|sum_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 810||湖北|2018-01-02| 290| 810||湖北|2018-01-03| 270| 810||浙江|2018-01-01| 500| 1500||浙江|2018-01-02| 450| 1500||浙江|2018-01-03| 550| 1500|+----+----------+--------+------------+ 指定order byDataFrame API方式实现方式一：12val sumWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.withColumn("sum_user_cnt", sum("user_cnt").over(sumWSpec)) 方式二：1234567val sumWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.select( $"site", $"date", $"user_cnt", sum($"user_cnt").over(sumWSpec).as("sum_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | sum(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following ) as sum_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|sum_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 810||湖北|2018-01-02| 290| 810||湖北|2018-01-03| 270| 810||浙江|2018-01-01| 500| 1500||浙江|2018-01-02| 450| 1500||浙江|2018-01-03| 550| 1500|+----+----------+--------+------------+ min函数不指定order byDataFrame API方式实现方式一：12val minWSpec = Window.partitionBy("site")df.withColumn("min_user_cnt", min("user_cnt").over(minWSpec)).show() 方式二：123456df.select( $"site", $"date", $"user_cnt", min($"user_cnt").over(minWSpec)).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | min(user_cnt) over(partition by site) as min_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+----------------------------------------------------------+|site| date|user_cnt|min(user_cnt) OVER (PARTITION BY site unspecifiedframe$())|+----+----------+--------+----------------------------------------------------------+|湖北|2018-01-01| 250| 250||湖北|2018-01-02| 290| 250||湖北|2018-01-03| 270| 250||浙江|2018-01-01| 500| 450||浙江|2018-01-02| 450| 450||浙江|2018-01-03| 550| 450|+----+----------+--------+----------------------------------------------------------+ 指定order by方式一：12val minWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.withColumn("min_user_cnt", min("user_cnt").over(minWSpec)).show() 方式二：1234567val minWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.select( $"site", $"date", $"user_cnt", min($"user_cnt").over(minWSpec)) sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | min(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+----------------------------------------------------------+|site| date|user_cnt|min(user_cnt) OVER (PARTITION BY site unspecifiedframe$())|+----+----------+--------+----------------------------------------------------------+|湖北|2018-01-01| 250| 250||湖北|2018-01-02| 290| 250||湖北|2018-01-03| 270| 250||浙江|2018-01-01| 500| 450||浙江|2018-01-02| 450| 450||浙江|2018-01-03| 550| 450|+----+----------+--------+----------------------------------------------------------+ max函数不指定order byDataFrame API方式实现方式一：12val maxWSpec = Window.partitionBy("site")df.withColumn("min_user_cnt", max("user_cnt").over(maxWSpec)) 方式二：1234567val maxWSpec = Window.partitionBy("site")df.select( $"site", $"date", $"user_cnt", max($"user_cnt").over(maxWSpec).as("max_user_cnt")) sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | max(user_cnt) over(partition by site ) as min_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|min_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 290||湖北|2018-01-02| 290| 290||湖北|2018-01-03| 270| 290||浙江|2018-01-01| 500| 550||浙江|2018-01-02| 450| 550||浙江|2018-01-03| 550| 550|+----+----------+--------+------------+ 指定order byDataFrame API方式实现方式一：12val maxWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.withColumn("min_user_cnt", max("user_cnt").over(maxWSpec)).show() 方式二：1234567val maxWSpec = Window.partitionBy("site").orderBy('date asc).rowsBetween(Long.MinValue, Long.MaxValue)df.select( $"site", $"date", $"user_cnt", max($"user_cnt").over(maxWSpec).as("max_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | max(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|min_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 290||湖北|2018-01-02| 290| 290||湖北|2018-01-03| 270| 290||浙江|2018-01-01| 500| 550||浙江|2018-01-02| 450| 550||浙江|2018-01-03| 550| 550|+----+----------+--------+------------+ avg函数说明：该函数用于计算平均值。 不指定order byDataFrame API方式实现方式一：12val avgWSpec = Window.partitionBy("site") df.withColumn("avg_user_cnt", avg("user_cnt").over(avgWSpec)).show() 方式二：1234567val avgWSpec = Window.partitionBy("site")df.select( $"site", $"date", $"user_cnt", avg("user_cnt").over(avgWSpec).as("avg_user_cnt")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site ) as avg_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|avg_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 270.0||湖北|2018-01-02| 290| 270.0||湖北|2018-01-03| 270| 270.0||浙江|2018-01-01| 500| 500.0||浙江|2018-01-02| 450| 500.0||浙江|2018-01-03| 550| 500.0|+----+----------+--------+------------+ 指定order byDataFrame API方式实现方式一：12val avgWSpec = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, Long.MaxValue)df.withColumn("avg_user_cnt", avg("user_cnt").over(avgWSpec)).show() 方式二：1234567val avgWSpec = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, Long.MaxValue)df.select( $"site", $"date", $"user_cnt", avg("user_cnt").over(avgWSpec).as("avg_user_cnt")) sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following ) as avg_user_cnt |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+------------+|site| date|user_cnt|avg_user_cnt|+----+----------+--------+------------+|湖北|2018-01-01| 250| 270.0||湖北|2018-01-02| 290| 270.0||湖北|2018-01-03| 270| 270.0||浙江|2018-01-01| 500| 500.0||浙江|2018-01-02| 450| 500.0||浙江|2018-01-03| 550| 500.0|+----+----------+--------+------------+ rank函数说明：该函数用于计算排名。 DataFrame API方式实现方式一：12val rankWSpec = Window.partitionBy("site").orderBy('user_cnt.desc)df.withColumn("rank", rank().over(rankWSpec)) 方式二：1234567val rankWSpec = Window.partitionBy("site").orderBy('user_cnt.desc)df.select( $"site", $"date", $"user_cnt", rank().over(rankWSpec).as("rank")).show() sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | rank() over(partition by site order by user_cnt desc) as rank_user_cnt |from site_info """.stripMargin).show() 结果12 row_number over 函数DataFrame API方式实现方式一：12val rowNUmberWSpec = Window.partitionBy("site").orderBy('date desc, 'user_cnt desc)df.withColumn("row_num", row_number().over(rowNUmberWSpec)).show() 方式二：1234567val rowNUmberWSpec = Window.partitionBy("site").orderBy('date desc, 'user_cnt desc)df.select( $"site", $"date", $"user_cnt", row_number().over(rowNUmberWSpec).as("row_num")).show() sql方式实现123456789spark.sql( """ |select site, | date, | user_cnt, | row_number() over(partition by site order by date desc , user_cnt desc ) as row_num |from site_info | """.stripMargin) 结果12345678910+----+----------+--------+-------+|site| date|user_cnt|row_num|+----+----------+--------+-------+|湖北|2018-01-03| 270| 1||湖北|2018-01-02| 290| 2||湖北|2018-01-01| 250| 3||浙江|2018-01-03| 550| 1||浙江|2018-01-02| 450| 2||浙江|2018-01-01| 500| 3|+----+----------+--------+-------+ dense_rank函数说明：该函数用于计算连续排名。 DataFrame API方式实现方式一：12val denseRankWSpec = Window.partitionBy("site").orderBy('date asc)df.withColumn("dense_rank", dense_rank() over (denseRankWSpec)).show() 方式二：1234567val denseRankWSpec = Window.partitionBy("site").orderBy('date asc)df.select( $"site", $"date", $"user_cnt", dense_rank().over(denseRankWSpec).as("dense_rank")).show() sql方式实现123456789 spark.sql( """ |select site, | date, | user_cnt, | dense_rank() over(partition by site order by date asc ) as dense_rank |from site_info |""".stripMargin).show() 结果12345678910+----+----------+--------+----------+|site| date|user_cnt|dense_rank|+----+----------+--------+----------+|湖北|2018-01-01| 250| 1||湖北|2018-01-02| 290| 2||湖北|2018-01-03| 270| 3||浙江|2018-01-01| 500| 1||浙江|2018-01-02| 450| 2||浙江|2018-01-03| 550| 3|+----+----------+--------+----------+ percent_rank函数说明：该函数用于计算一组数据中某行的相对排名。 DataFrame API方式实现方式一：12val percentRankWSpec = Window.partitionBy("site").orderBy('date asc)df.withColumn("percent_rank", percent_rank() over (percentRankWSpec)).show() 方式二：1234567val percentRankWSpec = Window.partitionBy("site").orderBy('date asc)df.select( $"site", $"date", $"user_cnt", percent_rank().over(percentRankWSpec).as("percent_rank")).show() sql方式实现123456789 spark.sql( """ |select site, | date, | user_cnt, | percent_rank() over(partition by site order by date asc ) as percent_rank |from site_info |""".stripMargin).show() 结果123456789|site| date|user_cnt|percent_rank|+----+----------+--------+------------+|湖北|2018-01-01| 250| 0.0||湖北|2018-01-02| 290| 0.5||湖北|2018-01-03| 270| 1.0||浙江|2018-01-01| 500| 0.0||浙江|2018-01-02| 450| 0.5||浙江|2018-01-03| 550| 1.0|+----+----------+--------+------------+ ntile函数说明：用于将分组数据按照顺序切分成n片，并返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。 DataFrame API方式实现方式一：12val ntileRankWSpec = Window.partitionBy("site").orderBy('date asc)df.withColumn("ntile", ntile(2).over(ntileRankWSpec)).show() 方式二：1234567val ntileRankWSpec = Window.partitionBy("site").orderBy('date asc)df.select( $"site", $"date", $"user_cnt", ntile(2).over(ntileRankWSpec).as("ntile")) sql方式实现12345678spark.sql( """ |select site, | date, | user_cnt, | ntile(2) over(partition by site order by date) as ntile |from site_info """.stripMargin).show() 结果12345678910+----+----------+--------+-----+|site| date|user_cnt|ntile|+----+----------+--------+-----+|湖北|2018-01-01| 250| 1||湖北|2018-01-02| 290| 1||湖北|2018-01-03| 270| 2||浙江|2018-01-01| 500| 1||浙江|2018-01-02| 450| 1||浙江|2018-01-03| 550| 2|+----+----------+--------+-----+ 致谢！本人能力有限，博客错误难免，有错往将错误发送到邮箱(t_spider@aliyun.com)]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)]]></title>
    <url>%2F2019%2F04%2F14%2Fidea%E4%B8%ADSpark%20on%20yarn%20%E8%BF%9C%E7%A8%8BDebug%20Spark%20%E6%BA%90%E7%A0%81(Yarn%20Client%2Battach%20%E6%A8%A1%E5%BC%8F)%2F</url>
    <content type="text"><![CDATA[背景idea 中连接yarn集群debug spark代码,有利于定位线上问题。 环境默认你hadoop环境已经搞定情况下的说明！！！scala版本： 2.11.8jdk版本：JDK1.8hadoop版本： 2.7.3spark版本： 2.4.1zookeeper： 3.4.6高能预警： 一定要保持yarn集群机器scala版本和项目pom文件中scala版本一致。 linux版本 IP hostname 进程 centos7 192.168.8.81 hadoop01 NameNode、DFSZKFailoverController centos7 192.168.8.82 hadoop02 NameNode、DFSZKFailoverController、ResourceManager、JournalNode、NodeManager、DataNode、QuorumPeerMain centos7 192.168.8.83 hadoop03 JournalNode、NodeManager、DataNode、QuorumPeerMain centos7 192.168.8.84 hadoop04 JournalNode、NodeManager、DataNode、QuorumPeerMain idea所在机器： linux版本 IP hostname ubuntu18.04 192.168.8.85 hadoop05 maven项目中resources目录中配置文件高能预警： 配置文件需要和hadoop集群一致 具体配置文件如下：core-site.xml1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop02:2181,hadoop03:2181,hadoop04:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;ns1&lt;/value&gt;&lt;/property&gt;&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;&lt;value&gt;hadoop01:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;&lt;value&gt;hadoop01:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;&lt;value&gt;hadoop02:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;&lt;value&gt;hadoop02:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/ns1&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/journaldata&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启NameNode失败自动切换 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置失败自动切换实现方式 --&gt;&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfenceshell(/bin/true)&lt;/value&gt;&lt;/property&gt;&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置sshfence隔离机制超时时间 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;&lt;!--修改block块的大小,默认为128M--&gt;&lt;property&gt;&lt;name&gt;dfs.block.seze&lt;/name&gt;&lt;value&gt;128&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!--开启RM高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop02&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop02:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop03:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop02:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop03:8030&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop02:2181,hadoop03:2181,hadoop04:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop02:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop03:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop02:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop03:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 两个可选值：org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore 以及 默认值org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore --&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop02:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.web-proxy.address&lt;/name&gt; &lt;value&gt;192.168.8.82:8089&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;!-- 打开日志聚合功能，这样才能从web界面查看日志 --&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;!-- 聚合日志最长保留时间 --&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;!-- 中间结果存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt; &lt;value&gt;/home/hadoop/hadoop/data/localdir1,/home/hadoop/hadoop/data/localdir2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;!-- 日志存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt; &lt;value&gt;/home/hadoop/hadoop/data/hdfs/logdir1,/home/hadoop/hadoop/data/hdfs/logdir2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; spark-defaults.conf配置文件：123456789101112131415161718192021222324252627282930313233## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.## Default system properties included when running spark-submit.# This is useful for setting default environmental settings.spark.driver.extraClassPath /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*spark.executor.extraClassPath /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*spark.driver.extraJavaOptions -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005spark.executor.extraJavaOptions -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005spark.driver.memory 512mspark.executor.memory 512m# Example:# spark.master spark://master:7077# spark.eventLog.enabled true# spark.eventLog.dir hdfs://namenode:8021/directory# spark.serializer org.apache.spark.serializer.KryoSerializer# spark.driver.memory 5g# spark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three" maven pom配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.xh.spark&lt;/groupId&gt; &lt;artifactId&gt;sparklearning&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;spark.version&gt;2.4.1&lt;/spark.version&gt; &lt;hadoop.version&gt;2.7.3&lt;/hadoop.version&gt; &lt;mysql.version&gt;5.1.35&lt;/mysql.version&gt; &lt;hive.version&gt;2.3.3&lt;/hive.version&gt; &lt;guava.version&gt;26.0-jre&lt;/guava.version&gt; &lt;fastjson.version&gt;1.2.40&lt;/fastjson.version&gt; &lt;zookeeper.version&gt;3.4.6&lt;/zookeeper.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--scala--&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;$&#123;zookeeper.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--spark--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-yarn_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--hive--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--hadoop--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--guava--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- This plugin compiles Scala files --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- This plugin compiles Java files --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--跳过test begin--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.20&lt;/version&gt; &lt;configuration&gt; &lt;skipTests&gt;true&lt;/skipTests&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- This plugin adds all dependencies to JAR file during 'package' command. Pay EXTRA attention to the 'mainClass' tag. You have to set name of class with entry point to program ('main' method) --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifestFile&gt;&lt;/manifestFile&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;com.defonds.RsaEncryptor&lt;/mainClass&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt; &lt;resource&gt;META-INF/spring.handlers&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt; &lt;resource&gt;META-INF/spring.schemas&lt;/resource&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 开发程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.xh.spark.sql.functionimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject WindowFunctionTest &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() .setAppName("WindowFunctionTest") .set("spark.master", "yarn") .set("spark.submit.deployMode", "client") // 部署模式为client .set("yarn.resourcemanager.hostname", "hadoop02") // resourcemanager主机名 .set("spark.executor.instances", "2") // Executor实例的数量 .set("spark.dynamicAllocation.enabled", "false") .setJars(List("/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar", "/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar" )) val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ val df = List( ("站点1", "2018-01-01", 50), ("站点1", "2018-01-02", 45), ("站点1", "2018-01-03", 55), ("站点2", "2018-01-01", 25), ("站点2", "2018-01-02", 29), ("站点2", "2018-01-03", 27) ).toDF("site", "date", "user_cnt") // 简单移动平均值 // API方式 // 窗口定义从 -1(前一行)到 1(后一行) ，每一个滑动的窗口总用有3行 val movinAvgSpec = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("MovingAvg", avg(df("user_cnt")).over(movinAvgSpec)).show() // sql方式 df.createOrReplaceTempView("site_info") spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg |from site_info """.stripMargin).show() spark.stop() &#125;&#125; 以上确认无误之后，直接在idea里面run该程序。 执行结果如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654/usr/lib/jvm/java-1.8.0-openjdk/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:45315,suspend=y,server=n -javaagent:/home/hadoop/idea/lib/rt/debugger-agent.jar -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-1.8.0-openjdk/jre/lib/charsets.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jce.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jsse.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/management-agent.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/resources.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/rt.jar:/home/hadoop/worker/sparklearning/target/classes:/home/hadoop/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar:/home/hadoop/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar:/home/hadoop/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/hadoop/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/hadoop/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar:/home/hadoop/repository/org/apache/spark/spark-yarn_2.11/2.4.1/spark-yarn_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/hadoop/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/hadoop/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/hadoop/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/hadoop/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.5/hadoop-yarn-server-web-proxy-2.6.5.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/hadoop/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/hadoop/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-core_2.11/2.4.1/spark-core_2.11-2.4.1.jar:/home/hadoop/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/hadoop/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/hadoop/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/hadoop/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/hadoop/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/hadoop/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/hadoop/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/hadoop/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/hadoop/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/hadoop/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/hadoop/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/hadoop/repository/org/apache/spark/spark-launcher_2.11/2.4.1/spark-launcher_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-kvstore_2.11/2.4.1/spark-kvstore_2.11-2.4.1.jar:/home/hadoop/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/hadoop/repository/org/apache/spark/spark-network-common_2.11/2.4.1/spark-network-common_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-network-shuffle_2.11/2.4.1/spark-network-shuffle_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-unsafe_2.11/2.4.1/spark-unsafe_2.11-2.4.1.jar:/home/hadoop/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/hadoop/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/hadoop/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/hadoop/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/hadoop/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/hadoop/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/hadoop/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/hadoop/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/hadoop/repository/org/xerial/snappy/snappy-java/1.1.7.1/snappy-java-1.1.7.1.jar:/home/hadoop/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/hadoop/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/hadoop/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/hadoop/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/hadoop/repository/org/scala-lang/modules/scala-xml_2.11/1.0.6/scala-xml_2.11-1.0.6.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/hadoop/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/hadoop/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/hadoop/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/hadoop/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/hadoop/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/hadoop/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/hadoop/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/hadoop/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/hadoop/repository/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/hadoop/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/hadoop/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/hadoop/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/hadoop/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/hadoop/repository/org/apache/spark/spark-tags_2.11/2.4.1/spark-tags_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-sql_2.11/2.4.1/spark-sql_2.11-2.4.1.jar:/home/hadoop/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/hadoop/repository/org/apache/spark/spark-sketch_2.11/2.4.1/spark-sketch_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-catalyst_2.11/2.4.1/spark-catalyst_2.11-2.4.1.jar:/home/hadoop/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/hadoop/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/hadoop/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/hadoop/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/hadoop/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/hadoop/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/hadoop/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/hadoop/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/hadoop/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/hadoop/repository/org/apache/spark/spark-mllib_2.11/2.4.1/spark-mllib_2.11-2.4.1.jar:/home/hadoop/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/hadoop/repository/org/apache/spark/spark-graphx_2.11/2.4.1/spark-graphx_2.11-2.4.1.jar:/home/hadoop/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/hadoop/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/hadoop/repository/org/apache/spark/spark-mllib-local_2.11/2.4.1/spark-mllib-local_2.11-2.4.1.jar:/home/hadoop/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/hadoop/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/hadoop/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/hadoop/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/hadoop/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/hadoop/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/hadoop/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/hadoop/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/hadoop/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-hive_2.11/2.4.1/spark-hive_2.11-2.4.1.jar:/home/hadoop/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/hadoop/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/hadoop/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/hadoop/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/hadoop/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/hadoop/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/hadoop/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/hadoop/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/hadoop/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/hadoop/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/hadoop/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/hadoop/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/hadoop/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/hadoop/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/hadoop/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/hadoop/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/hadoop/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/hadoop/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/hadoop/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/hadoop/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/hadoop/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/hadoop/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/hadoop/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming_2.11/2.4.1/spark-streaming_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.4.1/spark-streaming-kafka-0-10_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/hadoop/repository/com/alibaba/fastjson/1.2.40/fastjson-1.2.40.jar:/home/hadoop/repository/org/apache/hive/hive-jdbc/2.3.3/hive-jdbc-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-common/2.3.3/hive-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-storage-api/2.4.0/hive-storage-api-2.4.0.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.3.3/orc-core-1.3.3.jar:/home/hadoop/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/home/hadoop/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/home/hadoop/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/home/hadoop/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/home/hadoop/repository/asm/asm/3.1/asm-3.1.jar:/home/hadoop/repository/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-web/2.6.2/log4j-web-2.6.2.jar:/home/hadoop/repository/com/tdunning/json/1.8/json-1.8.jar:/home/hadoop/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/hadoop/repository/org/apache/hive/hive-service/2.3.3/hive-service-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-server/2.3.3/hive-llap-server-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3.jar:/home/hadoop/repository/org/apache/slider/slider-core/0.90.2-incubating/slider-core-0.90.2-incubating.jar:/home/hadoop/repository/com/beust/jcommander/1.30/jcommander-1.30.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-registry/2.7.1/hadoop-yarn-registry-2.7.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/hadoop/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop2-compat/1.1.1/hbase-hadoop2-compat-1.1.1.jar:/home/hadoop/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/hadoop/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-server/1.1.1/hbase-server-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-procedure/1.1.1/hbase-procedure-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-prefix-tree/1.1.1/hbase-prefix-tree-1.1.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop-compat/1.1.1/hbase-hadoop-compat-1.1.1.jar:/home/hadoop/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/hadoop/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/repository/javax/servlet/jsp-api/2.0/jsp-api-2.0.jar:/home/hadoop/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/hadoop/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/repository/javax/servlet/servlet-api/2.4/servlet-api-2.4.jar:/home/hadoop/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/repository/org/apache/hive/hive-serde/2.3.3/hive-serde-2.3.3.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop-bundle/1.8.1/parquet-hadoop-bundle-1.8.1.jar:/home/hadoop/repository/org/apache/hive/hive-metastore/2.3.3/hive-metastore-2.3.3.jar:/home/hadoop/repository/org/apache/hbase/hbase-client/1.1.1/hbase-client-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-annotations/1.1.1/hbase-annotations-1.1.1.jar:/usr/lib/jvm/java-1.8.0-openjdk/lib/tools.jar:/home/hadoop/repository/org/apache/hbase/hbase-protocol/1.1.1/hbase-protocol-1.1.1.jar:/home/hadoop/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/hadoop/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/hadoop/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/repository/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/home/hadoop/repository/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/home/hadoop/repository/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/home/hadoop/repository/co/cask/tephra/tephra-api/0.6.0/tephra-api-0.6.0.jar:/home/hadoop/repository/co/cask/tephra/tephra-core/0.6.0/tephra-core-0.6.0.jar:/home/hadoop/repository/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/hadoop/repository/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/hadoop/repository/org/apache/twill/twill-common/0.6.0-incubating/twill-common-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-core/0.6.0-incubating/twill-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-api/0.6.0-incubating/twill-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-api/0.6.0-incubating/twill-discovery-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-core/0.6.0-incubating/twill-discovery-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-zookeeper/0.6.0-incubating/twill-zookeeper-0.6.0-incubating.jar:/home/hadoop/repository/co/cask/tephra/tephra-hbase-compat-1.0/0.6.0/tephra-hbase-compat-1.0-0.6.0.jar:/home/hadoop/repository/org/apache/hive/hive-shims/2.3.3/hive-shims-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-common/2.3.3/hive-shims-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-0.23/2.3.3/hive-shims-0.23-2.3.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.7.2/hadoop-yarn-server-resourcemanager-2.7.2.jar:/home/hadoop/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.7.2/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6-tests.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-scheduler/2.3.3/hive-shims-scheduler-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-service-rpc/2.3.3/hive-service-rpc-2.3.3.jar:/home/hadoop/repository/org/apache/httpcomponents/httpcore/4.4/httpcore-4.4.jar:/home/hadoop/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/hadoop/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/hadoop/repository/org/apache/hive/hive-exec/2.3.3/hive-exec-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-vector-code-gen/2.3.3/hive-vector-code-gen-2.3.3.jar:/home/hadoop/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/hadoop/repository/org/apache/hive/hive-llap-tez/2.3.3/hive-llap-tez-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-client/2.3.3/hive-llap-client-2.3.3.jar:/home/hadoop/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-1.2-api/2.6.2/log4j-1.2-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar:/home/hadoop/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/hadoop/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/hadoop/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/hadoop/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/hadoop/repository/org/apache/commons/commons-compress/1.9/commons-compress-1.9.jar:/home/hadoop/repository/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar:/home/hadoop/repository/org/apache/calcite/calcite-druid/1.10.0/calcite-druid-1.10.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica/1.8.0/avatica-1.8.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica-metrics/1.8.0/avatica-metrics-1.8.0.jar:/home/hadoop/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/hadoop/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-client/2.7.3/hadoop-client-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-common/2.7.3/hadoop-common-2.7.3.jar:/home/hadoop/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/hadoop/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/hadoop/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-auth/2.7.3/hadoop-auth-2.7.3.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/hadoop/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/hadoop/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-hdfs/2.7.3/hadoop-hdfs-2.7.3.jar:/home/hadoop/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/hadoop/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.3/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.3/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.3/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.3/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.3/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-annotations/2.7.3/hadoop-annotations-2.7.3.jar:/home/hadoop/repository/mysql/mysql-connector-java/5.1.35/mysql-connector-java-5.1.35.jar:/home/hadoop/repository/com/google/guava/guava/26.0-jre/guava-26.0-jre.jar:/home/hadoop/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/home/hadoop/repository/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar:/home/hadoop/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/hadoop/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/hadoop/scala/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-reflect.jar:/home/hadoop/scala/lib/scala-actors-migration_2.11-1.1.0.jar:/home/hadoop/scala/lib/scala-xml_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-library.jar:/home/hadoop/scala/lib/scala-swing_2.11-1.0.2.jar:/home/hadoop/scala/lib/scala-actors-2.11.0.jar:/home/hadoop/idea/lib/idea_rt.jar com.xh.spark.sql.function.WindowFunctionTestConnected to the target VM, address: '127.0.0.1:45315', transport: 'socket'SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties19/04/12 16:47:02 INFO SparkContext: Running Spark version 2.4.119/04/12 16:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/04/12 16:47:04 INFO SparkContext: Submitted application: WindowFunctionTest19/04/12 16:47:05 INFO SecurityManager: Changing view acls to: hadoop19/04/12 16:47:05 INFO SecurityManager: Changing modify acls to: hadoop19/04/12 16:47:05 INFO SecurityManager: Changing view acls groups to: 19/04/12 16:47:05 INFO SecurityManager: Changing modify acls groups to: 19/04/12 16:47:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set()19/04/12 16:47:07 INFO Utils: Successfully started service 'sparkDriver' on port 43568.19/04/12 16:47:07 INFO SparkEnv: Registering MapOutputTracker19/04/12 16:47:07 INFO SparkEnv: Registering BlockManagerMaster19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up19/04/12 16:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07913749-f493-4bae-95aa-f9ea019dde5119/04/12 16:47:07 INFO MemoryStore: MemoryStore started with capacity 447.3 MB19/04/12 16:47:07 INFO SparkEnv: Registering OutputCommitCoordinator19/04/12 16:47:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.19/04/12 16:47:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop04:404019/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT.jar with timestamp 155505882868719/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 155505882870319/04/12 16:47:13 INFO Client: Requesting a new application from cluster with 3 NodeManagers19/04/12 16:47:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)19/04/12 16:47:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead19/04/12 16:47:13 INFO Client: Setting up container launch context for our AM19/04/12 16:47:13 INFO Client: Setting up the launch environment for our AM container19/04/12 16:47:14 INFO Client: Preparing resources for our AM container19/04/12 16:47:14 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.19/04/12 16:47:23 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_libs__8176900855339303501.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_libs__8176900855339303501.zip19/04/12 16:47:50 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_conf__4747135480716905311.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_conf__.zip19/04/12 16:47:51 INFO SecurityManager: Changing view acls to: hadoop19/04/12 16:47:51 INFO SecurityManager: Changing modify acls to: hadoop19/04/12 16:47:51 INFO SecurityManager: Changing view acls groups to: 19/04/12 16:47:51 INFO SecurityManager: Changing modify acls groups to: 19/04/12 16:47:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set()19/04/12 16:47:55 INFO Client: Submitting application application_1555049463744_0005 to ResourceManager19/04/12 16:47:55 INFO YarnClientImpl: Submitted application application_1555049463744_000519/04/12 16:47:55 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1555049463744_0005 and attemptId None19/04/12 16:47:57 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:47:57 INFO Client: client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.hadoop start time: 1555058875812 final status: UNDEFINED tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/ user: hadoop19/04/12 16:47:58 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:47:59 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:00 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:01 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:02 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:03 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:04 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:05 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:06 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:07 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:08 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:09 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:10 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:11 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:12 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:13 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:14 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:15 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:16 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:17 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:18 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:19 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)19/04/12 16:48:20 INFO Client: Application report for application_1555049463744_0005 (state: RUNNING)19/04/12 16:48:20 INFO Client: client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.8.82 ApplicationMaster RPC port: -1 queue: root.hadoop start time: 1555058875812 final status: UNDEFINED tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/ user: hadoop19/04/12 16:48:20 INFO YarnClientSchedulerBackend: Application application_1555049463744_0005 has started running.19/04/12 16:48:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42715.19/04/12 16:48:21 INFO NettyBlockTransferService: Server created on hadoop04:4271519/04/12 16:48:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy19/04/12 16:48:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop04, 42715, None)19/04/12 16:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:42715 with 447.3 MB RAM, BlockManagerId(driver, hadoop04, 42715, None)19/04/12 16:48:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop04, 42715, None)19/04/12 16:48:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop04, 42715, None)19/04/12 16:48:23 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)19/04/12 16:48:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; 192.168.8.82, PROXY_URI_BASES -&gt; http://192.168.8.82:8089/proxy/application_1555049463744_0005, RM_HA_URLS -&gt; hadoop02:8088,hadoop03:8088), /proxy/application_1555049463744_000519/04/12 16:48:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill, /metrics/json.19/04/12 16:48:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)19/04/12 16:48:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/worker/sparklearning/spark-warehouse').19/04/12 16:48:32 INFO SharedState: Warehouse path is 'file:/home/hadoop/worker/sparklearning/spark-warehouse'.19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.19/04/12 16:48:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint19/04/12 16:48:55 INFO CodeGenerator: Code generated in 2751.822246 ms19/04/12 16:52:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.83:53042) with ID 119/04/12 16:52:30 INFO BlockManagerMasterEndpoint: Registering block manager hadoop03:45104 with 366.3 MB RAM, BlockManagerId(1, hadoop03, 45104, None)19/04/12 16:52:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.84:37146) with ID 219/04/12 16:52:39 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:38267 with 366.3 MB RAM, BlockManagerId(2, hadoop04, 38267, None)19/04/12 16:53:38 INFO CodeGenerator: Code generated in 668.087005 ms19/04/12 16:53:45 INFO CodeGenerator: Code generated in 732.99189 ms19/04/12 16:53:47 INFO CodeGenerator: Code generated in 1265.744172 ms19/04/12 16:53:47 INFO CodeGenerator: Code generated in 271.771791 ms19/04/12 16:53:51 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:4519/04/12 16:53:51 INFO DAGScheduler: Registering RDD 2 (show at WindowFunctionTest.scala:45)19/04/12 16:53:51 INFO DAGScheduler: Got job 0 (show at WindowFunctionTest.scala:45) with 1 output partitions19/04/12 16:53:51 INFO DAGScheduler: Final stage: ResultStage 1 (show at WindowFunctionTest.scala:45)19/04/12 16:53:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)19/04/12 16:53:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)19/04/12 16:53:51 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:53:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.1 KB, free 447.3 MB)19/04/12 16:53:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KB, free 447.3 MB)19/04/12 16:53:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:42715 (size: 3.1 KB, free: 447.3 MB)19/04/12 16:53:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:116119/04/12 16:53:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0, 1))19/04/12 16:53:55 INFO YarnScheduler: Adding task set 0.0 with 2 tasks19/04/12 16:53:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 8230 bytes)19/04/12 16:53:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 8230 bytes)19/04/12 16:55:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop03:45104 (size: 3.1 KB, free: 366.3 MB)19/04/12 16:55:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:38267 (size: 3.1 KB, free: 366.3 MB)19/04/12 16:56:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 137230 ms on hadoop03 (executor 1) (1/2)19/04/12 16:56:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 147247 ms on hadoop04 (executor 2) (2/2)19/04/12 16:56:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 19/04/12 16:56:24 INFO DAGScheduler: ShuffleMapStage 0 (show at WindowFunctionTest.scala:45) finished in 151.743 s19/04/12 16:56:24 INFO DAGScheduler: looking for newly runnable stages19/04/12 16:56:24 INFO DAGScheduler: running: Set()19/04/12 16:56:24 INFO DAGScheduler: waiting: Set(ResultStage 1)19/04/12 16:56:24 INFO DAGScheduler: failed: Set()19/04/12 16:56:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:56:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.4 KB, free 447.3 MB)19/04/12 16:56:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.3 MB)19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)19/04/12 16:56:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:116119/04/12 16:56:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0))19/04/12 16:56:25 INFO YarnScheduler: Adding task set 1.0 with 1 tasks19/04/12 16:56:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.83:5304219/04/12 16:56:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2345 ms on hadoop03 (executor 1) (1/1)19/04/12 16:56:27 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 19/04/12 16:56:27 INFO DAGScheduler: ResultStage 1 (show at WindowFunctionTest.scala:45) finished in 3.127 s19/04/12 16:56:27 INFO DAGScheduler: Job 0 finished: show at WindowFunctionTest.scala:45, took 156.360095 s19/04/12 16:56:28 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:4519/04/12 16:56:28 INFO DAGScheduler: Got job 1 (show at WindowFunctionTest.scala:45) with 4 output partitions19/04/12 16:56:28 INFO DAGScheduler: Final stage: ResultStage 3 (show at WindowFunctionTest.scala:45)19/04/12 16:56:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)19/04/12 16:56:28 INFO DAGScheduler: Missing parents: List()19/04/12 16:56:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)19/04/12 16:56:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:116119/04/12 16:56:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(1, 2, 3, 4))19/04/12 16:56:28 INFO YarnScheduler: Adding task set 3.0 with 4 tasks19/04/12 16:56:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:28 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, hadoop03, executor 1, partition 2, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:29 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5, hadoop03, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:29 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 447 ms on hadoop03 (executor 1) (1/4)19/04/12 16:56:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6, hadoop03, executor 1, partition 4, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:29 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 290 ms on hadoop03 (executor 1) (2/4)19/04/12 16:56:29 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 334 ms on hadoop03 (executor 1) (3/4)19/04/12 16:56:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.84:3714619/04/12 16:56:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3241 ms on hadoop04 (executor 2) (4/4)19/04/12 16:56:31 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 19/04/12 16:56:31 INFO DAGScheduler: ResultStage 3 (show at WindowFunctionTest.scala:45) finished in 3.803 s19/04/12 16:56:31 INFO DAGScheduler: Job 1 finished: show at WindowFunctionTest.scala:45, took 3.848347 s19/04/12 16:56:32 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:4519/04/12 16:56:32 INFO DAGScheduler: Got job 2 (show at WindowFunctionTest.scala:45) with 20 output partitions19/04/12 16:56:32 INFO DAGScheduler: Final stage: ResultStage 5 (show at WindowFunctionTest.scala:45)19/04/12 16:56:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)19/04/12 16:56:32 INFO DAGScheduler: Missing parents: List()19/04/12 16:56:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)19/04/12 16:56:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:116119/04/12 16:56:32 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))19/04/12 16:56:32 INFO YarnScheduler: Adding task set 5.0 with 20 tasks19/04/12 16:56:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7, hadoop03, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:32 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, hadoop04, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:33 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 9, hadoop04, executor 2, partition 7, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:33 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 726 ms on hadoop04 (executor 2) (1/20)19/04/12 16:56:33 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 10, hadoop03, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 811 ms on hadoop03 (executor 1) (2/20)19/04/12 16:56:33 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 11, hadoop03, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:33 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 10) in 393 ms on hadoop03 (executor 1) (3/20)19/04/12 16:56:33 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 12, hadoop04, executor 2, partition 10, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:33 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 9) in 523 ms on hadoop04 (executor 2) (4/20)19/04/12 16:56:33 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 13, hadoop03, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:33 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 11) in 317 ms on hadoop03 (executor 1) (5/20)19/04/12 16:56:34 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 14, hadoop03, executor 1, partition 12, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:34 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 15, hadoop03, executor 1, partition 13, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:34 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 13) in 851 ms on hadoop03 (executor 1) (6/20)19/04/12 16:56:34 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 14) in 541 ms on hadoop03 (executor 1) (7/20)19/04/12 16:56:34 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 16, hadoop04, executor 2, partition 14, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:34 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 12) in 1219 ms on hadoop04 (executor 2) (8/20)19/04/12 16:56:34 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 17, hadoop04, executor 2, partition 15, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:34 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 18, hadoop03, executor 1, partition 16, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:34 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 16) in 313 ms on hadoop04 (executor 2) (9/20)19/04/12 16:56:34 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 15) in 596 ms on hadoop03 (executor 1) (10/20)19/04/12 16:56:35 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 19, hadoop04, executor 2, partition 17, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 20, hadoop03, executor 1, partition 18, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 18) in 158 ms on hadoop03 (executor 1) (11/20)19/04/12 16:56:35 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 17) in 167 ms on hadoop04 (executor 2) (12/20)19/04/12 16:56:35 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 21, hadoop03, executor 1, partition 19, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 22, hadoop04, executor 2, partition 20, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 20) in 250 ms on hadoop03 (executor 1) (13/20)19/04/12 16:56:35 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 19) in 343 ms on hadoop04 (executor 2) (14/20)19/04/12 16:56:35 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 23, hadoop04, executor 2, partition 21, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 22) in 247 ms on hadoop04 (executor 2) (15/20)19/04/12 16:56:35 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 24, hadoop03, executor 1, partition 22, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 21) in 481 ms on hadoop03 (executor 1) (16/20)19/04/12 16:56:35 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 25, hadoop04, executor 2, partition 23, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:35 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 23) in 393 ms on hadoop04 (executor 2) (17/20)19/04/12 16:56:36 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 26, hadoop04, executor 2, partition 24, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:36 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 25) in 473 ms on hadoop04 (executor 2) (18/20)19/04/12 16:56:36 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 24) in 898 ms on hadoop03 (executor 1) (19/20)19/04/12 16:56:36 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 26) in 524 ms on hadoop04 (executor 2) (20/20)19/04/12 16:56:36 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 19/04/12 16:56:36 INFO DAGScheduler: ResultStage 5 (show at WindowFunctionTest.scala:45) finished in 4.493 s19/04/12 16:56:36 INFO DAGScheduler: Job 2 finished: show at WindowFunctionTest.scala:45, took 4.566402 s19/04/12 16:56:37 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:4519/04/12 16:56:37 INFO DAGScheduler: Got job 3 (show at WindowFunctionTest.scala:45) with 100 output partitions19/04/12 16:56:37 INFO DAGScheduler: Final stage: ResultStage 7 (show at WindowFunctionTest.scala:45)19/04/12 16:56:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)19/04/12 16:56:37 INFO DAGScheduler: Missing parents: List()19/04/12 16:56:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)19/04/12 16:56:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:116119/04/12 16:56:37 INFO DAGScheduler: Submitting 100 missing tasks from ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39))19/04/12 16:56:37 INFO YarnScheduler: Adding task set 7.0 with 100 tasks19/04/12 16:56:37 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 27, hadoop04, executor 2, partition 110, NODE_LOCAL, 7778 bytes)19/04/12 16:56:37 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 28, hadoop03, executor 1, partition 102, NODE_LOCAL, 7778 bytes)19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:56:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 29, hadoop04, executor 2, partition 25, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:39 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 27) in 1607 ms on hadoop04 (executor 2) (1/100)19/04/12 16:56:39 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 30, hadoop04, executor 2, partition 26, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:40 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 31, hadoop04, executor 2, partition 27, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 29) in 1083 ms on hadoop04 (executor 2) (2/100)19/04/12 16:56:40 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 30) in 659 ms on hadoop04 (executor 2) (3/100)19/04/12 16:56:40 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 32, hadoop04, executor 2, partition 28, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:40 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 31) in 473 ms on hadoop04 (executor 2) (4/100)19/04/12 16:56:40 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 33, hadoop04, executor 2, partition 29, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 34, hadoop03, executor 1, partition 30, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 28) in 3578 ms on hadoop03 (executor 1) (5/100)19/04/12 16:56:41 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 32) in 505 ms on hadoop04 (executor 2) (6/100)19/04/12 16:56:41 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 35, hadoop04, executor 2, partition 31, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 33) in 438 ms on hadoop04 (executor 2) (7/100)19/04/12 16:56:41 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 36, hadoop03, executor 1, partition 32, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 37, hadoop04, executor 2, partition 33, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 35) in 353 ms on hadoop04 (executor 2) (8/100)19/04/12 16:56:41 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 34) in 583 ms on hadoop03 (executor 1) (9/100)19/04/12 16:56:41 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 38, hadoop03, executor 1, partition 34, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:41 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 36) in 301 ms on hadoop03 (executor 1) (10/100)19/04/12 16:56:42 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 39, hadoop03, executor 1, partition 35, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:42 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 40, hadoop04, executor 2, partition 36, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:42 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 38) in 453 ms on hadoop03 (executor 1) (11/100)19/04/12 16:56:42 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 41, hadoop04, executor 2, partition 37, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:42 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 37) in 1017 ms on hadoop04 (executor 2) (12/100)19/04/12 16:56:42 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 42, hadoop03, executor 1, partition 38, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:42 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 39) in 407 ms on hadoop03 (executor 1) (13/100)19/04/12 16:56:42 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 40) in 486 ms on hadoop04 (executor 2) (14/100)19/04/12 16:56:42 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 43, hadoop04, executor 2, partition 39, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:42 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 41) in 259 ms on hadoop04 (executor 2) (15/100)19/04/12 16:56:43 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 44, hadoop04, executor 2, partition 40, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:43 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 43) in 472 ms on hadoop04 (executor 2) (16/100)19/04/12 16:56:43 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 45, hadoop03, executor 1, partition 41, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:43 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 42) in 964 ms on hadoop03 (executor 1) (17/100)19/04/12 16:56:45 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 46, hadoop03, executor 1, partition 42, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:46 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 47, hadoop04, executor 2, partition 43, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:46 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 44) in 2989 ms on hadoop04 (executor 2) (18/100)19/04/12 16:56:46 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 45) in 3210 ms on hadoop03 (executor 1) (19/100)19/04/12 16:56:46 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 48, hadoop03, executor 1, partition 44, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:46 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 46) in 1507 ms on hadoop03 (executor 1) (20/100)19/04/12 16:56:47 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 49, hadoop04, executor 2, partition 45, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:47 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 47) in 1533 ms on hadoop04 (executor 2) (21/100)19/04/12 16:56:48 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 50, hadoop04, executor 2, partition 46, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:48 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 49) in 715 ms on hadoop04 (executor 2) (22/100)19/04/12 16:56:48 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 51, hadoop03, executor 1, partition 47, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:49 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 48) in 2132 ms on hadoop03 (executor 1) (23/100)19/04/12 16:56:49 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 52, hadoop04, executor 2, partition 48, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:49 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 50) in 1332 ms on hadoop04 (executor 2) (24/100)19/04/12 16:56:49 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 53, hadoop03, executor 1, partition 49, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:49 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 51) in 776 ms on hadoop03 (executor 1) (25/100)19/04/12 16:56:50 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 54, hadoop04, executor 2, partition 50, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:50 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 52) in 1066 ms on hadoop04 (executor 2) (26/100)19/04/12 16:56:50 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 55, hadoop03, executor 1, partition 51, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:50 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 53) in 698 ms on hadoop03 (executor 1) (27/100)19/04/12 16:56:50 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 56, hadoop04, executor 2, partition 52, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:50 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 54) in 679 ms on hadoop04 (executor 2) (28/100)19/04/12 16:56:50 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 57, hadoop03, executor 1, partition 53, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:51 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 55) in 1082 ms on hadoop03 (executor 1) (29/100)19/04/12 16:56:51 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 58, hadoop04, executor 2, partition 54, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:51 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 56) in 731 ms on hadoop04 (executor 2) (30/100)19/04/12 16:56:51 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 59, hadoop03, executor 1, partition 55, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:51 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 57) in 592 ms on hadoop03 (executor 1) (31/100)19/04/12 16:56:51 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 60, hadoop04, executor 2, partition 56, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:51 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 58) in 557 ms on hadoop04 (executor 2) (32/100)19/04/12 16:56:51 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 61, hadoop03, executor 1, partition 57, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:51 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 59) in 468 ms on hadoop03 (executor 1) (33/100)19/04/12 16:56:51 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 62, hadoop04, executor 2, partition 58, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:52 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 60) in 241 ms on hadoop04 (executor 2) (34/100)19/04/12 16:56:52 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 63, hadoop04, executor 2, partition 59, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:52 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 62) in 728 ms on hadoop04 (executor 2) (35/100)19/04/12 16:56:52 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 64, hadoop03, executor 1, partition 60, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:52 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 61) in 804 ms on hadoop03 (executor 1) (36/100)19/04/12 16:56:52 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 65, hadoop04, executor 2, partition 61, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:52 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 63) in 392 ms on hadoop04 (executor 2) (37/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 66, hadoop03, executor 1, partition 62, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 64) in 320 ms on hadoop03 (executor 1) (38/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 67, hadoop04, executor 2, partition 63, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 65) in 154 ms on hadoop04 (executor 2) (39/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 68, hadoop03, executor 1, partition 64, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 66) in 297 ms on hadoop03 (executor 1) (40/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 69, hadoop03, executor 1, partition 65, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 68) in 463 ms on hadoop03 (executor 1) (41/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 70, hadoop04, executor 2, partition 66, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 67) in 672 ms on hadoop04 (executor 2) (42/100)19/04/12 16:56:53 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 71, hadoop03, executor 1, partition 67, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:53 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 69) in 165 ms on hadoop03 (executor 1) (43/100)19/04/12 16:56:55 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 72, hadoop03, executor 1, partition 68, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:55 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 71) in 1656 ms on hadoop03 (executor 1) (44/100)19/04/12 16:56:55 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 73, hadoop03, executor 1, partition 69, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:55 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 72) in 534 ms on hadoop03 (executor 1) (45/100)19/04/12 16:56:56 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 74, hadoop03, executor 1, partition 70, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:56 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 73) in 376 ms on hadoop03 (executor 1) (46/100)19/04/12 16:56:56 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 75, hadoop04, executor 2, partition 71, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:56 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 70) in 2844 ms on hadoop04 (executor 2) (47/100)19/04/12 16:56:56 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 76, hadoop03, executor 1, partition 72, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:56 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 74) in 444 ms on hadoop03 (executor 1) (48/100)19/04/12 16:56:56 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 77, hadoop03, executor 1, partition 73, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:56 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 76) in 341 ms on hadoop03 (executor 1) (49/100)19/04/12 16:56:57 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 78, hadoop03, executor 1, partition 74, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:57 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 77) in 169 ms on hadoop03 (executor 1) (50/100)19/04/12 16:56:57 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 79, hadoop03, executor 1, partition 75, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:57 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 78) in 193 ms on hadoop03 (executor 1) (51/100)19/04/12 16:56:57 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 80, hadoop04, executor 2, partition 76, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:57 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 75) in 1174 ms on hadoop04 (executor 2) (52/100)19/04/12 16:56:57 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 81, hadoop03, executor 1, partition 77, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:57 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 82, hadoop04, executor 2, partition 78, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:57 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 79) in 337 ms on hadoop03 (executor 1) (53/100)19/04/12 16:56:57 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 80) in 310 ms on hadoop04 (executor 2) (54/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 83, hadoop03, executor 1, partition 79, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 81) in 761 ms on hadoop03 (executor 1) (55/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 84, hadoop04, executor 2, partition 80, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 82) in 696 ms on hadoop04 (executor 2) (56/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 85, hadoop03, executor 1, partition 81, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 83) in 691 ms on hadoop03 (executor 1) (57/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 86, hadoop03, executor 1, partition 82, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 85) in 118 ms on hadoop03 (executor 1) (58/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 87, hadoop04, executor 2, partition 83, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 84) in 447 ms on hadoop04 (executor 2) (59/100)19/04/12 16:56:58 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 88, hadoop03, executor 1, partition 84, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:58 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 86) in 240 ms on hadoop03 (executor 1) (60/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 89, hadoop04, executor 2, partition 85, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 87) in 325 ms on hadoop04 (executor 2) (61/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 90, hadoop03, executor 1, partition 86, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 88) in 255 ms on hadoop03 (executor 1) (62/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 91, hadoop04, executor 2, partition 87, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 89) in 147 ms on hadoop04 (executor 2) (63/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 92, hadoop03, executor 1, partition 88, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 90) in 130 ms on hadoop03 (executor 1) (64/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 93, hadoop03, executor 1, partition 89, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 92) in 110 ms on hadoop03 (executor 1) (65/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 94, hadoop04, executor 2, partition 90, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 91) in 264 ms on hadoop04 (executor 2) (66/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 95, hadoop03, executor 1, partition 91, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 93) in 165 ms on hadoop03 (executor 1) (67/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 96, hadoop04, executor 2, partition 92, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 94) in 267 ms on hadoop04 (executor 2) (68/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 97, hadoop03, executor 1, partition 93, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 95) in 222 ms on hadoop03 (executor 1) (69/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 98, hadoop04, executor 2, partition 94, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 96) in 191 ms on hadoop04 (executor 2) (70/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 99, hadoop03, executor 1, partition 95, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 97) in 173 ms on hadoop03 (executor 1) (71/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 100, hadoop04, executor 2, partition 96, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 98) in 165 ms on hadoop04 (executor 2) (72/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 101, hadoop03, executor 1, partition 97, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 99) in 181 ms on hadoop03 (executor 1) (73/100)19/04/12 16:56:59 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 102, hadoop04, executor 2, partition 98, PROCESS_LOCAL, 7778 bytes)19/04/12 16:56:59 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 100) in 176 ms on hadoop04 (executor 2) (74/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 103, hadoop04, executor 2, partition 99, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 102) in 103 ms on hadoop04 (executor 2) (75/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 104, hadoop03, executor 1, partition 100, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 101) in 202 ms on hadoop03 (executor 1) (76/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 105, hadoop04, executor 2, partition 101, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 103) in 159 ms on hadoop04 (executor 2) (77/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 106, hadoop04, executor 2, partition 103, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 105) in 378 ms on hadoop04 (executor 2) (78/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 107, hadoop03, executor 1, partition 104, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 104) in 551 ms on hadoop03 (executor 1) (79/100)19/04/12 16:57:00 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 108, hadoop04, executor 2, partition 105, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:00 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 106) in 437 ms on hadoop04 (executor 2) (80/100)19/04/12 16:57:01 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 109, hadoop03, executor 1, partition 106, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:01 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 107) in 621 ms on hadoop03 (executor 1) (81/100)19/04/12 16:57:01 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 110, hadoop04, executor 2, partition 107, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:01 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 108) in 1254 ms on hadoop04 (executor 2) (82/100)19/04/12 16:57:02 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 111, hadoop03, executor 1, partition 108, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:02 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 109) in 1316 ms on hadoop03 (executor 1) (83/100)19/04/12 16:57:02 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 112, hadoop04, executor 2, partition 109, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:02 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 110) in 793 ms on hadoop04 (executor 2) (84/100)19/04/12 16:57:02 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 113, hadoop03, executor 1, partition 111, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:02 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 111) in 780 ms on hadoop03 (executor 1) (85/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 114, hadoop04, executor 2, partition 112, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 112) in 567 ms on hadoop04 (executor 2) (86/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 115, hadoop03, executor 1, partition 113, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 113) in 689 ms on hadoop03 (executor 1) (87/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 116, hadoop04, executor 2, partition 114, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 114) in 513 ms on hadoop04 (executor 2) (88/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 117, hadoop03, executor 1, partition 115, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 115) in 432 ms on hadoop03 (executor 1) (89/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 118, hadoop04, executor 2, partition 116, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 116) in 271 ms on hadoop04 (executor 2) (90/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 119, hadoop03, executor 1, partition 117, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 120, hadoop04, executor 2, partition 118, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 118) in 129 ms on hadoop04 (executor 2) (91/100)19/04/12 16:57:03 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 117) in 173 ms on hadoop03 (executor 1) (92/100)19/04/12 16:57:03 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 121, hadoop04, executor 2, partition 119, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:03 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 120) in 139 ms on hadoop04 (executor 2) (93/100)19/04/12 16:57:04 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 122, hadoop03, executor 1, partition 120, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:04 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 119) in 232 ms on hadoop03 (executor 1) (94/100)19/04/12 16:57:04 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 123, hadoop04, executor 2, partition 121, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:04 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 121) in 110 ms on hadoop04 (executor 2) (95/100)19/04/12 16:57:04 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 124, hadoop03, executor 1, partition 122, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:04 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 122) in 115 ms on hadoop03 (executor 1) (96/100)19/04/12 16:57:04 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 125, hadoop04, executor 2, partition 123, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:04 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 123) in 136 ms on hadoop04 (executor 2) (97/100)19/04/12 16:57:04 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 126, hadoop03, executor 1, partition 124, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:04 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 124) in 124 ms on hadoop03 (executor 1) (98/100)19/04/12 16:57:04 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 125) in 140 ms on hadoop04 (executor 2) (99/100)19/04/12 16:57:04 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 126) in 145 ms on hadoop03 (executor 1) (100/100)19/04/12 16:57:04 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 19/04/12 16:57:04 INFO DAGScheduler: ResultStage 7 (show at WindowFunctionTest.scala:45) finished in 26.819 s19/04/12 16:57:04 INFO DAGScheduler: Job 3 finished: show at WindowFunctionTest.scala:45, took 27.209729 s19/04/12 16:57:05 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:4519/04/12 16:57:05 INFO DAGScheduler: Got job 4 (show at WindowFunctionTest.scala:45) with 75 output partitions19/04/12 16:57:05 INFO DAGScheduler: Final stage: ResultStage 9 (show at WindowFunctionTest.scala:45)19/04/12 16:57:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)19/04/12 16:57:05 INFO DAGScheduler: Missing parents: List()19/04/12 16:57:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)19/04/12 16:57:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:116119/04/12 16:57:05 INFO DAGScheduler: Submitting 75 missing tasks from ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139))19/04/12 16:57:05 INFO YarnScheduler: Adding task set 9.0 with 75 tasks19/04/12 16:57:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 127, hadoop04, executor 2, partition 125, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:05 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 128, hadoop03, executor 1, partition 126, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:57:06 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 129, hadoop04, executor 2, partition 127, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 127) in 398 ms on hadoop04 (executor 2) (1/75)19/04/12 16:57:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)19/04/12 16:57:06 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 130, hadoop04, executor 2, partition 128, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 129) in 259 ms on hadoop04 (executor 2) (2/75)19/04/12 16:57:06 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 131, hadoop03, executor 1, partition 129, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 128) in 796 ms on hadoop03 (executor 1) (3/75)19/04/12 16:57:06 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 132, hadoop04, executor 2, partition 130, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 130) in 277 ms on hadoop04 (executor 2) (4/75)19/04/12 16:57:06 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 133, hadoop03, executor 1, partition 131, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 131) in 188 ms on hadoop03 (executor 1) (5/75)19/04/12 16:57:06 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 134, hadoop03, executor 1, partition 132, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 133) in 173 ms on hadoop03 (executor 1) (6/75)19/04/12 16:57:06 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 135, hadoop04, executor 2, partition 133, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:06 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 132) in 289 ms on hadoop04 (executor 2) (7/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 136, hadoop03, executor 1, partition 134, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 134) in 206 ms on hadoop03 (executor 1) (8/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 137, hadoop04, executor 2, partition 135, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 135) in 187 ms on hadoop04 (executor 2) (9/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 138, hadoop03, executor 1, partition 136, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 139, hadoop04, executor 2, partition 137, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 136) in 218 ms on hadoop03 (executor 1) (10/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 140, hadoop03, executor 1, partition 138, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 138) in 102 ms on hadoop03 (executor 1) (11/75)19/04/12 16:57:07 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 137) in 232 ms on hadoop04 (executor 2) (12/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 141, hadoop03, executor 1, partition 139, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 140) in 96 ms on hadoop03 (executor 1) (13/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 142, hadoop04, executor 2, partition 140, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 139) in 200 ms on hadoop04 (executor 2) (14/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 143, hadoop03, executor 1, partition 141, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 141) in 156 ms on hadoop03 (executor 1) (15/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 144, hadoop03, executor 1, partition 142, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 143) in 234 ms on hadoop03 (executor 1) (16/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 145, hadoop04, executor 2, partition 143, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 142) in 453 ms on hadoop04 (executor 2) (17/75)19/04/12 16:57:07 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 146, hadoop03, executor 1, partition 144, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:07 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 144) in 273 ms on hadoop03 (executor 1) (18/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 147, hadoop04, executor 2, partition 145, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 145) in 220 ms on hadoop04 (executor 2) (19/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 148, hadoop03, executor 1, partition 146, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 146) in 178 ms on hadoop03 (executor 1) (20/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 149, hadoop04, executor 2, partition 147, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 147) in 313 ms on hadoop04 (executor 2) (21/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 150, hadoop03, executor 1, partition 148, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 148) in 279 ms on hadoop03 (executor 1) (22/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 151, hadoop04, executor 2, partition 149, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 149) in 371 ms on hadoop04 (executor 2) (23/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 152, hadoop03, executor 1, partition 150, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 150) in 291 ms on hadoop03 (executor 1) (24/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 153, hadoop04, executor 2, partition 151, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 151) in 321 ms on hadoop04 (executor 2) (25/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 154, hadoop03, executor 1, partition 152, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:08 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 152) in 385 ms on hadoop03 (executor 1) (26/75)19/04/12 16:57:08 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 155, hadoop03, executor 1, partition 153, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 156, hadoop04, executor 2, partition 154, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 153) in 386 ms on hadoop04 (executor 2) (27/75)19/04/12 16:57:09 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 154) in 332 ms on hadoop03 (executor 1) (28/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 157, hadoop03, executor 1, partition 155, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 155) in 199 ms on hadoop03 (executor 1) (29/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 158, hadoop04, executor 2, partition 156, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 156) in 303 ms on hadoop04 (executor 2) (30/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 159, hadoop04, executor 2, partition 157, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 158) in 94 ms on hadoop04 (executor 2) (31/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 160, hadoop03, executor 1, partition 158, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 157) in 276 ms on hadoop03 (executor 1) (32/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 161, hadoop04, executor 2, partition 159, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 159) in 171 ms on hadoop04 (executor 2) (33/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 162, hadoop03, executor 1, partition 160, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 160) in 182 ms on hadoop03 (executor 1) (34/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 163, hadoop03, executor 1, partition 161, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 164, hadoop04, executor 2, partition 162, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 165, hadoop04, executor 2, partition 163, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 164) in 52 ms on hadoop04 (executor 2) (35/75)19/04/12 16:57:09 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 161) in 191 ms on hadoop04 (executor 2) (36/75)19/04/12 16:57:09 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 162) in 206 ms on hadoop03 (executor 1) (37/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 166, hadoop03, executor 1, partition 164, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 167, hadoop04, executor 2, partition 165, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 165) in 108 ms on hadoop04 (executor 2) (38/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 168, hadoop04, executor 2, partition 166, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 163) in 218 ms on hadoop03 (executor 1) (39/75)19/04/12 16:57:09 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 167) in 88 ms on hadoop04 (executor 2) (40/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 169, hadoop03, executor 1, partition 167, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 166) in 112 ms on hadoop03 (executor 1) (41/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 170, hadoop03, executor 1, partition 168, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 169) in 75 ms on hadoop03 (executor 1) (42/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 171, hadoop04, executor 2, partition 169, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 168) in 162 ms on hadoop04 (executor 2) (43/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 172, hadoop03, executor 1, partition 170, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 173, hadoop04, executor 2, partition 171, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 170) in 96 ms on hadoop03 (executor 1) (44/75)19/04/12 16:57:09 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 174, hadoop03, executor 1, partition 172, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:09 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 171) in 75 ms on hadoop04 (executor 2) (45/75)19/04/12 16:57:09 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 172) in 49 ms on hadoop03 (executor 1) (46/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 175, hadoop03, executor 1, partition 173, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 174) in 85 ms on hadoop03 (executor 1) (47/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 176, hadoop04, executor 2, partition 174, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 177, hadoop03, executor 1, partition 175, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 178, hadoop03, executor 1, partition 176, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 177) in 38 ms on hadoop03 (executor 1) (48/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 173) in 141 ms on hadoop04 (executor 2) (49/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 179, hadoop03, executor 1, partition 177, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 175) in 123 ms on hadoop03 (executor 1) (50/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 178) in 57 ms on hadoop03 (executor 1) (51/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 180, hadoop04, executor 2, partition 178, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 176) in 114 ms on hadoop04 (executor 2) (52/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 181, hadoop03, executor 1, partition 179, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 182, hadoop03, executor 1, partition 180, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 183, hadoop04, executor 2, partition 181, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 179) in 159 ms on hadoop03 (executor 1) (53/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 184, hadoop03, executor 1, partition 182, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 181) in 110 ms on hadoop03 (executor 1) (54/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 185, hadoop04, executor 2, partition 183, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 180) in 165 ms on hadoop04 (executor 2) (55/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 182) in 262 ms on hadoop03 (executor 1) (56/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 186, hadoop03, executor 1, partition 184, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 187, hadoop04, executor 2, partition 185, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 183) in 331 ms on hadoop04 (executor 2) (57/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 184) in 301 ms on hadoop03 (executor 1) (58/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 185) in 289 ms on hadoop04 (executor 2) (59/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 188, hadoop03, executor 1, partition 186, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 186) in 151 ms on hadoop03 (executor 1) (60/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 189, hadoop04, executor 2, partition 187, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 187) in 197 ms on hadoop04 (executor 2) (61/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 190, hadoop03, executor 1, partition 188, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 188) in 114 ms on hadoop03 (executor 1) (62/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 191, hadoop04, executor 2, partition 189, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 189) in 148 ms on hadoop04 (executor 2) (63/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 192, hadoop03, executor 1, partition 190, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 190) in 113 ms on hadoop03 (executor 1) (64/75)19/04/12 16:57:10 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 193, hadoop03, executor 1, partition 191, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 194, hadoop04, executor 2, partition 192, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:10 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 191) in 129 ms on hadoop04 (executor 2) (65/75)19/04/12 16:57:10 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 192) in 105 ms on hadoop03 (executor 1) (66/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 195, hadoop03, executor 1, partition 193, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 193) in 126 ms on hadoop03 (executor 1) (67/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 196, hadoop04, executor 2, partition 194, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 194) in 131 ms on hadoop04 (executor 2) (68/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 197, hadoop04, executor 2, partition 195, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 196) in 217 ms on hadoop04 (executor 2) (69/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 198, hadoop03, executor 1, partition 196, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 195) in 301 ms on hadoop03 (executor 1) (70/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 199, hadoop04, executor 2, partition 197, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 197) in 270 ms on hadoop04 (executor 2) (71/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 200, hadoop03, executor 1, partition 198, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 198) in 256 ms on hadoop03 (executor 1) (72/75)19/04/12 16:57:11 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 201, hadoop04, executor 2, partition 199, PROCESS_LOCAL, 7778 bytes)19/04/12 16:57:11 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 199) in 282 ms on hadoop04 (executor 2) (73/75)19/04/12 16:57:11 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 200) in 214 ms on hadoop03 (executor 1) (74/75)19/04/12 16:57:11 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 201) in 104 ms on hadoop04 (executor 2) (75/75)19/04/12 16:57:11 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 19/04/12 16:57:11 INFO DAGScheduler: ResultStage 9 (show at WindowFunctionTest.scala:45) finished in 6.626 s19/04/12 16:57:11 INFO DAGScheduler: Job 4 finished: show at WindowFunctionTest.scala:45, took 6.700226 s19/04/12 16:57:14 INFO SparkUI: Stopped Spark web UI at http://hadoop04:404019/04/12 16:57:14 INFO YarnClientSchedulerBackend: Interrupting monitor thread19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Shutting down all executors19/04/12 16:57:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down19/04/12 16:57:15 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices(serviceOption=None, services=List(), started=false)19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Stopped19/04/12 16:57:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!19/04/12 16:57:17 INFO MemoryStore: MemoryStore cleared19/04/12 16:57:17 INFO BlockManager: BlockManager stopped19/04/12 16:57:17 INFO BlockManagerMaster: BlockManagerMaster stopped19/04/12 16:57:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!19/04/12 16:57:18 INFO SparkContext: Successfully stopped SparkContext19/04/12 16:57:19 INFO ShutdownHookManager: Shutdown hook called19/04/12 16:57:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33+-----+----------+--------+---------+| site| date|user_cnt|MovingAvg|+-----+----------+--------+---------+|站点1|2018-01-01| 50| 47.5||站点1|2018-01-02| 45| 50.0||站点1|2018-01-03| 55| 50.0||站点2|2018-01-01| 25| 27.0||站点2|2018-01-02| 29| 27.0||站点2|2018-01-03| 27| 28.0|+-----+----------+--------+---------+Disconnected from the target VM, address: '127.0.0.1:45315', transport: 'socket'Process finished with exit code 0 至此,IDEA已经能连接上yarn集群。 开启远程yarn client debug调试第一步将我们的应用程序打包(在hadoop05机器)，发送到hadoop01机器，并在hadoop01上启动我们的应用程序,具体如下： 12345mkdir -p /home/hadoop/worker/sparklearning/target/ scp -r sparklearning-1.0-SNAPSHOT.jar hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar scp -r sparklearning-1.0-SNAPSHOT.jar hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar 在hadoop01启动12345678./bin/spark-submit --class com.xh.spark.sql.function.WindowFunctionTest \ --master yarn \ --deploy-mode client \ --driver-memory 512m \ --executor-memory 512m \ --executor-cores 6 \ --driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005" \ /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar 第二步在idea中Edit Configurations ——&gt; 点击+号 ——&gt;Remote —— Configuration ——&gt; Debugger Mode (用Attach to remote JVM)——&gt;host(hadoop01)——&gt; Port(默认5005，也可以选择使用未使用的端口，比如5656) ——&gt; Command line arguments for remote JVN (-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005) 然后点击小虫子图标就可以run程序了！！！！ 问题解决分享Could not parse Master URL解决： 在pom.xml中添加如下依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-yarn_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; Name node is in safe mode解决： 在hadoop所在linux环境输入如下命令1hdfs dfsadmin -safemode leave Exception in thread “main” org.apache.hadoop.security.AccessControlException: Exception in thread “main” org.apache.hadoop.security.AccessControlException: Permission denied: user=deeplearning, access=WRITE, inode=”/user/deeplearning/.sparkStaging/application_1554947367832_0002”:hadoop:supergroup:drwxr-xr-x 解决： 在hdfs-site.xml添加如下配置1234 &lt;property&gt;&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt; Diagnostics: Container [pid=3293,containerID=container_e12_1555047553207_0002_02_000001] is running beyond virtual memory limits. Current usage: 116.1 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container.解决： 在yarn-site.xml文件中添加如下配置12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;5&lt;/value&gt;&lt;/property&gt; 问题解决参考文章https://stackoverflow.com/questions/41054700/could-not-parse-master-url https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml http://www.cnblogs.com/lisi2016/p/6863923.html 致谢!如有遇到问题，将问题发送至本人邮箱(t_spider@aliyun.com)。欢迎大家一起讨论问题！]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql实战案例]]></title>
    <url>%2F2019%2F04%2F10%2Fspark%20sql%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[累计统计准备数据access.csv12345678910111213141516171819A,2015-01,5A,2015-01,15A,2015-01,5A,2015-01,8A,2015-02,4A,2015-02,6A,2015-03,16A,2015-03,22A,2015-04,10A,2015-04,50B,2015-01,5B,2015-01,25B,2015-02,10B,2015-02,5B,2015-03,23B,2015-03,10B,2015-03,1B,2015-04,10B,2015-04,50 准备环境123456789101112131415161718192021object AccumulatorCount &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[*]") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ // 读取数据 val usersDF = spark.read.format("csv") .option("sep", ",") .option("inferSchema", "true") .option("header", "false") .load("src/main/resources/access.csv") .toDF("name", "mounth", "amount") &#125;&#125; 具体实现逻辑DataFrame API 方式方式一:123// rowsBetween(Long.MinValue, 0):窗口的大小是按照排序从最小值到当前行val accuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(Long.MinValue, 0)usersDF.withColumn("acc_amount", sum(usersDF("amount")).over(accuCntSpec)).show() 方式二123456usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(accuCntSpec).as("acc_amount")).show() sql方式思路：根据DF算子意思，找到SqlBase.g4文件，看看是否有该类sql支持。在SqlBase.g4文件中刚好找到如下内容123456789101112windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ;frameBound : UNBOUNDED boundType=(PRECEDING | FOLLOWING) | boundType=CURRENT ROW | expression boundType=(PRECEDING | FOLLOWING) ; 在spark源码sql模块core项目org.apache.spark.sql.execution包中找到SQLWindowFunctionSuite类找到如下测试方法1234567891011121314151617181920212223242526272829303132333435test("window function: multiple window expressions in a single expression") &#123; val nums = sparkContext.parallelize(1 to 10).map(x =&gt; (x, x % 2)).toDF("x", "y") nums.createOrReplaceTempView("nums") val expected = Row(1, 1, 1, 55, 1, 57) :: Row(0, 2, 3, 55, 2, 60) :: Row(1, 3, 6, 55, 4, 65) :: Row(0, 4, 10, 55, 6, 71) :: Row(1, 5, 15, 55, 9, 79) :: Row(0, 6, 21, 55, 12, 88) :: Row(1, 7, 28, 55, 16, 99) :: Row(0, 8, 36, 55, 20, 111) :: Row(1, 9, 45, 55, 25, 125) :: Row(0, 10, 55, 55, 30, 140) :: Nil val actual = sql( """ |SELECT | y, | x, | sum(x) OVER w1 AS running_sum, | sum(x) OVER w2 AS total_sum, | sum(x) OVER w3 AS running_sum_per_y, | ((sum(x) OVER w1) + (sum(x) OVER w2) + (sum(x) OVER w3)) as combined2 |FROM nums |WINDOW w1 AS (ORDER BY x ROWS BETWEEN UnBOUNDED PRECEDiNG AND CuRRENT RoW), | w2 AS (ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOuNDED FoLLOWING), | w3 AS (PARTITION BY y ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) """.stripMargin) checkAnswer(actual, expected) spark.catalog.dropTempView("nums") &#125; 下面就可以开心的照着案例写sql去了，真嗨皮！！！！12345678910usersDF.createOrReplaceTempView("access")spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between unbounded preceding and current row ) as acc_amount |from access | """.stripMargin).show() 累加N天之前,假设N=3DataFrame API方式123456val preThreeAccuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(-3, 0)usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(preThreeAccuCntSpec).as("acc_amount")).show() sql方式123456789spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between 3 preceding and current row) as acc_amount |from access | """.stripMargin).show() 累加前3天，后3天API方式123456val preThreeFiveAccuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(3, 3)usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(preThreeFiveAccuCntSpec).as("acc_amount")) sql方式123456789spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between 3 preceding and 3 following) as acc_amount |from access | """.stripMargin).show() 基本窗口函数案例准备环境12345678910111213141516171819202122232425object WindowFunctionTest extends BaseSparkSession &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[*]") .appName("WindowFunctionTest") .config("spark.some.config.option", "some-value") .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ val df = List( ("位置1", "2018-01-01", 50), ("位置1", "2018-01-02", 45), ("位置1", "2018-01-03", 55), ("位置2", "2018-01-01", 25), ("位置2", "2018-01-02", 29), ("位置2", "2018-01-03", 27) ).toDF("site", "date", "user_cnt") &#125;&#125; 平均移动值DataFrame API方式实现123// 窗口定义从 -1(前一行)到 1(后一行) ，每一个滑动的窗口总用有3行 val movinAvgSpec = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("MovingAvg", avg(df("user_cnt")).over(movinAvgSpec)).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg |from site_info """.stripMargin).show() 前一行数据DataFrame API方式实现12val lagwSpec = Window.partitionBy("site").orderBy("date")df.withColumn("prevUserCnt", lag(df("user_cnt"), 1).over(lagwSpec)).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | lag(user_cnt,1) over(partition by site order by date asc ) as prevUserCnt |from site_info """.stripMargin).show() 排名DataFrame API方式实现12val rankwSpec = Window.partitionBy("site").orderBy("date") df.withColumn("rank", rank().over(rankwSpec)) sql方式12345678spark.sql( """ |select site, | date, | user_cnt, | rank() over(partition by site order by date asc ) as prevUserCnt |from site_info """.stripMargin).show() 分组topn和分组取最小123456789101112131415161718192021222324252627282930313233343536373839import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;Row, SparkSession&#125;object GroupBy &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() val rows = spark.sparkContext.parallelize( List( ("shop2", "2018-02-22", 1), ("shop2", "2018-02-27", 1), ("shop2", "2018-03-13", 1), ("shop2", "2018-03-20", 5), ("shop1", "2018-03-27", 1), ("shop1", "2018-04-03", 1), ("shop1", "2018-04-10", 1), ("shop1", "2018-04-17", 1), ("shop2", "2018-04-28", 1), ("shop2", "2018-04-05", 10), ("shop2", "2018-04-09", 1))) val rowRDD = rows.map(t =&gt; Row(t._1, t._2, t._3)) val schema = StructType( Array( StructField("shop", StringType), StructField("ycd_date", StringType), StructField("ycd_num", IntegerType))) val df = spark.createDataFrame(rowRDD, schema) df.createOrReplaceTempView("ycd_order") // 分组topN val topN = spark.sql("select * from (SELECT o.shop,o.ycd_date, row_number() over (PARTITION BY o.shop ORDER BY o.ycd_date DESC) rank FROM ycd_order as o) o1 where rank &lt; 2") // 根据某一个字段分组,取某一个字段的最小值 val groupMin = spark.sql("select o.shop,min(o.ycd_num) as min_num from ycd_order as o group by o.shop order by min_num ") topN.show() spark.stop() &#125;&#125; 优雅方式定义scheme1234567891011def getScheme(): StructType = &#123; val schemaString = "store_id:String,order_date:String,sale_amount: Int" val fields = schemaString.split(",") .map(fieldName =&gt; StructField(fieldName.split(":")(0).trim, fieldName.split(":")(1).trim match &#123; case "String" =&gt; StringType case "Int" =&gt; IntegerType &#125;, true)) StructType(fields) &#125; 保存小数点后n位10表示总的位数，2表示保留几位小数，10要&gt;=实际的位数，否则为NULL1spark.sql("select cast(sale_amount as decimal(10, 2))from ycd").show() 重命名行1234567891011121314import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.functions._import org.apache.spark.sql.&#123;SQLContext, Row, DataFrame, Column&#125;import org.apache.spark.ml.feature.VectorAssemblerval firstDF = spark.createDataFrame(Seq( (1, 1, 2, 3, 8, 4, 5), (2, 4, 3, 8, 7, 9, 8), (3, 6, 1, 9, 2, 3, 6), (4, 7, 8, 6, 9, 4, 5), (5, 9, 2, 7, 8, 7, 3), (6, 1, 1, 4, 2, 8, 4))).toDF()val colNames = Seq("uid", "col1", "col2", "col3", "col4", "col5", "col6")val secondDF = firstDF.toDF(colNames: _*) 转置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import org.apache.spark.sql.DataFrameimport org.apache.spark.sql.functions.&#123;array, col, explode, lit, struct&#125;val df = spark.createDataFrame(Seq( (1, 1, 2, 3, 8, 4, 5), (2, 4, 3, 8, 7, 9, 8), (3, 6, 1, 9, 2, 3, 6), (4, 7, 8, 6, 9, 4, 5), (5, 9, 2, 7, 8, 7, 3), (6, 1, 1, 4, 2, 8, 4))).toDF("uid", "col1", "col2", "col3", "col4", "col5", "col6")df.show(10,false)// Create the transpose user defined function.// Imputs:// transDF: The dataframe which will be transposed// transBy: The column that the dataframe will be transposed by// Outputs:// Dataframe datatype consisting of three columns:// transBy// column_name// column_valuedef transposeUDF(transDF: DataFrame, transBy: Seq[String]): DataFrame = &#123; val (cols, types) = transDF.dtypes.filter&#123; case (c, _) =&gt; !transBy.contains(c)&#125;.unzip require(types.distinct.size == 1) val kvs = explode(array( cols.map(c =&gt; struct(lit(c).alias("column_name"), col(c).alias("column_value"))): _* )) val byExprs = transBy.map(col(_)) transDF .select(byExprs :+ kvs.alias("_kvs"): _*) .select(byExprs ++ Seq($"_kvs.column_name", $"_kvs.column_value"): _*)&#125;transposeUDF(df, Seq("uid")).show(12,false)Output:df.show(10,false)+---+----+----+----+----+----+----+|uid|col1|col2|col3|col4|col5|col6|+---+----+----+----+----+----+----+|1 |1 |2 |3 |8 |4 |5 ||2 |4 |3 |8 |7 |9 |8 ||3 |6 |1 |9 |2 |3 |6 ||4 |7 |8 |6 |9 |4 |5 ||5 |9 |2 |7 |8 |7 |3 ||6 |1 |1 |4 |2 |8 |4 |+---+----+----+----+----+----+----+transposeUDF(df, Seq("uid")).show(12,false)+---+-----------+------------+|uid|column_name|column_value|+---+-----------+------------+|1 |col1 |1 ||1 |col2 |2 ||1 |col3 |3 ||1 |col4 |8 ||1 |col5 |4 ||1 |col6 |5 ||2 |col1 |4 ||2 |col2 |3 ||2 |col3 |8 ||2 |col4 |7 ||2 |col5 |9 ||2 |col6 |8 |+---+-----------+------------+only showing top 12 rows]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamicVariable详解]]></title>
    <url>%2F2019%2F02%2F13%2FDynamicVariable%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[内容来源于：https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable 说明12protected val _response = new DynamicVariable[HttpServletResponse](null)protected val _request = new DynamicVariable[HttpServletRequest](null) DynamicVariable是贷款和动态范围模式的实现。 DynamicVariable的使用情况与Java中的ThreadLocal非常相似(事实上，DynamicVariable在后台使用InheritableThreadLocal) – 当需要在一个封闭的作用域内进行计算时，每个线程都有自己的副本的变量值：123dynamicVariable.withValue(value)&#123; valueInContext =&gt; // value used in the context&#125; 由于DynamicVariable使用可继承的ThreadLocal，变量的值被传递给上下文中生成的线程：12345dynamicVariable.withValue(value)&#123; valueInContext =&gt; spawn&#123; // value is passed to the spawned thread &#125;&#125; DynamicVariable(和ThreadLocal)在Scalatra中使用的原因与在许多其他框架(Lift，Spring，Struts等)中使用的相同 – 它是一种非侵入性的方式来存储和传递上下文(线程)特定的信息。 使HttpServletResponse和HttpServletRequest动态变量(并因此，绑定到处理请求的特定线程)只是最简单的方法来获取它们在代码中的任何地方(不通过方法参数或任何其他显式)。 案例1234567val dyn = new DynamicVariable[String]("withoutValue")def print=println(dyn.value)printdyn.withValue("withValue") &#123; print&#125;print 结果withoutValuewithValuewithoutValue]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java concurrent包类详解]]></title>
    <url>%2F2019%2F02%2F13%2Fjava%20concurrent%E5%8C%85%E7%B1%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Semaphore原文：https://zhuanlan.zhihu.com/p/27314456Semaphore(信号量)是java.util.concurrent下的一个工具类.用来控制可同时访问特定资源的线程数.内部是通过维护父类(AQS)的 int state值实现.Semaphore中有一个”许可”的概念: 访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。访问资源后，使用release()释放许可。 这个许可在构造时传入,赋给state值,它等同于state. Semaphore应用场景系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。使用案例：spark LiveListenerBus类1234567891011121314151617181920212223242526272829303132private val eventLock = new Semaphore(0)private val listenerThread = new Thread(name) &#123; setDaemon(true) override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) &#123; LiveListenerBus.withinListenerThread.withValue(true) &#123; while (true) &#123; eventLock.acquire() self.synchronized &#123; processingEvent = true &#125; try &#123; // 消费消息 val event = eventQueue.poll if (event == null) &#123; // Get out of the while loop and shutdown the daemon thread if (!stopped.get) &#123; throw new IllegalStateException("Polling `null` from eventQueue means" + " the listener bus has been stopped. So `stopped` must be true") &#125; return &#125; postToAll(event) &#125; finally &#123; self.synchronized &#123; processingEvent = false &#125; &#125; &#125; &#125; &#125;&#125; Semaphore属于一种较常见的限流手段，Google Guava封装了一层。123456789101112131415161718//JDK API：流速控制在每秒执行100个任务final Semaphore semaphore = new Semaphore(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; semaphore.acquire(); // 也许需要等待 executor.execute(task); semaphore.release(); &#125;&#125;//Google Guava API：流速控制在每秒执行100个任务final RateLimiter rateLimiter = RateLimiter.create(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; rateLimiter.acquire(); // 也许需要等待 executor.execute(task); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala学习笔记]]></title>
    <url>%2F2019%2F01%2F09%2Fscala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[高级函数高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。 匿名函数1(x: Int) =&gt; x * 3 带函数参数的函数Scala集合类（collections）的高阶函数map。1234567891011121314151617181920def map[B, That](f: A =&gt; B)(implicit bf: CanBuildFrom[Repr, B, That]): That = &#123; def builder = &#123; val b = bf(repr) b.sizeHint(this) b &#125; val b = builder for (x &lt;- this) b += f(x) b.result &#125; val seq = Seq(100, 200, 300) def doubleSalary(x: Int): Int = &#123; x * 2 &#125; seq.map(doubleSalary).foreach(x =&gt; println(x)) seq.map(x =&gt; x * 2) seq.map(_ * 2) 闭包123def sum(x: Int) = (y: Int) =&gt; x + yval first = sum(1)val second = first(4) 嵌套方法在Scala中可以嵌套定义方法。12345678910def factorial(x: Int): Int = &#123; def fact(x: Int, accumulator: Int): Int = &#123; if (x &lt;= 1) accumulator else fact(x - 1, x * accumulator) &#125; fact(x, 1) &#125; println("Factorial of 2: " + factorial(2)) println("Factorial of 3: " + factorial(3)) 柯里化柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。接受两个参数的过程：1def mul(x:Int , y: Int )= x * y 接受一个参数的过程12def mul(x: Int) = (y: Int) =&gt; x + ymul(1)(2) scala支持简写成如下的柯里化：123def mul(x: Int)(y: Int) = x + ymul(1)(2) 在Scala集合中定义的特质TraversableOnce[+A]。Traversable： 能横过的；能越过的；可否定的。123456789101112131415def foldLeft[B](z: B)(op: (B, A) =&gt; B): B = &#123; var result = z this foreach (x =&gt; result = op(result, x)) result &#125;``` foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：从初值0开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。 ```scalaval numbers = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)val res = numbers.foldLeft(0)((m, n) =&gt; m + n)val res2 = numbers.foldLeft(0)(_ + _)println(res) // 55pringln(res2) 多参数列表有更复杂的调用语法，因此应该谨慎使用。建议的使用场景包括: 单一的函数参数。在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样： 1numbers.foldLeft(0, &#123;(m: Int, n: Int) =&gt; m + n&#125;) 隐式（IMPLICIT）参数。 1def execute(arg: Int)(implicit ec: ExecutionContext) = ??? 模式匹配match对应java里的Switch,不过它写在选择器表达式之后。 选择器 match {备选项}取代了：switch(选择器){备选项} match与switch的比较匹配表达式可以被看做java风格switch的泛化。match的不同：1、match是scala表达式，也就是说，它始终以值作为结果;2、 scala的备选项表达式永远不会”掉到”下一个case;3、 如果没有模式匹配，MatchErro异常会被抛出。 提取器对象提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。123456789101112131415161718case class Person(name: String, age: Int) def main(args: Array[String]): Unit = &#123; // 调用工厂构造方法，构造出对象实例 val person = Person("Spark", 6) // 这种写法居然可以正常编译 val Person(name, age) = person /* * 使用了提取器， * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类 */ println(name + " : " + age) //正常输出: Spark 6 person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125; 自定义unapply方法主要用于模式匹配中。1234567891011121314151617181920212223class Person(val name: String, val salary: Int)object Person &#123; def apply(name: String, salary: Int):Person = &#123; new Person(name, salary) &#125; def unapply(money: Person): Option[(String, Int)] = &#123; if(money == null) &#123; None &#125; else &#123; Some(money.name, money.salary) &#125; &#125; def main(args: Array[String]): Unit = &#123; val person = Person("spark", 800); person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125;&#125; for 表达式Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。例子1234567891011case class User(name: String, age: Int)val userBase = List(User("Travis", 28), User("Kelly", 33), User("Jennifer", 44), User("Dennis", 23))val twentySomethings = for (user &lt;- userBase if (user.age &gt;=20 &amp;&amp; user.age &lt; 30)) yield user.name twentySomethings.foreach(name =&gt; println(name)) 这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user =20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。 泛型泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。 定义一个泛型类泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。12345678910class Stack[A] &#123; private var elements: List[A] = Nil def push(x: A) &#123; elements = x :: elements &#125; def peek: A = elements.head def pop(): A = &#123; val currentTop = peek elements = elements.tail currentTop &#125;&#125; 上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。 使用要使用一个泛型类，将一个具体类型放到方括号中来代替 A。12345val stack = new Stack[Int]stack.push(1)stack.push(2)println(stack.pop) // prints 2println(stack.pop) // prints 1 型变型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。123class Foo[+A] // 一个协变类class Bar[-A] // 一个逆变类class Baz[A] // 一个不变类 协变使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。 考虑以下简单的类结构：12345abstract class Animal &#123; def name: String&#125;case class Cat(name: String) extends Animalcase class Dog(name: String) extends Animal 类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。 在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。1234567891011121314151617object CovarianceTest extends App &#123; def printAnimalNames(animals: List[Animal]): Unit = &#123; animals.foreach &#123; animal =&gt; println(animal.name) &#125; &#125; val cats: List[Cat] = List(Cat("Whiskers"), Cat("Tom")) val dogs: List[Dog] = List(Dog("Fido"), Dog("Rex")) printAnimalNames(cats) // Whiskers // Tom printAnimalNames(dogs) // Fido // Rex&#125; 逆变通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。 考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：123abstract class Printer[-A] &#123; def print(value: A): Unit&#125; 这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：123456789class AnimalPrinter extends Printer[Animal] &#123; def print(animal: Animal): Unit = println("The animal's name is: " + animal.name)&#125;class CatPrinter extends Printer[Cat] &#123; def print(cat: Cat): Unit = println("The cat's name is: " + cat.name)&#125; 如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。12345678910111213object ContravarianceTest extends App &#123; val myCat: Cat = Cat("Boots") def printMyCat(printer: Printer[Cat]): Unit = &#123; printer.print(myCat) &#125; val catPrinter: Printer[Cat] = new CatPrinter val animalPrinter: Printer[Animal] = new AnimalPrinter printMyCat(catPrinter) printMyCat(animalPrinter)&#125; 这个程序的输出如下： The cat’s name is: BootsThe animal’s name is: Boots 不变默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。1234567class Container[A](value: A) &#123; private var _value: A = value def getValue: A = _value def setValue(value: A): Unit = &#123; _value = value &#125;&#125; 可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：1234val catContainer: Container[Cat] = new Container(Cat("Felix"))val animalContainer: Container[Animal] = catContaineranimalContainer.setValue(Dog("Spot"))val cat: Cat = catContainer.getValue 糟糕，我们最终会将一只狗作为值分配给一只猫幸运的是，编译器在此之前就会阻止我们。 上界在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。1234567891011121314151617181920212223242526abstract class Animal &#123; def name: String&#125;abstract class Pet extends Animal &#123;&#125;class Cat extends Pet &#123; override def name: String = "Cat"&#125;class Dog extends Pet &#123; override def name: String = "Dog"&#125;class Lion extends Animal &#123; override def name: String = "Lion"&#125;class PetContainer[P &lt;: Pet](p: P) &#123; def pet: P = p&#125;val dogContainer = new PetContainer[Dog](new Dog)val catContainer = new PetContainer[Cat](new Cat)// this would not compileval lionContainer = new PetContainer[Lion](new Lion) 类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：1type arguments [Lion] do not conform to class PetContainer's type parameter bounds [P &lt;: Pet] 这是因为Lion并不是Pet的子类。 下界类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。 下面看一个适合用类型下界的例子：下面看一个适合用类型下界的例子：12345678910111213trait Node[+B] &#123; def prepend(elem: B): Node[B]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this)&#125; 该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。 但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。 要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。12345678910111213trait Node[+B] &#123; def prepend[U &gt;: B](elem: U): Node[U]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this)&#125; 现在我们像下面这么做：123456trait Birdcase class AfricanSwallow() extends Birdcase class EuropeanSwallow() extends Birdval africanSwallowList= ListNode[AfricanSwallow](AfricanSwallow(), Nil())val birdList: Node[Bird] = africanSwallowListbirdList.prepend(new EuropeanSwallow) 可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。 抽象类型特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345trait Buffer &#123; type T val element: T&#125;&#125; 这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345abstract class SeqBuffer extends Buffer &#123; type U type T &lt;: Seq[U] def length = element.length&#125; 注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。 含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。12345678910111213abstract class IntSeqBuffer extends SeqBuffer &#123; type U = Int&#125;def newIntSeqBuf(elem1: Int, elem2: Int): IntSeqBuffer = new IntSeqBuffer &#123; type T = List[U] val element = List(elem1, elem2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。 把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：123456789101112131415abstract class Buffer[+T] &#123; val element: T&#125;abstract class SeqBuffer[U, +T &lt;: Seq[U]] extends Buffer[T] &#123; def length = element.length&#125;def newIntSeqBuf(e1: Int, e2: Int): SeqBuffer[Int, Seq[Int]] = new SeqBuffer[Int, List[Int]] &#123; val element = List(e1, e2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。 复合类型需求:有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。 案例：假设我们有两个特质 Cloneable 和 Resetable：12345678trait Cloneable extends java.lang.Cloneable &#123; override def clone(): Cloneable = &#123; super.clone().asInstanceOf[Cloneable] &#125;&#125;trait Resetable &#123; def reset: Unit&#125; 现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：12345def cloneAndReset(obj: ?): Cloneable = &#123; val cloned = obj.clone() obj.reset cloned&#125; 这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。 以下是更新后的方法：123def cloneAndReset(obj: Cloneable with Resetable): Cloneable = &#123; //...&#125; 复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement } 关于使用细化的例子参考 通过混入（mixin）来组合类。 自类型自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。 自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。 要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。123456789101112131415trait User &#123; def username: String&#125;trait Tweeter &#123; this: User =&gt; // 重新赋予 this 的类型 def tweet(tweetText: String) = println(s"$username: $tweetText")&#125;class VerifiedTweeter(val username_ : String) extends Tweeter with User &#123; // 我们混入特质 User 因为 Tweeter 需要 def username = s"real $username_"&#125;val realBeyoncé = new VerifiedTweeter("Beyoncé")realBeyoncé.tweet("Just spilled my glass of lemonade") // 打印出 "real Beyoncé: Just spilled my glass of lemonade" 因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。自类型别名12345678910object Demo &#123; self =&gt; def sum(num1: Int, num2: Int): Int = &#123; num1 + num2 &#125; def main(args: Array[String]): Unit = &#123; println(self.sum(1, 2)) &#125;&#125; 隐式转换定义：指的是那种以implicit关键字声明的带有单个参数的函数。通过隐式转换，程序员可以在编写Scala程序时故意漏掉一些信息，让编译器去尝试在编译期间自动推导出这些信息来，这种特性可以极大的减少代码量，忽略那些冗长，过于细节的代码。1.将方法或变量标记为implicit2.将方法的参数列表标记为implicit3.将类标记为implicit Scala支持两种形式的隐式转换：隐式值：用于给方法提供参数隐式视图：用于类型间转换或使针对某类型的方法能调用成功 案例给File类增加read方法123class RichFile(val f: File) &#123; def read() = Source.fromFile(f).mkString&#125; 门面类12345import java.io.Fileobject RichFilePredef &#123; implicit def fileToRichFile(f: File) = new RichFile(f)&#125; 使用 1234567891011import java.io.Fileimport scala.io.Sourceimport com.tm.scala.implic.RichFilePredef._object RichFile &#123; def main(args: Array[String]) &#123; val f = new File("/home/hadoop/sample_movielens_data.txt") // 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object val contents = f.read() println(contents) &#125;&#125; 排序中的使用案例Ordered方式1class Girl(val name: String, var faceValue: Int, var age: Int) 定义比较规则123456789101112/** * 视图定界 * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。 * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。 * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html */class OrderedChooser[T &lt;% Ordered[T]] &#123; def choose(first: T, second: T): T = &#123; if (first &gt; second) first else second &#125;&#125; 门面类1234567891011121314151617181920212223object OrderedPredef &#123; // 方式一 implicit def gilrToOrdered(girl: Girl):Ordered[Girl] = new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125; // 方式二 implicit val gilrToOrdered = (girl: Girl) =&gt; new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125;&#125; 测试类12345678910object TestOrdered &#123; def main(args: Array[String]): Unit = &#123; val girl1 = new Girl("spark", 100,50) val girl2 = new Girl("mxnet", 90,30) import OrderedPredef._ val chooser = new OrderingChoose[Girl] val g = chooser.choose(girl1, girl2) println(g.faceValue) &#125;&#125; Ordering方式定义比较规则 12345678910/** * 文本定界 */class OrderingChoose[T: Ordering] &#123; def choose(first: T, second: T): T = &#123; val ord = implicitly[Ordering[T]] if (ord.gt(first, second)) first else second &#125;&#125; 门面类123456789101112131415161718192021222324252627282930313233343536object OrderingPredef &#123; // 方式一 implicit val gilrToOrdering = new Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式二 implicit object GilrToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式三 /** * 参考：Ordering中的下面方法 * trait IntOrdering extends Ordering[Int] &#123; * def compare(x: Int, y: Int) = * if (x &lt; y) -1 * else if (x == y) 0 * else 1 * &#125; * implicit object Int extends IntOrdering */ trait GirlToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; implicit object Girl extends GirlToOrdering&#125; 测试类12345678910object TestOrdering &#123; def main(args: Array[String]): Unit = &#123; val g1 = new Girl("zhangsan", 50,50) val g2 = new Girl("lisi", 500,50) import OrderingPredef._ val choose = new OrderingChoose[Girl] val g = choose.choose(g1, g2) println(g.name) &#125;&#125; scala导入类并取别名导入Map,并取别名1import java.util.&#123;Map =&gt; JMap&#125; scala Try的使用spark Utils中的使用案例：1234567def classIsLoadable(clazz: String): Boolean = &#123; // scalastyle:off classforname Try &#123; Class.forName(clazz, false, getContextOrSparkClassLoader) &#125;.isSuccess // scalastyle:on classforname &#125; quasiquotes(q字符串)用于代码生成官方文档： https://docs.scala-lang.org/overviews/quasiquotes/intro.html参考文档：https://www.cnblogs.com/shishanyuan/p/8455786.html作用： Quasiquotes允许在Scala语言中对抽象语法树（AST）进行编程式构建，然后在运行时将其提供给Scala编译器以生成字节码。以q开头的字符串是quasiquotes，虽然它们看起来像字符串，但它们在编译时由Scala编译器解析，并代表其代码的AST。 val tree = q"i am { a quasiquote }" tree: universe.Tree = i.am(a.quasiquote) 符号详解== 和===的区别参考：http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie == 返回一个布尔值=== 返回一列（包含两列元素比较的结果） call-by-value and call-by-name传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部 半生对象object中的构造器在第一次调用执行一次，以后调用的话不会多次执行。object会有自己的构造方法，默认是没有参数的构造方法。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala枚举]]></title>
    <url>%2F2019%2F01%2F09%2Fscala%E6%9E%9A%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[定义枚举对象1234567/** 定义一个枚举类 */private[deploy] object SparkSubmitAction extends Enumeration &#123; // 声明枚举对外暴露的变量类型 type SparkSubmitAction = Value // 枚举的定义 val SUBMIT, KILL, REQUEST_STATUS = Value&#125; 使用枚举12345678910111213141516/** 定义一个枚举类 */object SparkSubmitAction extends Enumeration &#123; // 声明枚举对外暴露的变量类型 type SparkSubmitAction = Value // 枚举的定义 val SUBMIT, KILL, REQUEST_STATUS = Value def getAction(action: SparkSubmitAction)&#123; action match &#123; case SUBMIT =&gt; println ("action is " + action) case KILL =&gt; println ("action is " + action) case REQUEST_STATUS =&gt; println ("action is " + action) case _ =&gt; println ("Unknown type") &#125;&#125;&#125; 测试用例123456789object EnumerationTest &#123; def main(args: Array[String]): Unit = &#123; val action = SparkSubmitAction.apply(1) SparkSubmitAction.getAction(action) val action1 = SparkSubmitAction.withName("quit") SparkSubmitAction.getAction(action1) &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala特殊符号使用]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[_*符号作用：它告诉编译器将序列类型的单个参数视为可变参数序列。举例：12345public ProcessBuilder(String... command) &#123; this.command = new ArrayList&lt;&gt;(command.length); for (String arg : command) this.command.add(arg); &#125; 12val commandSeq = Seq(command.mainClass) ++ command.argumentsval builder = new ProcessBuilder(commandSeq: _*) ::该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。1234567891011121314151617181920212223242526272829303132def main(args: Array[String]) &#123; args.toList match &#123; case workerUrl :: userJar :: mainClass :: extraArgs =&gt; val conf = new SparkConf() val rpcEnv = RpcEnv.create("Driver", Utils.localHostName(), 0, conf, new SecurityManager(conf)) rpcEnv.setupEndpoint("workerWatcher", new WorkerWatcher(rpcEnv, workerUrl)) val currentLoader = Thread.currentThread.getContextClassLoader val userJarUrl = new File(userJar).toURI().toURL() val loader = if (sys.props.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) &#123; new ChildFirstURLClassLoader(Array(userJarUrl), currentLoader) &#125; else &#123; new MutableURLClassLoader(Array(userJarUrl), currentLoader) &#125; Thread.currentThread.setContextClassLoader(loader) // Delegate to supplied main class val clazz = Utils.classForName(mainClass) val mainMethod = clazz.getMethod("main", classOf[Array[String]]) mainMethod.invoke(null, extraArgs.toArray[String]) rpcEnv.shutdown() case _ =&gt; // scalastyle:off println System.err.println("Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]") // scalastyle:on println System.exit(-1) &#125; &#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala通过反射获取当前类类名]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[先编写HelloWord.scala文件123456object HelloWorld &#123; def main(args: Array[String]) &#123; // 获取当前类类名 println(this.getClass.getName.stripSuffix("$")) &#125;&#125; 命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class spark源码中的案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.spark.internalimport org.apache.log4j.&#123;Level, LogManager, PropertyConfigurator&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;import org.slf4j.impl.StaticLoggerBinderimport org.apache.spark.util.Utils/** * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows * logging messages at different levels using methods that only evaluate parameters lazily if the * log level is enabled. */private[spark] trait Logging &#123; // Make the log field transient so that objects with Logging can // be serialized and used on another machine @transient private var log_ : Logger = null // Method to get the logger name for this object protected def logName = &#123; // Ignore trailing $'s in the class names for Scala objects this.getClass.getName.stripSuffix("$") &#125; // Method to get or create the logger for this object protected def log: Logger = &#123; if (log_ == null) &#123; initializeLogIfNecessary(false) log_ = LoggerFactory.getLogger(logName) &#125; log_ &#125; // Log methods that take only a String protected def logInfo(msg: =&gt; String) &#123; if (log.isInfoEnabled) log.info(msg) &#125; protected def logDebug(msg: =&gt; String) &#123; if (log.isDebugEnabled) log.debug(msg) &#125; protected def logTrace(msg: =&gt; String) &#123; if (log.isTraceEnabled) log.trace(msg) &#125; protected def logWarning(msg: =&gt; String) &#123; if (log.isWarnEnabled) log.warn(msg) &#125; protected def logError(msg: =&gt; String) &#123; if (log.isErrorEnabled) log.error(msg) &#125; // Log methods that take Throwables (Exceptions/Errors) too protected def logInfo(msg: =&gt; String, throwable: Throwable) &#123; if (log.isInfoEnabled) log.info(msg, throwable) &#125; protected def logDebug(msg: =&gt; String, throwable: Throwable) &#123; if (log.isDebugEnabled) log.debug(msg, throwable) &#125; protected def logTrace(msg: =&gt; String, throwable: Throwable) &#123; if (log.isTraceEnabled) log.trace(msg, throwable) &#125; protected def logWarning(msg: =&gt; String, throwable: Throwable) &#123; if (log.isWarnEnabled) log.warn(msg, throwable) &#125; protected def logError(msg: =&gt; String, throwable: Throwable) &#123; if (log.isErrorEnabled) log.error(msg, throwable) &#125; protected def isTraceEnabled(): Boolean = &#123; log.isTraceEnabled &#125; protected def initializeLogIfNecessary(isInterpreter: Boolean): Unit = &#123; if (!Logging.initialized) &#123; Logging.initLock.synchronized &#123; if (!Logging.initialized) &#123; initializeLogging(isInterpreter) &#125; &#125; &#125; &#125; private def initializeLogging(isInterpreter: Boolean): Unit = &#123; // Don't use a logger in here, as this is itself occurring during initialization of a logger // If Log4j 1.2 is being used, but is not initialized, load a default properties file val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr // This distinguishes the log4j 1.2 binding, currently // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently // org.apache.logging.slf4j.Log4jLoggerFactory val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass) if (usingLog4j12) &#123; val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements // scalastyle:off println if (!log4j12Initialized) &#123; val defaultLogProps = "org/apache/spark/log4j-defaults.properties" Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match &#123; case Some(url) =&gt; PropertyConfigurator.configure(url) System.err.println(s"Using Spark's default log4j profile: $defaultLogProps") case None =&gt; System.err.println(s"Spark was unable to load $defaultLogProps") &#125; &#125; if (isInterpreter) &#123; // Use the repl's main class to define the default log level when running the shell, // overriding the root logger's config if they're different. val rootLogger = LogManager.getRootLogger() val replLogger = LogManager.getLogger(logName) val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN) if (replLevel != rootLogger.getEffectiveLevel()) &#123; System.err.printf("Setting default log level to \"%s\".\n", replLevel) System.err.println("To adjust logging level use sc.setLogLevel(newLevel). " + "For SparkR, use setLogLevel(newLevel).") rootLogger.setLevel(replLevel) &#125; &#125; // scalastyle:on println &#125; Logging.initialized = true // Force a call into slf4j to initialize it. Avoids this happening from multiple threads // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html log &#125;&#125;private object Logging &#123; @volatile private var initialized = false val initLock = new Object() try &#123; // We use reflection here to handle the case where users remove the // slf4j-to-jul bridge order to route their logs to JUL. val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler") bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null) val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean] if (!installed) &#123; bridgeClass.getMethod("install").invoke(null) &#125; &#125; catch &#123; case e: ClassNotFoundException =&gt; // can't log anything yet so just fail silently &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala自定注解]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[为什么要注解除了编译和允许之外，还可以对程序做：1、使用Scaladoc 自动产生文档;2、漂亮的打印印出符合你偏爱分割的代码;3、代码常见错误检查，如：打开了文件却没(在全部逻辑分支中)关闭。4、实验类型检查，例如副作用管理或所有权属性确认。这类工具（程序）被称为元编程工具，它们把其它程序当做输入程序。 scala注解所在包和基本语法格式注解所在包： 标准库定义的注解相关内容在包scala.annotation中。基本语法： @注解名称(注解参数) 自定义注解自定义注解需要从注解特质继承，scala提供两种注解： 1、基本语法： @注解名称(注解参数) scala中的自定义注解不是接口/特质，而是类。自定义注解需要从注解特质中继承，Scala中提供了两类注解特质： scala.annotation.ClassfileAnnotation 由Java编译器生成注解scala.annotation.StaticAnnotation 由Scala编译器生成注解]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql详解]]></title>
    <url>%2F2019%2F01%2F04%2Fsparksql%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark SQL的Dataset基本操作https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解 基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。 1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是1type DataFrame = Dataset[Row] 结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。 首先要创建一个SparkSession:12345678910111213141516171819val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]").set("yarn.resourcemanager.hostname", "localhost") // executor的实例数.set("spark.executor.instances","2") .set("spark.default.parallelism","4") // sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。.set("spark.sql.shuffle.partitions","4").setJars(List("/Users/meitu/Desktop/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar"))val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() 创建dataset123456val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100), ("Warsaw", 2017, 200), ("Warsaw", 2015, 100), ("Warsaw", 2017, 200), ("Beijing", 2017, 200), ("Beijing", 2016, 200), ("Beijing", 2015, 200), ("Beijing", 2014, 200), ("Warsaw", 2014, 200), ("Boston", 2017, 50), ("Boston", 2016, 50), ("Boston", 2015, 50), ("Boston", 2014, 150))).toDF("city", "year", "amount") 使用函数的时候要导入包：1import org.apache.spark.sql.functions.&#123;col,expr&#125; select列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。使用col函数，可以直接对列进行一些逻辑操作。12sales.select("city","year","amount").show(1)sales.select(col("city"),col("amount")+1).show(1) selectExpr参数是字符串，且直接可以使用表达式。也可以使用select+expr函数来替代。12sales.selectExpr("city","year as date","amount+1").show(10)sales.select(expr("city"),expr("year as date"),expr("amount+1")).show(10) filter参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。123sales.filter(col("amount")&gt;150).show()sales.filter(row=&gt;&#123; row.getInt(2)&gt;150&#125;).show(10)sales.filter("amount &gt; 150 ").show(10) where类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。12sales.where(col("amount")&gt;150).show()sales.where("amount &gt; 150 ").show() group by主要是以count和agg聚合函数为例讲解groupby函数。12sales.groupBy("city").count().show(10)sales.groupBy(col("city")).agg(sum("amount").as("total")).show(10) union两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。1sales.union(sales).groupBy("city").count().show() joinjoin操作相对比较复杂，具体如下：123456789101112131415161718192021222324252627282930// 相同的列进行join sales.join(sales,"city").show(10) // 多列joinsales.join(sales,Seq("city","year")).show() /* 指定join类型， join 类型可以选择: `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti`. */ // 内部joinsales.join(sales,Seq("city","year"),"inner").show() /* join条件 ：可以在join方法里放入join条件，也可以使用where,这两种情况都要求字段名称不一样。*/ sales.join(sales, col("city").alias("city1") === col("city")).show() sales.join(sales).where(col("city").alias("city1") === col("city")).show() /* dataset的self join 此处使用where作为条件，需要增加配置.set("spark.sql.crossJoin.enabled","true") 也可以加第三个参数，join类型，可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` */ sales.join(sales,sales("city") === sales("city")).show() sales.join(sales).where(sales("city") === sales("city")).show()/* joinwith,可以指定第三个参数，join类型，类型可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`。 */ sales.joinWith(sales,sales("city") === sales("city"),"inner").show() 输出结果： order byorderby 全局有序，其实用的还是sort12sales.orderBy(col("year").desc,col("amount").asc).show()sales.orderBy("city","year").show() sort全局排序，直接替换掉8小结的orderby即可。 sortwithinpartition在分区内部进行排序，局部排序。12sales.sortWithinPartitions(col("year").desc,col("amount").asc).show()sales.sortWithinPartitions("city","year").show() 可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。 withColumn1234567891011/* withColumn 假如列，存在就替换，不存在新增 withColumnRenamed 对已有的列进行重命名 *///相当于给原来amount列，+1sales.withColumn("amount",col("amount")+1).show()// 对amount列+1，然后将值增加到一个新列 amount1sales.withColumn("amount1",col("amount")+1).show()// 将amount列名，修改为amount1sales.withColumnRenamed("amount","amount1").show() foreach这个跟rdd的foreach一样，元素类型是row。123sales.foreach(row=&gt;&#123; println(row.getString(0))&#125;) foreachPartition跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。12345678sales.foreachPartition(partition=&gt;&#123; //打开数据库链接等 partition.foreach(each=&gt;&#123; println(each.getString(0)) //插入数据库 &#125;) //关闭数据库链接 &#125;) distinct针对dataset的行去重，返回的是所有行都不重复的dataset。1sales.distinct().show(10) dropDuplicates这个适用于dataset有唯一的主键，然后对主键进行去重。1234val before = sales.count()val after = sales.dropDuplicates("city").count()println("before ====&gt; " +before)println("after ====&gt; "+after) drop删除一列，或者多列，这是一个变参数算子。12345sales.drop("city").show()打印出来schema信息如下：root|-- year: integer (nullable = false)|-- amount: integer (nullable = false) printSchema输出dataset的schema信息1234567891011sales.printSchema()输出结果如下：root|-- city: string (nullable = true)|-- year: integer (nullable = false)|-- amount: integer (nullable = false) explain()打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程123456sales.orderBy(col("year").desc,col("amount").asc).explain()执行计划输出如下：== Physical Plan ==*(1) Sort [year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST], true, 0+- Exchange rangepartitioning(year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST, 3)+- LocalTableScan [city#6, year#7, amount#8] Spark SQL入门到精通之第二篇Dataset的复杂操作本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：cube，rollup，pivot操作。 cube简单的理解就是维度及度量组成的数据体。1234sales.cube("city","year") .agg(sum("amount")) .sort(col("city").desc_nulls_first,col("year").desc_nulls_first) .show() 举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：123456789101112131415val city_year = sales.groupBy("city","year").agg(sum("amount"))val city = sales.groupBy("city") .agg(sum("amount") as "amount") .select(col("city"), lit(null) as "year", col("amount"))val year = sales.groupBy("year") .agg(sum("amount") as "amount") .select( lit(null) as "city",col("year"), col("amount"))val none = sales .groupBy().agg(sum("amount") as "amount") .select(lit(null) as "city", lit(null) as "year", col("amount")) city_year.union(city).union(year).union(none) .sort(desc_nulls_first("city"), desc_nulls_first("year")) .show() rollup这里也是以案例开始，代码如下：12345678910111213val expenses = spark.createDataFrame(Seq( ((2012, Month.DECEMBER, 12), 5), ((2016, Month.AUGUST, 13), 10), ((2017, Month.MAY, 27), 15)) .map &#123; case ((yy, mm, dd), a) =&gt; (LocalDate.of(yy, mm, dd), a) &#125; .map &#123; case (d, a) =&gt; (d.toString, a) &#125; .map &#123; case (d, a) =&gt; (Date.valueOf(d), a) &#125;).toDF("date", "amount")// rollup time!val res = expenses .rollup(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount") .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：123456789101112val year_month = expenses.groupBy(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount")val yearOnly = expenses.groupBy(year(col("date")) as "year") .agg(sum("amount") as "amount") .select(col("year"), lit(null) as "month", col("amount"))val none = expenses.groupBy() .agg(sum("amount") as "amount") .select(lit(null) as "year", lit(null) as "month", col("amount"))year_month.union(yearOnly).union(none) .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() pivot旋转操作https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEwPivot 算子是 spark 1.6 版本开始引入的，在 spark2.4版本中功能做了增强，还是比较强大的，做过数据清洗ETL工作的都知道，行列转换是一个常见的数据整理需求。spark 中的Pivot 可以根据枢轴点(Pivot Point) 把多行的值归并到一行数据的不同列，这个估计不太好理解，我们下面使用例子说明，看看pivot 这个算子在处理复杂数据时候的威力。12345678910111213val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Boston", 2015, 50,"Boston"), ("Boston", 2016, 150,"Boston"), ("Toronto", 2017, 50,"Toronto"))).toDF("city", "year", "amount","test")sales.groupBy("year").pivot("city",Seq("Warsaw","Boston","Toronto")) .agg(sum("amount") as "amount") .show() 思路就是首先对year分组，然后旋转city字段，只取:“Warsaw”,”Boston”,”Toronto”然后对amount进行聚合操作案例：使用Pivot 来统计天气走势。下面是西雅图的天气数据表，每行代表一天的天气最高值：12345678910 Date Temp (°F) 07-22-2018 86 07-23-2018 90 07-24-2018 91 07-25-2018 92 07-26-2018 92 07-27-2018 88 07-28-2018 85 07-29-2018 94 07-30-2018 89 如果我们想看下最近几年的天气走势，如果这样一天一行数据，是很难看出趋势来的，最直观的方式是 按照年来分行，然后每一列代表一个月的平均天气，这样一行数据，就可以看到这一年12个月的一个天气走势，下面我们使用 pivot 来构造这样一个查询结果：1234567891011121314151617181920212223SELECT * FROM ( SELECT year(date) year, month(date) month, temp FROM high_temps WHERE date between DATE '2015-01-01' and DATE '2018-08-31' ) PIVOT ( CAST ( AVG(temp) as DECIMAL(4, 1) ) FOR month in ( 1 JAN, 2 FEB, 3 MAR, 4 APR, 5 MAY, 6 JUN, 7 JUL, 8 AUG, 9 SEP, 10 OCT, 11 NOV, 12 DEC ) ) ORDER BY year DESC; 结果如下图：是不是很直观，第一行就代表 2018 年，从1月到12月的平均天气，能看出一年的天气走势。我们来看下这个 sql 是怎么玩的，首先是一个 子查询语句，我们最终关心的是 年份，月份，和最高天气值，所以先使用子查询对原始数据进行处理，从日期里面抽取出来年份和月份。 下面在子查询的结果上使用 pivot 语句，pivot 第一个参数是一个聚合语句，这个代表聚合出来一个月30天的一个平均气温，第二个参数是 FOR month，这个是指定以哪个列为枢轴列，第三个 In 子语句指定我们需要进行行列转换的具体的 枢轴点(Pivot Point)的值，上面的例子中 1到12月份都包含了，而且给了一个别名，如果只指定 1到6月份，结果就如下了：上面sql语句里面有个特别的点需要注意， 就是聚合的时候有个隐含的维度字段，就是 年份，按理来讲，我们没有写 group-by year， 为啥结果表里面不同年份区分在了不同的行，原因是，FORM 子查询出来的每行有 3个列， year，month，tmp，如果一个列既不出现在进行聚合计算的列中（temp 是聚合计算的列）， 也不作为枢轴列 ， 就会作为聚合的时候一个隐含的维度。我们的例子中算平均值聚合操作的维度是 (year, month)，一个是隐含维度，一个是 枢轴列维度， 这一点一定要注意，如果不需要按照 year 来区分，FORM 查询的时候就不要加上这个列。指定多个聚合语句上文中只有一个聚合语句，就是计算平均天气，其实是可以加多个聚合语句的，比如我们需要看到 7，8，9 月份每个月的最大气温和平均气温，就可以用以下SQL语句。123456789101112131415161718192021SELECT * FROM ( SELECT year(date) year, month(date) month, temp FROM high_temps WHERE date between DATE '2015-01-01' and DATE '2018-08-31' ) PIVOT ( CAST ( AVG(temp) as DECIMAL(4, 1) ) avg, max(temp) max FOR month in (6 JUN, 7 JUL, 8 AUG, 9 SEP) ) ORDER BY year DESC; 上文中指定了两个聚合语句，查询后， 枢轴点(Pivot Point) 和 聚合语句 的笛卡尔积作为结果的不同列，也就是 _， 看下图：聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处现在假如我们有西雅图每天的最低温数据，我们需要把最高温和最低温放在同一张表里面看对比着看.123456789Date Temp (°F)… …08-01-2018 5908-02-2018 5808-03-2018 5908-04-2018 5808-05-2018 5908-06-2018 59… … 我们使用 UNION ALL 把两张表做一个合并：1234567SELECT date, temp, 'H' as flag FROM high_temps UNION ALL SELECT date, temp, 'L' as flag FROM low_temps; 现在使用 pivot 来进行处理：123456789101112SELECT * FROM (SELECT date,temp,'H' as flag FROM high_temps UNION ALL SELECT date,temp,'L' as flag FROM low_temps)WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'PIVOT(CAST(avg(temp) as DECIMAL(4,1))FOR month in (6 JUN,7 JUL,8 AUG , 9 SEP)) ORDER BY year DESC ,`H/L` ASC; 我们统计了 4年中 7，8，9 月份最低温和最高温的平均值，这里要注意的是，我们把 year 和 一个最低最高的标记（H/L）都作为隐含维度，不然算出来的就是最低最高温度在一起的平均值了。结果如下图：上面的查询中，我们把最低最高的标记（H/L）也作为了一个隐含维度，group-by 的维度就变成了 （year， H/L, month）, 但是year 和 H/L 体现在了不同行上面，month 体现在了不同的列上面，这就是 聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处。 再接再厉，我们把 H/L 作为 枢轴列（Pivot Columns） 进行查询：1234567891011121314151617SELECT * FROM (SELECT date,temp,'H' as flag FROM high_temps UNION ALL SELECT date,temp,'L' as flag FROM low_temps)WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'PIVOT(CAST(avg(temp) as DECIMAL(4,1))FOR (month,flag) in ((6,'H') JUN_hi,(6,'L') JUN_lo,(7,'H') JUl_hi,(7,'L') JUl_lo,(8,'H') AUG_hi,(8,'L') AUG_lo,(9,'H') SEP_hi,(9,'L') SEP_lo)) ORDER BY year DESC; 结果的展现方式就和上面的不同了，虽然每个单元格值都是相同的，但是把 H/L 和 month 笛卡尔积作为列，H/L 维度体现在了列上面了： 源码:Spark SQL 分区特性第一弹常见RDD分区Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。举几个浪尖在星球里分享比较多的例子，比如：Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据首先是由Seq数据集合生成一个Dataset1234567891011121314151617181920212223242526val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 110), ("Warsaw", 2017, 10), ("Warsaw", 2015, 100), ("Warsaw", 2015, 50), ("Warsaw", 2015, 80), ("Warsaw", 2015, 100), ("Warsaw", 2015, 130), ("Warsaw", 2015, 160), ("Warsaw", 2017, 200), ("Beijing", 2017, 100), ("Beijing", 2016, 150), ("Beijing", 2015, 50), ("Beijing", 2015, 30), ("Beijing", 2015, 10), ("Beijing", 2014, 200), ("Beijing", 2014, 170), ("Boston", 2017, 50), ("Boston", 2017, 70), ("Boston", 2017, 110), ("Boston", 2017, 150), ("Boston", 2017, 180), ("Boston", 2016, 30), ("Boston", 2015, 200), ("Boston", 2014, 20) )).toDF("city", "year", "amount") 将Dataset存处为partquet格式的hive表，分两种情况：用city和year字段分区1sales.write.partitionBy("city","year").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCityAndYear") 用city字段分区1sales.write.partitionBy("city").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCity") 读取数据采用的是1val res = spark.read.parquet("/user/hive/warehouse/parquettestcity") 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40Dataset的分区数是由参数：目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4Dataset的分区数是由参数：1println("partition size = "+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private def createNonBucketedReadRDD( readFile: (PartitionedFile) =&gt; Iterator[InternalRow], selectedPartitions: Seq[PartitionDirectory], fsRelation: HadoopFsRelation): RDD[InternalRow] = &#123; /* selectedPartitions 的大小代表目录数目 */ println("selectedPartitions.size : "+ selectedPartitions.size) val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes // spark.default.parallelism val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism // 计算文件总大小，单位字节数 val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum //计算平均每个并行度读取数据大小 val bytesPerCore = totalBytes / defaultParallelism // 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 // 然后，比较spark.sql.files.maxPartitionBytes 取小者 val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " + s"open cost is considered as scanning $openCostInBytes bytes.") // 这对目录遍历 val splitFiles = selectedPartitions.flatMap &#123; partition =&gt; partition.files.flatMap &#123; file =&gt; val blockLocations = getBlockLocations(file) //判断文件类型是否支持分割，以parquet为例，是支持分割的 if (fsRelation.fileFormat.isSplitable( fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123; // eg. 0 until 2不包括 2。相当于 // println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) (0L until file.getLen by maxSplitBytes).map &#123; offset =&gt; // 计算文件剩余的量 val remaining = file.getLen - offset // 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining // 位置信息 val hosts = getBlockHosts(blockLocations, offset, size) PartitionedFile( partition.values, file.getPath.toUri.toString, offset, size, hosts) &#125; &#125; else &#123; // 不可分割的话，那即是一个文件一个分区 val hosts = getBlockHosts(blockLocations, 0, file.getLen) Seq(PartitionedFile( partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &#125; &#125; &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) val partitions = new ArrayBuffer[FilePartition] val currentFiles = new ArrayBuffer[PartitionedFile] var currentSize = 0L /** Close the current partition and move to the next. */ def closePartition(): Unit = &#123; if (currentFiles.nonEmpty) &#123; val newPartition = FilePartition( partitions.size, currentFiles.toArray.toSeq) // Copy to a new Array. partitions += newPartition &#125; currentFiles.clear() currentSize = 0 &#125; // Assign files to partitions using "Next Fit Decreasing" splitFiles.foreach &#123; file =&gt; if (currentSize + file.length &gt; maxSplitBytes) &#123; closePartition() &#125; // Add the given file to the current partition. currentSize += file.length + openCostInBytes currentFiles += file &#125; closePartition() println("FileScanRDD partitions size : "+partitions.size) new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &#125; 找到一列中的中位数https://spark.apache.org/docs/latest/api/sql/index.htmldf函数： approxQuantilesql函数： percentile_approx 自定义数据源ServiceLoaderhttps://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3wServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。要配置在相关项目的固定目录下：resources/META-INF/services/接口全称。这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：SparkContext类1234567891011 private def getClusterManager(url: String): Option[ExternalClusterManager] = &#123; val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size &gt; 1) &#123; throw new SparkException( s"Multiple external cluster managers registered for the url $url: $serviceLoaders") &#125; serviceLoaders.headOption &#125;&#125; 配置是在spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。简单测试首先实现一个接口1234567package bigdata.spark.services;public interface DoSomething &#123; //可以制定实现类名加载 public String shortName(); public void doSomeThing();&#125; 然后将接口配置在resources/META-INF/services/bigdata.spark.services.DoSomething文件内容：实现该接口12345678910111213package bigdata.spark.services;public class SayHello implements DoSomething &#123; @Override public String shortName() &#123; return "SayHello"; &#125; @Override public void doSomeThing() &#123; System.out.println("hello !!!"); &#125;&#125; 测试12345678910111213package bigdata.spark.services;import java.util.ServiceLoader;public class test &#123; static ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class); public static void main(String[] args)&#123; for(DoSomething sayhello : loader)&#123; //要加载的类名称我们可以制定 if(sayhello.shortName().equalsIgnoreCase("SayHello"))&#123; sayhello.doSomeThing(); &#125; &#125; &#125;&#125; 这个主要是为讲自定义数据源作准备。 https://articles.zsxq.com/id_702s32f46zet.html首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** Given a provider name, look up the data source class definition. */ def lookupDataSource(provider: String): Class[_] = &#123; val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) // 指定路径加载，默认加载类名是DefaultSource val provider2 = s"$provider1.DefaultSource" val loader = Utils.getContextOrSparkClassLoader // ServiceLoader加载 val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader) try &#123; serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match &#123; // the provider format did not match any given registered aliases case Nil =&gt; try &#123; Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match &#123; case Success(dataSource) =&gt; // Found the data source using fully qualified path dataSource case Failure(error) =&gt; if (provider1.toLowerCase == "orc" || provider1.startsWith("org.apache.spark.sql.hive.orc")) &#123; throw new AnalysisException( "The ORC data source must be used with Hive support enabled") &#125; else if (provider1.toLowerCase == "avro" || provider1 == "com.databricks.spark.avro") &#123; throw new AnalysisException( s"Failed to find data source: $&#123;provider1.toLowerCase&#125;. Please find an Avro " + "package at http://spark.apache.org/third-party-projects.html") &#125; else &#123; throw new ClassNotFoundException( s"Failed to find data source: $provider1. Please find packages at " + "http://spark.apache.org/third-party-projects.html", error) &#125; &#125; &#125; catch &#123; case e: NoClassDefFoundError =&gt; // This one won't be caught by Scala NonFatal // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"$className was removed in Spark 2.0. " + "Please check if your library is compatible with Spark 2.0", e) &#125; else &#123; throw e &#125; &#125; case head :: Nil =&gt; // there is exactly one registered alias head.getClass case sources =&gt; // There are multiple registered aliases for the input sys.error(s"Multiple sources found for $provider1 " + s"($&#123;sources.map(_.getClass.getName).mkString(", ")&#125;), " + "please specify the fully qualified class name.") &#125; &#125; catch &#123; case e: ServiceConfigurationError if e.getCause.isInstanceOf[NoClassDefFoundError] =&gt; // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getCause.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"Detected an incompatible DataSourceRegister. " + "Please remove the incompatible library from classpath or upgrade it. " + s"Error: $&#123;e.getMessage&#125;", e) &#125; else &#123; throw e &#125; &#125; &#125; 其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。 主要思路是： 实现DefaultSource。 实现工厂类。 实现具体的数据加载类。 首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：12345678package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.sources.v2.&#123;DataSourceOptions, DataSourceV2, ReadSupport&#125;class DefaultSource extends DataSourceV2 with ReadSupport &#123; def createReader(options: DataSourceOptions) = new SimpleDataSourceReader()&#125; 然后是，要实现DataSourceReader，负责创建阅读器工厂：12345678910111213141516package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReaderFactory, DataSourceReader&#125;import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;class SimpleDataSourceReader extends DataSourceReader &#123; def readSchema() = StructType(Array(StructField("value", StringType))) def createDataReaderFactories = &#123; val factoryList = new java.util.ArrayList[DataReaderFactory[Row]] factoryList.add(new SimpleDataSourceReaderFactory()) factoryList &#125;&#125; 数据源的具体实现类：12345678910111213141516171819202122package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReader, DataReaderFactory&#125;class SimpleDataSourceReaderFactory extends DataReaderFactory[Row] with DataReader[Row] &#123; def createDataReader = new SimpleDataSourceReaderFactory() val values = Array("1", "2", "3", "4", "5") var index = 0 def next = index &lt; values.length def get = &#123; val row = Row(values(index)) index = index + 1 row &#125; def close() = Unit&#125; 使用我们默认的数据源：12345678910111213141516171819202122232425262728package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]") .set("yarn.resourcemanager.hostname", "mt-mdh.local") .set("spark.executor.instances","2") .setJars(List("/opt/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar")) val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() val simpleDf = spark.read .format("bigdata.spark.SparkSQL.DataSources") .load() simpleDf.show() spark.stop() &#125;&#125; format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。 删除多个字段123def dropColumns(columns: Seq[String]):DataFAME=&#123; columns.foldLeft(dataFame)((df,column)=&gt; df.drop(column))&#125; spark sql 窗口函数https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw最近理了下spark sql 中窗口函数的知识，打算开几篇文章讲讲，窗口函数有很多应用场景，比如说炒股的时候有个5日移动均线，或者让你对一个公司所有销售所有部门按照销售业绩进行排名等等，都是要用到窗口函数，在spark sql中，窗口函数和聚合函数的区别，之前文章中也提到过，就是聚合函数是按照你聚合的维度，每个分组中算出来一个聚合值，而窗口函数是对每一行，都根据当前窗口（5日均线就是今日往前的5天组成的窗口），都聚合出一个值。 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势2 spark sql 中窗口函数深入理解3 不是UDF，也不是UDAF，教你自定义一个窗口函数（UDWF）4 从 spark sql 源码层面理解窗口函数 什么是简单移动平均值简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。例子12345678910111213141516171819202122232425262728293031val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)) .toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ /** * 这个 window spec 中，数据根据用户(customer)来分去。 * 每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行) * ，每一个滑动的窗口总用有3行 */ val wSpce = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("movinAvg", avg("user_cnt").over(wSpce)).show() spark.stop() 结果：2~3~212345678910+----+---------+--------+------------------+|site| date|user_cnt| movinAvg|+----+---------+--------+------------------+| 站点1|201902025| 50| 70.0|| 站点1|201902026| 90| 80.0|| 站点1|201902026| 100| 95.0|| 站点2|201902027| 70| 65.0|| 站点2|201902028| 60|56.666666666666664|| 站点2|201902029| 40| 50.0|+----+---------+--------+------------------+ 窗口函数和窗口特征定义正如上述例子中，窗口函数主要包含两个部分： 指定窗口特征（wSpec）： “partitionyBY” 定义数据如何分组；在上面的例子中，是用户 site “orderBy” 定义分组中的排序 “rowsBetween” 定义窗口的大小 指定窗口函数函数： 指定窗口函数函数，你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数. 累计汇总1234567891011121314151617181920212223242526272829val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate()import spark.implicits._val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)) .toDF("site", "date", "user_cnt")import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions._/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */val wSpce = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, 1)df.withColumn("cumsum", sum("user_cnt").over(wSpce)).show()spark.stop() 结果12345678910+----+---------+--------+------+|site| date|user_cnt|cumsum|+----+---------+--------+------+| 站点1|201902025| 50| 140|| 站点1|201902026| 90| 240|| 站点1|201902026| 100| 240|| 站点2|201902027| 70| 130|| 站点2|201902028| 60| 170|| 站点2|201902029| 40| 170|+----+---------+--------+------+ 前一行数据123456789101112131415161718192021222324252627val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)).toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ /** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */ val wSpce = Window.partitionBy("site").orderBy("date") df.withColumn("preUserCnt", lag(df("user_cnt"),1).over(wSpce)).show() spark.stop() 结果12345678910+----+---------+--------+----------+|site| date|user_cnt|preUserCnt|+----+---------+--------+----------+| 站点1|201902025| 50| null|| 站点1|201902026| 90| 50|| 站点1|201902026| 100| 90|| 站点2|201902027| 70| null|| 站点2|201902028| 60| 70|| 站点2|201902029| 40| 60|+----+---------+--------+----------+ 如果计算环比的时候，是不是特别有用啊？！ 在介绍几个常用的行数： first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到 lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件 排名1234567891011121314151617181920212223val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)).toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ val wSpce = Window.partitionBy("site").orderBy("date") df.withColumn("rank", rank().over(wSpce)).show() 结果12345678910+----+---------+--------+----+|site| date|user_cnt|rank|+----+---------+--------+----+| 站点1|201902025| 50| 1|| 站点1|201902026| 90| 2|| 站点1|201902026| 100| 2|| 站点2|201902027| 70| 1|| 站点2|201902028| 60| 2|| 站点2|201902029| 40| 3|+----+---------+--------+----+ 这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。 自定义窗口函数https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQhttps://github.com/zheniantoushipashi/spark-udwf-session 背景在使用 spark sql 的时候，有时候默认提供的sql 函数可能满足不了需求，这时候可以自定义一些函数，可以自定义 UDF 或者UDAF。 UDF 在sql中只是简单的处理转换一些字段，类似默认的trim 函数把一个字符串类型的列的头尾空格去掉， UDAF函数不同于UDF，是在sql聚合语句中使用的函数，必须配合 GROUP BY 一同使用，类似默认提供的count，sum函数，但是还有一种自定义函数叫做 UDWF， 这种一般人就不知道了，这种叫做窗口自定义函数，不了解窗口函数的，可以参考 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势 ，或者官方的介绍 https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html 窗口函数是 SQL 中一类特别的函数。和聚合函数相似，窗口函数的输入也是多行记录。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口 这里怎么理解一个窗口呢，spark君在这里得好好的解释解释，一个窗口是怎么定义的， 窗口语句中，partition by用来指定分区的列，在同一个分区的行属于同一个窗口 order by用来指定窗口内的多行，如何排序 windowing_clause 用来指定开窗方式，在spark sql 中开窗方式有那么几种 一个分区中的所有行作为一个窗口:UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING（上下都没有边界)，这种情况下，spark sql 会把所有行作为一个输入，进行一次求值 Growing frame:UNBOUNDED PRECEDING AND ….（上无边界）, 这种就是不断的把当前行加入的窗口中，而不删除， 例子：.rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行，在数据迭代过程中，不断的把当前行加入的窗口中。 Shrinking frame：… AND UNBOUNDED FOLLOWING（下无边界）和Growing frame 相反，窗口不断的把迭代到的当前行从窗口中删除掉。 Moving frame：滑动的窗口，举例：.rowsBetween(-1, 1) 就是指窗口定义从 -1(当前行前一行)到 1(当前行后一行) ，每一个滑动的窗口总用有3行 Offset frame： 窗口中只有一条数据，就是偏移当前行一定距离的那一行，举例：lag(field, n): 就是取从当前行往前的n个行的字段值 这里就针对窗口函数就介绍这么多，如果不懂请参考相关文档，加强理解，我们在平时使用 spark sql 的过程中，会发现有很多教你自定义 UDF 和 UDAF 的教程，却没有针对UDWF的教程，这是为啥呢，这是因为 UDF 和UDAF 都作为上层API暴露给用户了，使用scala很简单就可以写一个函数出来，但是UDWF没有对上层用户暴露，只能使用 Catalyst expressions. 也就是Catalyst框架底层的表达式语句才可以定义，如果没有对源码有很深入的研究，根本就搞不出来。spark 君在工作中写了一些UDWF的函数，但是都比较复杂，不太好单独抽出来作为一个简明的例子给大家讲解，挑一个网上的例子来进行说明，这个例子 spark君亲测可用。 窗口函数的使用场景我们来举个实际例子来说明 窗口函数的使用场景，在网站的统计指标中，有一个概念叫做用户会话，什么叫做用户会话呢，我来说明一下，我们在网站服务端使用用户session来管理用户状态，过程如下 1) 服务端session是用户第一次访问应用时，服务器就会创建的对象，代表用户的一次会话过程，可以用来存放数据。服务器为每一个session都分配一个唯一的sessionid，以保证每个用户都有一个不同的session对象。 2）服务器在创建完session后，会把sessionid通过cookie返回给用户所在的浏览器，这样当用户第二次及以后向服务器发送请求的时候，就会通过cookie把sessionid传回给服务器，以便服务器能够根据sessionid找到与该用户对应的session对象。 3）session通常有失效时间的设定，比如1个小时。当失效时间到，服务器会销毁之前的session，并创建新的session返回给用户。但是只要用户在失效时间内，有发送新的请求给服务器，通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长1个小时。 也就是说如果用户在1个超过一个小时不产生用户事件，当前会话就结束了，如果后续再产生用户事件，就当做新的用户会话，我们现在就使用spark sql 来统计用户的会话数，首先我们先加一个列，作为当前列的session，然后再 distinct 这个列就得到用户的会话数了，所以关键是怎么加上这个newSession 这个列，这种场景就很适合使用窗口函数来做统计，因为判断当前是否是一个新会话的依据，需要依赖当前行的前一行的时间戳和当前行的时间戳的间隔来判断，下面的表格可以帮助你理解这个概念，例子中有3列数据，用户，event字段代表用户访问了一个页面产生了一个用户事件，time字段代表访问页面的时间戳： user event time session user1 page1 10:12 session1(new session) user1 page2 10:20 session1(same session,8 minutes from last event) user1 page1 11:13 session1(same session,53 minutes from last event) user1 page3 14:12 session1(new session,3 minutes from last event) 上面只有一个用户，如果多个用户，可以使用 partition by 来进行分区。 深入研究构造数据12345678910111213141516171819case class UserActivityData(user:String, ts:Long, session:String) val st = System.currentTimeMillis() val one_minute = 60 * 1000 val d = Array[UserActivityData]( UserActivityData("user1", st, "f237e656-1e53-4a24-9ad5-2b4576a4125d"), UserActivityData("user2", st + 5*one_minute, null), UserActivityData("user1", st + 10*one_minute, null), UserActivityData("user1", st + 15*one_minute, null), UserActivityData("user2", st + 15*one_minute, null), UserActivityData("user1", st + 140*one_minute, null), UserActivityData("user1", st + 160*one_minute, null)) "a CustomWindowFunction" should "correctly create a session " in &#123; val sqlContext = new SQLContext(sc) val df = sqlContext.createDataFrame(sc.parallelize(d)) val specs = Window.partitionBy(f.col("user")).orderBy(f.col("ts").asc) val res = df.withColumn( "newsession", MyUDWF.calculateSession(f.col("ts"), f.col("session")) over specs) 怎么使用 spark sql 来统计会话数目呢，因为不同用户产生的是不同的会话，首先使用user字段进行分区，然后按照时间戳进行排序，然后我们需要一个自定义函数来加一个列，这个列的值的逻辑如下：1234IF (no previous event) create new sessionELSE (if cerrent event was past session window)THEN create new sessionELSE use current session 运行结果如下：我们使用 UUID 来作为会话id， 当后一行的时间戳和前一行的时间戳间隔大于1小时的时候，就创建一个新的会话id作为列值，否则使用老的会话id作为列值。 这种就涉及到状态，我们在内部需要维护的状态数据 当前的session ID 当前session的最后活动事件的时间戳 下面我们就看看这个怎样自定义 caculateSession 这个窗口函数，先自定义一个静态对象：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.UUIDimport org.apache.spark.sql.&#123;Column, Row&#125;import org.apache.spark.sql.catalyst.expressions.&#123;Add, AggregateWindowFunction, AttributeReference, Expression, If, IsNotNull, LessThanOrEqual, Literal, ScalaUDF, Subtract&#125;import org.apache.spark.sql.types._import org.apache.spark.unsafe.types.UTF8Stringobject MyUDWF &#123; val defaultMaxSessionLengthms = 3600 * 1000 case class SessionUDWF(timestamp:Expression, session:Expression, sessionWindow:Expression = Literal(defaultMaxSessionLengthms)) extends AggregateWindowFunction &#123; self: Product =&gt; override def children: Seq[Expression] = Seq(timestamp, session) override def dataType: DataType = StringType protected val zero = Literal( 0L ) protected val nullString = Literal(null:String) protected val curentSession = AttributeReference("currentSession", StringType, nullable = true)() protected val previousTs = AttributeReference("lastTs", LongType, nullable = false)() override val aggBufferAttributes: Seq[AttributeReference] = curentSession :: previousTs :: Nil protected val assignSession = If(LessThanOrEqual(Subtract(timestamp, aggBufferAttributes(1)), sessionWindow), aggBufferAttributes(0), // if ScalaUDF( createNewSession, StringType, children = Nil)) override val initialValues: Seq[Expression] = nullString :: zero :: Nil override val updateExpressions: Seq[Expression] = If(IsNotNull(session), session, assignSession) :: timestamp :: Nil override val evaluateExpression: Expression = aggBufferAttributes(0) override def prettyName: String = "makeSession" &#125; protected val createNewSession = () =&gt; org.apache.spark.unsafe.types.UTF8String.fromString(UUID.randomUUID().toString) def calculateSession(ts:Column,sess:Column): Column = withExpr &#123; SessionUDWF(ts.expr,sess.expr, Literal(defaultMaxSessionLengthms)) &#125; def calculateSession(ts:Column,sess:Column, sessionWindow:Column): Column = withExpr &#123; SessionUDWF(ts.expr,sess.expr, sessionWindow.expr) &#125; private def withExpr(expr: Expression): Column = new Column(expr)&#125; 代码说明： 状态保存在 Seq[AttributeReference]中 重写 initialValues方法进行初始化 updateExpressions 函数针对每一行数据都会调用 spark sql 在迭代处理每一行数据的时候，都会调用 updateExpressions 函数来处理，根据当后一行的时间戳和前一行的时间戳间隔大于1小时来进行不同的逻辑处理，如果不大于，就使用 aggBufferAttributes(0) 中保存的老的sessionid，如果大于，就把 createNewSession 包装为一个scalaUDF作为一个子表达式来创建一个新的sessionID，并且每次都把当前行的时间戳作为用户活动的最后时间戳。 最后包装为静态对象的方法，就可以在spark sql中使用这个自定义窗口函数了，下面是两个重载的方法，一个最大间隔时间使用默认值，一个可以运行用户自定义，perfect 现在，我们就可以拿来用在我们的main函数中了。1234567891011121314151617val st = System.currentTimeMillis()val one_minute = 60 * 1000val d = Array[UserActivityData]( UserActivityData("user1", st, "f237e656-1e53-4a24-9ad5-2b4576a4125d"), UserActivityData("user2", st + 5*one_minute, null), UserActivityData("user1", st + 10*one_minute, null), UserActivityData("user1", st + 15*one_minute, null), UserActivityData("user2", st + 15*one_minute, null), UserActivityData("user1", st + 140*one_minute, null), UserActivityData("user1", st + 160*one_minute, null)) val df = spark.createDataFrame(sc.parallelize(d)) val specs = Window.partitionBy(f.col("user")).orderBy(f.col("ts").asc) val res = df.withColumn( "newsession", MyUDWF.calculateSession(f.col("ts"), f.col("session")) over specs) df.show(20) res.show(20, false) 如果我们学会了自定义 spark 窗口函数，原理是就可以处理一切这种对前后数据依赖的统计需求，不过这种自定义毕竟需要对 spark 源码有很深入的研究才可以，这就需要功力了，希望 spark君的读者可以跟着spark君日益精进。文章中的代码可能不太清楚，完整demo参考 https://github.com/zheniantoushipashi/spark-udwf-session。]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java自定义注解]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解和注释区别注释：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。注解：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。 注解的定义和使用模板注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。 定义格式：修饰符 @interface 注解名{} 使用格式： @注解名;例如：junit测试中的 @Test 复写方法中的@Override 自定义注解注解可以书写的位置定义的注解，可以书写在类的不同位置：类上、成员变量上、成员方法、构造方法等位置。在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置： ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上 ElementType.METHOD:当前的注解可以书写在方法上 ElementType.FIELD:当前的注解可以书写在成员变量（字段）上 ElementType.TYPE:当前的注解可以书写在类或接口上 注解生命周期 由@Retention 声明自己的注解可以存活的时间具体的存活的时间需要通过RetentionPolicy 中提供的值。RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。 注解中的成员变量注解定义的变量格式：数据类型 变量名(); 基本案例自定义注解12345678910111213import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD , ElementType.FIELD&#125;)public @interface MyAnnotation &#123; // 定义注解的变量 String name(); int age(); String sex();&#125; 自定义注解的使用123456789101112public class UseAnnotation &#123; @MyAnnotation(name="zhangsan",age=23,sex="nv") public UseAnnotation()&#123; &#125; @MyAnnotation(name="zhangsan",age=23,sex="nv") private String name; @MyAnnotation(name="zhangsan",age=23,sex="nv") public void show()&#123; &#125;&#125; 经典使用案例（mysql连接驱动）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/* * 自定义注解，封装数据库连接的四个参数 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface MyDriver &#123; String driver(); String url(); String user(); String pwd();&#125;import java.lang.reflect.Method;import java.sql.Connection;import java.sql.DriverManager;/* * 专门负责获取数据库的连接工具类 */public class JDBCUtils &#123; @MyDriver(driver="com.mysql.jdbc.Driver", url="jdbc:mysql://localhost:3306/estore",user="root",pwd="abc") public static Connection getConnection() throws Exception&#123; //获取当前类的class文件 Class clazz = JDBCUtils.class; Method method = clazz.getMethod("getConnection", null); //获取当前方法上的注解信息 if( method.isAnnotationPresent(MyDriver.class) )&#123; MyDriver an = method.getAnnotation(MyDriver.class); String driver = an.driver(); String url = an.url(); String user = an.user(); String pwd = an.pwd(); //获取数据库的连接，加载驱动，获取连接 Class.forName(driver); //获取连接 Connection conn = DriverManager.getConnection(url, user, pwd); return conn; &#125; return null; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; Connection conn = JDBCUtils.getConnection(); System.out.println(conn); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven删除本地仓库lastUpdated文件]]></title>
    <url>%2F2018%2F08%2F13%2Fmaven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[12windows:for /r %i in (*.lastUpdated) do del %i 12linux:find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 查询数据1select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
