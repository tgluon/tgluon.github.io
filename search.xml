<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[scala通过反射获取当前类类名]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[先编写HelloWord.scala文件123456object HelloWorld &#123; def main(args: Array[String]) &#123; // 获取当前类类名 println(this.getClass.getName.stripSuffix("$")) &#125;&#125; 命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class spark源码中的案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.spark.internalimport org.apache.log4j.&#123;Level, LogManager, PropertyConfigurator&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;import org.slf4j.impl.StaticLoggerBinderimport org.apache.spark.util.Utils/** * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows * logging messages at different levels using methods that only evaluate parameters lazily if the * log level is enabled. */private[spark] trait Logging &#123; // Make the log field transient so that objects with Logging can // be serialized and used on another machine @transient private var log_ : Logger = null // Method to get the logger name for this object protected def logName = &#123; // Ignore trailing $'s in the class names for Scala objects this.getClass.getName.stripSuffix("$") &#125; // Method to get or create the logger for this object protected def log: Logger = &#123; if (log_ == null) &#123; initializeLogIfNecessary(false) log_ = LoggerFactory.getLogger(logName) &#125; log_ &#125; // Log methods that take only a String protected def logInfo(msg: =&gt; String) &#123; if (log.isInfoEnabled) log.info(msg) &#125; protected def logDebug(msg: =&gt; String) &#123; if (log.isDebugEnabled) log.debug(msg) &#125; protected def logTrace(msg: =&gt; String) &#123; if (log.isTraceEnabled) log.trace(msg) &#125; protected def logWarning(msg: =&gt; String) &#123; if (log.isWarnEnabled) log.warn(msg) &#125; protected def logError(msg: =&gt; String) &#123; if (log.isErrorEnabled) log.error(msg) &#125; // Log methods that take Throwables (Exceptions/Errors) too protected def logInfo(msg: =&gt; String, throwable: Throwable) &#123; if (log.isInfoEnabled) log.info(msg, throwable) &#125; protected def logDebug(msg: =&gt; String, throwable: Throwable) &#123; if (log.isDebugEnabled) log.debug(msg, throwable) &#125; protected def logTrace(msg: =&gt; String, throwable: Throwable) &#123; if (log.isTraceEnabled) log.trace(msg, throwable) &#125; protected def logWarning(msg: =&gt; String, throwable: Throwable) &#123; if (log.isWarnEnabled) log.warn(msg, throwable) &#125; protected def logError(msg: =&gt; String, throwable: Throwable) &#123; if (log.isErrorEnabled) log.error(msg, throwable) &#125; protected def isTraceEnabled(): Boolean = &#123; log.isTraceEnabled &#125; protected def initializeLogIfNecessary(isInterpreter: Boolean): Unit = &#123; if (!Logging.initialized) &#123; Logging.initLock.synchronized &#123; if (!Logging.initialized) &#123; initializeLogging(isInterpreter) &#125; &#125; &#125; &#125; private def initializeLogging(isInterpreter: Boolean): Unit = &#123; // Don't use a logger in here, as this is itself occurring during initialization of a logger // If Log4j 1.2 is being used, but is not initialized, load a default properties file val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr // This distinguishes the log4j 1.2 binding, currently // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently // org.apache.logging.slf4j.Log4jLoggerFactory val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass) if (usingLog4j12) &#123; val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements // scalastyle:off println if (!log4j12Initialized) &#123; val defaultLogProps = "org/apache/spark/log4j-defaults.properties" Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match &#123; case Some(url) =&gt; PropertyConfigurator.configure(url) System.err.println(s"Using Spark's default log4j profile: $defaultLogProps") case None =&gt; System.err.println(s"Spark was unable to load $defaultLogProps") &#125; &#125; if (isInterpreter) &#123; // Use the repl's main class to define the default log level when running the shell, // overriding the root logger's config if they're different. val rootLogger = LogManager.getRootLogger() val replLogger = LogManager.getLogger(logName) val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN) if (replLevel != rootLogger.getEffectiveLevel()) &#123; System.err.printf("Setting default log level to \"%s\".\n", replLevel) System.err.println("To adjust logging level use sc.setLogLevel(newLevel). " + "For SparkR, use setLogLevel(newLevel).") rootLogger.setLevel(replLevel) &#125; &#125; // scalastyle:on println &#125; Logging.initialized = true // Force a call into slf4j to initialize it. Avoids this happening from multiple threads // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html log &#125;&#125;private object Logging &#123; @volatile private var initialized = false val initLock = new Object() try &#123; // We use reflection here to handle the case where users remove the // slf4j-to-jul bridge order to route their logs to JUL. val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler") bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null) val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean] if (!installed) &#123; bridgeClass.getMethod("install").invoke(null) &#125; &#125; catch &#123; case e: ClassNotFoundException =&gt; // can't log anything yet so just fail silently &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql详解]]></title>
    <url>%2F2019%2F01%2F04%2Fsparksql%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark SQL的Dataset基本操作说明：文章来自https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解 1. 基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。 1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是1type DataFrame = Dataset[Row] 结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。 首先要创建一个SparkSession:12345678910111213141516171819val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]").set("yarn.resourcemanager.hostname", "localhost") // executor的实例数.set("spark.executor.instances","2") .set("spark.default.parallelism","4") // sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。.set("spark.sql.shuffle.partitions","4").setJars(List("/Users/meitu/Desktop/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar"))val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() 创建dataset123456val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100), ("Warsaw", 2017, 200), ("Warsaw", 2015, 100), ("Warsaw", 2017, 200), ("Beijing", 2017, 200), ("Beijing", 2016, 200), ("Beijing", 2015, 200), ("Beijing", 2014, 200), ("Warsaw", 2014, 200), ("Boston", 2017, 50), ("Boston", 2016, 50), ("Boston", 2015, 50), ("Boston", 2014, 150))).toDF("city", "year", "amount") 使用函数的时候要导入包：1import org.apache.spark.sql.functions.&#123;col,expr&#125; 2.select列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。使用col函数，可以直接对列进行一些逻辑操作。12sales.select("city","year","amount").show(1)sales.select(col("city"),col("amount")+1).show(1) 3. selectExpr参数是字符串，且直接可以使用表达式。也可以使用select+expr函数来替代。12sales.selectExpr("city","year as date","amount+1").show(10)sales.select(expr("city"),expr("year as date"),expr("amount+1")).show(10) 4. filter参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。123sales.filter(col("amount")&gt;150).show()sales.filter(row=&gt;&#123; row.getInt(2)&gt;150&#125;).show(10)sales.filter("amount &gt; 150 ").show(10) 5. where类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。12sales.where(col("amount")&gt;150).show()sales.where("amount &gt; 150 ").show() 6. group by主要是以count和agg聚合函数为例讲解groupby函数。12sales.groupBy("city").count().show(10)sales.groupBy(col("city")).agg(sum("amount").as("total")).show(10) 7.union两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。1sales.union(sales).groupBy("city").count().show() 8.joinjoin操作相对比较复杂，具体如下：123456789101112131415161718192021222324252627282930// 相同的列进行join sales.join(sales,"city").show(10) // 多列joinsales.join(sales,Seq("city","year")).show() /* 指定join类型， join 类型可以选择: `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti`. */ // 内部joinsales.join(sales,Seq("city","year"),"inner").show() /* join条件 ：可以在join方法里放入join条件，也可以使用where,这两种情况都要求字段名称不一样。*/ sales.join(sales, col("city").alias("city1") === col("city")).show() sales.join(sales).where(col("city").alias("city1") === col("city")).show() /* dataset的self join 此处使用where作为条件，需要增加配置.set("spark.sql.crossJoin.enabled","true") 也可以加第三个参数，join类型，可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` */ sales.join(sales,sales("city") === sales("city")).show() sales.join(sales).where(sales("city") === sales("city")).show()/* joinwith,可以指定第三个参数，join类型，类型可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`。 */ sales.joinWith(sales,sales("city") === sales("city"),"inner").show() 输出结果： 9. order byorderby 全局有序，其实用的还是sort12sales.orderBy(col("year").desc,col("amount").asc).show()sales.orderBy("city","year").show() 10.sort全局排序，直接替换掉8小结的orderby即可。 11.sortwithinpartition在分区内部进行排序，局部排序。12sales.sortWithinPartitions(col("year").desc,col("amount").asc).show()sales.sortWithinPartitions("city","year").show() 可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。 12. withColumn1234567891011/* withColumn 假如列，存在就替换，不存在新增 withColumnRenamed 对已有的列进行重命名 *///相当于给原来amount列，+1sales.withColumn("amount",col("amount")+1).show()// 对amount列+1，然后将值增加到一个新列 amount1sales.withColumn("amount1",col("amount")+1).show()// 将amount列名，修改为amount1sales.withColumnRenamed("amount","amount1").show() 13. foreach这个跟rdd的foreach一样，元素类型是row。123sales.foreach(row=&gt;&#123; println(row.getString(0))&#125;) 14. foreachPartition跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。12345678sales.foreachPartition(partition=&gt;&#123; //打开数据库链接等 partition.foreach(each=&gt;&#123; println(each.getString(0)) //插入数据库 &#125;) //关闭数据库链接 &#125;) 15. distinct针对dataset的行去重，返回的是所有行都不重复的dataset。1sales.distinct().show(10) 16. dropDuplicates这个适用于dataset有唯一的主键，然后对主键进行去重。1234val before = sales.count()val after = sales.dropDuplicates("city").count()println("before ====&gt; " +before)println("after ====&gt; "+after) 17. drop删除一列，或者多列，这是一个变参数算子。12345sales.drop("city").show()打印出来schema信息如下：root|-- year: integer (nullable = false)|-- amount: integer (nullable = false) 18.printSchema输出dataset的schema信息1234567891011sales.printSchema()输出结果如下：root|-- city: string (nullable = true)|-- year: integer (nullable = false)|-- amount: integer (nullable = false) 17.explain()打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程123456sales.orderBy(col("year").desc,col("amount").asc).explain()执行计划输出如下：== Physical Plan ==*(1) Sort [year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST], true, 0+- Exchange rangepartitioning(year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST, 3)+- LocalTableScan [city#6, year#7, amount#8] Spark SQL入门到精通之第二篇Dataset的复杂操作本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：cube，rollup，pivot操作。 1. cube简单的理解就是维度及度量组成的数据体。1234sales.cube("city","year") .agg(sum("amount")) .sort(col("city").desc_nulls_first,col("year").desc_nulls_first) .show() 举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：123456789101112131415val city_year = sales.groupBy("city","year").agg(sum("amount"))val city = sales.groupBy("city") .agg(sum("amount") as "amount") .select(col("city"), lit(null) as "year", col("amount"))val year = sales.groupBy("year") .agg(sum("amount") as "amount") .select( lit(null) as "city",col("year"), col("amount"))val none = sales .groupBy().agg(sum("amount") as "amount") .select(lit(null) as "city", lit(null) as "year", col("amount")) city_year.union(city).union(year).union(none) .sort(desc_nulls_first("city"), desc_nulls_first("year")) .show() 2. rollup这里也是以案例开始，代码如下：12345678910111213val expenses = spark.createDataFrame(Seq( ((2012, Month.DECEMBER, 12), 5), ((2016, Month.AUGUST, 13), 10), ((2017, Month.MAY, 27), 15)) .map &#123; case ((yy, mm, dd), a) =&gt; (LocalDate.of(yy, mm, dd), a) &#125; .map &#123; case (d, a) =&gt; (d.toString, a) &#125; .map &#123; case (d, a) =&gt; (Date.valueOf(d), a) &#125;).toDF("date", "amount")// rollup time!val res = expenses .rollup(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount") .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：123456789101112val year_month = expenses.groupBy(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount")val yearOnly = expenses.groupBy(year(col("date")) as "year") .agg(sum("amount") as "amount") .select(col("year"), lit(null) as "month", col("amount"))val none = expenses.groupBy() .agg(sum("amount") as "amount") .select(lit(null) as "year", lit(null) as "month", col("amount"))year_month.union(yearOnly).union(none) .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 3. pivot旋转操作12345678910111213val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Boston", 2015, 50,"Boston"), ("Boston", 2016, 150,"Boston"), ("Toronto", 2017, 50,"Toronto"))).toDF("city", "year", "amount","test")sales.groupBy("year").pivot("city",Seq("Warsaw","Boston","Toronto")) .agg(sum("amount") as "amount") .show() 思路就是首先对year分组，然后旋转city字段，只取:“Warsaw”,”Boston”,”Toronto”然后对amount进行聚合操作 源码:Spark SQL 分区特性第一弹常见RDD分区Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。举几个浪尖在星球里分享比较多的例子，比如：Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据首先是由Seq数据集合生成一个Dataset1234567891011121314151617181920212223242526val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 110), ("Warsaw", 2017, 10), ("Warsaw", 2015, 100), ("Warsaw", 2015, 50), ("Warsaw", 2015, 80), ("Warsaw", 2015, 100), ("Warsaw", 2015, 130), ("Warsaw", 2015, 160), ("Warsaw", 2017, 200), ("Beijing", 2017, 100), ("Beijing", 2016, 150), ("Beijing", 2015, 50), ("Beijing", 2015, 30), ("Beijing", 2015, 10), ("Beijing", 2014, 200), ("Beijing", 2014, 170), ("Boston", 2017, 50), ("Boston", 2017, 70), ("Boston", 2017, 110), ("Boston", 2017, 150), ("Boston", 2017, 180), ("Boston", 2016, 30), ("Boston", 2015, 200), ("Boston", 2014, 20) )).toDF("city", "year", "amount") 将Dataset存处为partquet格式的hive表，分两种情况：用city和year字段分区1sales.write.partitionBy("city","year").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCityAndYear") 用city字段分区1sales.write.partitionBy("city").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCity") 读取数据采用的是1val res = spark.read.parquet("/user/hive/warehouse/parquettestcity") 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40Dataset的分区数是由参数：目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4Dataset的分区数是由参数：1println("partition size = "+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private def createNonBucketedReadRDD( readFile: (PartitionedFile) =&gt; Iterator[InternalRow], selectedPartitions: Seq[PartitionDirectory], fsRelation: HadoopFsRelation): RDD[InternalRow] = &#123; /* selectedPartitions 的大小代表目录数目 */ println("selectedPartitions.size : "+ selectedPartitions.size) val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes // spark.default.parallelism val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism // 计算文件总大小，单位字节数 val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum //计算平均每个并行度读取数据大小 val bytesPerCore = totalBytes / defaultParallelism // 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 // 然后，比较spark.sql.files.maxPartitionBytes 取小者 val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " + s"open cost is considered as scanning $openCostInBytes bytes.") // 这对目录遍历 val splitFiles = selectedPartitions.flatMap &#123; partition =&gt; partition.files.flatMap &#123; file =&gt; val blockLocations = getBlockLocations(file) //判断文件类型是否支持分割，以parquet为例，是支持分割的 if (fsRelation.fileFormat.isSplitable( fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123; // eg. 0 until 2不包括 2。相当于 // println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) (0L until file.getLen by maxSplitBytes).map &#123; offset =&gt; // 计算文件剩余的量 val remaining = file.getLen - offset // 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining // 位置信息 val hosts = getBlockHosts(blockLocations, offset, size) PartitionedFile( partition.values, file.getPath.toUri.toString, offset, size, hosts) &#125; &#125; else &#123; // 不可分割的话，那即是一个文件一个分区 val hosts = getBlockHosts(blockLocations, 0, file.getLen) Seq(PartitionedFile( partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &#125; &#125; &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) val partitions = new ArrayBuffer[FilePartition] val currentFiles = new ArrayBuffer[PartitionedFile] var currentSize = 0L /** Close the current partition and move to the next. */ def closePartition(): Unit = &#123; if (currentFiles.nonEmpty) &#123; val newPartition = FilePartition( partitions.size, currentFiles.toArray.toSeq) // Copy to a new Array. partitions += newPartition &#125; currentFiles.clear() currentSize = 0 &#125; // Assign files to partitions using "Next Fit Decreasing" splitFiles.foreach &#123; file =&gt; if (currentSize + file.length &gt; maxSplitBytes) &#123; closePartition() &#125; // Add the given file to the current partition. currentSize += file.length + openCostInBytes currentFiles += file &#125; closePartition() println("FileScanRDD partitions size : "+partitions.size) new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &#125; 找到一列中的中位数https://spark.apache.org/docs/latest/api/sql/index.htmldf函数： approxQuantilesql函数： percentile_approx 自定义数据源ServiceLoaderhttps://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3wServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。要配置在相关项目的固定目录下：resources/META-INF/services/接口全称。这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：SparkContext类1234567891011 private def getClusterManager(url: String): Option[ExternalClusterManager] = &#123; val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size &gt; 1) &#123; throw new SparkException( s"Multiple external cluster managers registered for the url $url: $serviceLoaders") &#125; serviceLoaders.headOption &#125;&#125; 配置是在spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。简单测试首先实现一个接口1234567package bigdata.spark.services;public interface DoSomething &#123; //可以制定实现类名加载 public String shortName(); public void doSomeThing();&#125; 然后将接口配置在resources/META-INF/services/bigdata.spark.services.DoSomething文件内容：实现该接口12345678910111213package bigdata.spark.services;public class SayHello implements DoSomething &#123; @Override public String shortName() &#123; return "SayHello"; &#125; @Override public void doSomeThing() &#123; System.out.println("hello !!!"); &#125;&#125; 测试12345678910111213package bigdata.spark.services;import java.util.ServiceLoader;public class test &#123; static ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class); public static void main(String[] args)&#123; for(DoSomething sayhello : loader)&#123; //要加载的类名称我们可以制定 if(sayhello.shortName().equalsIgnoreCase("SayHello"))&#123; sayhello.doSomeThing(); &#125; &#125; &#125;&#125; 这个主要是为讲自定义数据源作准备。 https://articles.zsxq.com/id_702s32f46zet.html首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** Given a provider name, look up the data source class definition. */ def lookupDataSource(provider: String): Class[_] = &#123; val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) // 指定路径加载，默认加载类名是DefaultSource val provider2 = s"$provider1.DefaultSource" val loader = Utils.getContextOrSparkClassLoader // ServiceLoader加载 val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader) try &#123; serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match &#123; // the provider format did not match any given registered aliases case Nil =&gt; try &#123; Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match &#123; case Success(dataSource) =&gt; // Found the data source using fully qualified path dataSource case Failure(error) =&gt; if (provider1.toLowerCase == "orc" || provider1.startsWith("org.apache.spark.sql.hive.orc")) &#123; throw new AnalysisException( "The ORC data source must be used with Hive support enabled") &#125; else if (provider1.toLowerCase == "avro" || provider1 == "com.databricks.spark.avro") &#123; throw new AnalysisException( s"Failed to find data source: $&#123;provider1.toLowerCase&#125;. Please find an Avro " + "package at http://spark.apache.org/third-party-projects.html") &#125; else &#123; throw new ClassNotFoundException( s"Failed to find data source: $provider1. Please find packages at " + "http://spark.apache.org/third-party-projects.html", error) &#125; &#125; &#125; catch &#123; case e: NoClassDefFoundError =&gt; // This one won't be caught by Scala NonFatal // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"$className was removed in Spark 2.0. " + "Please check if your library is compatible with Spark 2.0", e) &#125; else &#123; throw e &#125; &#125; case head :: Nil =&gt; // there is exactly one registered alias head.getClass case sources =&gt; // There are multiple registered aliases for the input sys.error(s"Multiple sources found for $provider1 " + s"($&#123;sources.map(_.getClass.getName).mkString(", ")&#125;), " + "please specify the fully qualified class name.") &#125; &#125; catch &#123; case e: ServiceConfigurationError if e.getCause.isInstanceOf[NoClassDefFoundError] =&gt; // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getCause.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"Detected an incompatible DataSourceRegister. " + "Please remove the incompatible library from classpath or upgrade it. " + s"Error: $&#123;e.getMessage&#125;", e) &#125; else &#123; throw e &#125; &#125; &#125; 其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。 主要思路是： 实现DefaultSource。 实现工厂类。 实现具体的数据加载类。 首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：12345678package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.sources.v2.&#123;DataSourceOptions, DataSourceV2, ReadSupport&#125;class DefaultSource extends DataSourceV2 with ReadSupport &#123; def createReader(options: DataSourceOptions) = new SimpleDataSourceReader()&#125; 然后是，要实现DataSourceReader，负责创建阅读器工厂：12345678910111213141516package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReaderFactory, DataSourceReader&#125;import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;class SimpleDataSourceReader extends DataSourceReader &#123; def readSchema() = StructType(Array(StructField("value", StringType))) def createDataReaderFactories = &#123; val factoryList = new java.util.ArrayList[DataReaderFactory[Row]] factoryList.add(new SimpleDataSourceReaderFactory()) factoryList &#125;&#125; 数据源的具体实现类：12345678910111213141516171819202122package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReader, DataReaderFactory&#125;class SimpleDataSourceReaderFactory extends DataReaderFactory[Row] with DataReader[Row] &#123; def createDataReader = new SimpleDataSourceReaderFactory() val values = Array("1", "2", "3", "4", "5") var index = 0 def next = index &lt; values.length def get = &#123; val row = Row(values(index)) index = index + 1 row &#125; def close() = Unit&#125; 使用我们默认的数据源：12345678910111213141516171819202122232425262728package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]") .set("yarn.resourcemanager.hostname", "mt-mdh.local") .set("spark.executor.instances","2") .setJars(List("/opt/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar")) val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() val simpleDf = spark.read .format("bigdata.spark.SparkSQL.DataSources") .load() simpleDf.show() spark.stop() &#125;&#125; format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala特殊符号使用]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[_*符号作用：它告诉编译器将序列类型的单个参数视为可变参数序列。举例：12345public ProcessBuilder(String... command) &#123; this.command = new ArrayList&lt;&gt;(command.length); for (String arg : command) this.command.add(arg); &#125; 12val commandSeq = Seq(command.mainClass) ++ command.argumentsval builder = new ProcessBuilder(commandSeq: _*) ::该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。1234567891011121314151617181920212223242526272829303132def main(args: Array[String]) &#123; args.toList match &#123; case workerUrl :: userJar :: mainClass :: extraArgs =&gt; val conf = new SparkConf() val rpcEnv = RpcEnv.create("Driver", Utils.localHostName(), 0, conf, new SecurityManager(conf)) rpcEnv.setupEndpoint("workerWatcher", new WorkerWatcher(rpcEnv, workerUrl)) val currentLoader = Thread.currentThread.getContextClassLoader val userJarUrl = new File(userJar).toURI().toURL() val loader = if (sys.props.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) &#123; new ChildFirstURLClassLoader(Array(userJarUrl), currentLoader) &#125; else &#123; new MutableURLClassLoader(Array(userJarUrl), currentLoader) &#125; Thread.currentThread.setContextClassLoader(loader) // Delegate to supplied main class val clazz = Utils.classForName(mainClass) val mainMethod = clazz.getMethod("main", classOf[Array[String]]) mainMethod.invoke(null, extraArgs.toArray[String]) rpcEnv.shutdown() case _ =&gt; // scalastyle:off println System.err.println("Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]") // scalastyle:on println System.exit(-1) &#125; &#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java自定义注解]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解和注释区别注释：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。注解：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。 注解的定义和使用模板注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。 定义格式：修饰符 @interface 注解名{} 使用格式： @注解名;例如：junit测试中的 @Test 复写方法中的@Override 自定义注解注解可以书写的位置定义的注解，可以书写在类的不同位置：类上、成员变量上、成员方法、构造方法等位置。在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置： ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上 ElementType.METHOD:当前的注解可以书写在方法上 ElementType.FIELD:当前的注解可以书写在成员变量（字段）上 ElementType.TYPE:当前的注解可以书写在类或接口上 注解生命周期 由@Retention 声明自己的注解可以存活的时间具体的存活的时间需要通过RetentionPolicy 中提供的值。RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。 注解中的成员变量注解定义的变量格式：数据类型 变量名(); 基本案例自定义注解12345678910111213import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD , ElementType.FIELD&#125;)public @interface MyAnnotation &#123; // 定义注解的变量 String name(); int age(); String sex();&#125; 自定义注解的使用123456789101112public class UseAnnotation &#123; @MyAnnotation(name="zhangsan",age=23,sex="nv") public UseAnnotation()&#123; &#125; @MyAnnotation(name="zhangsan",age=23,sex="nv") private String name; @MyAnnotation(name="zhangsan",age=23,sex="nv") public void show()&#123; &#125;&#125; 经典使用案例（mysql连接驱动）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/* * 自定义注解，封装数据库连接的四个参数 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface MyDriver &#123; String driver(); String url(); String user(); String pwd();&#125;import java.lang.reflect.Method;import java.sql.Connection;import java.sql.DriverManager;/* * 专门负责获取数据库的连接工具类 */public class JDBCUtils &#123; @MyDriver(driver="com.mysql.jdbc.Driver", url="jdbc:mysql://localhost:3306/estore",user="root",pwd="abc") public static Connection getConnection() throws Exception&#123; //获取当前类的class文件 Class clazz = JDBCUtils.class; Method method = clazz.getMethod("getConnection", null); //获取当前方法上的注解信息 if( method.isAnnotationPresent(MyDriver.class) )&#123; MyDriver an = method.getAnnotation(MyDriver.class); String driver = an.driver(); String url = an.url(); String user = an.user(); String pwd = an.pwd(); //获取数据库的连接，加载驱动，获取连接 Class.forName(driver); //获取连接 Connection conn = DriverManager.getConnection(url, user, pwd); return conn; &#125; return null; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; Connection conn = JDBCUtils.getConnection(); System.out.println(conn); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven删除本地仓库lastUpdated文件]]></title>
    <url>%2F2018%2F08%2F13%2Fmaven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[12windows:for /r %i in (*.lastUpdated) do del %i 12linux:find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 查询数据1select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
