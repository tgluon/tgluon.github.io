<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 向表中导入数据1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
