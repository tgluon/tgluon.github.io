<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sparksql实战案例]]></title>
    <url>%2F2019%2F04%2F10%2Fspark%20sql%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[累计统计准备数据access.csv12345678910111213141516171819A,2015-01,5A,2015-01,15A,2015-01,5A,2015-01,8A,2015-02,4A,2015-02,6A,2015-03,16A,2015-03,22A,2015-04,10A,2015-04,50B,2015-01,5B,2015-01,25B,2015-02,10B,2015-02,5B,2015-03,23B,2015-03,10B,2015-03,1B,2015-04,10B,2015-04,50 准备环境123456789101112131415161718192021object AccumulatorCount &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[*]") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ // 读取数据 val usersDF = spark.read.format("csv") .option("sep", ",") .option("inferSchema", "true") .option("header", "false") .load("src/main/resources/access.csv") .toDF("name", "mounth", "amount") &#125;&#125; 具体实现逻辑DataFrame API 方式方式一:123// rowsBetween(Long.MinValue, 0):窗口的大小是按照排序从最小值到当前行val accuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(Long.MinValue, 0)usersDF.withColumn("acc_amount", sum(usersDF("amount")).over(accuCntSpec)).show() 方式二123456usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(accuCntSpec).as("acc_amount")).show() sql方式思路：根据DF算子意思，找到SqlBase.g4文件，看看是否有该类sql支持。在SqlBase.g4文件中刚好找到如下内容123456789101112windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ;frameBound : UNBOUNDED boundType=(PRECEDING | FOLLOWING) | boundType=CURRENT ROW | expression boundType=(PRECEDING | FOLLOWING) ; 在spark源码sql模块core项目org.apache.spark.sql.execution包中找到SQLWindowFunctionSuite类找到如下测试方法1234567891011121314151617181920212223242526272829303132333435test("window function: multiple window expressions in a single expression") &#123; val nums = sparkContext.parallelize(1 to 10).map(x =&gt; (x, x % 2)).toDF("x", "y") nums.createOrReplaceTempView("nums") val expected = Row(1, 1, 1, 55, 1, 57) :: Row(0, 2, 3, 55, 2, 60) :: Row(1, 3, 6, 55, 4, 65) :: Row(0, 4, 10, 55, 6, 71) :: Row(1, 5, 15, 55, 9, 79) :: Row(0, 6, 21, 55, 12, 88) :: Row(1, 7, 28, 55, 16, 99) :: Row(0, 8, 36, 55, 20, 111) :: Row(1, 9, 45, 55, 25, 125) :: Row(0, 10, 55, 55, 30, 140) :: Nil val actual = sql( """ |SELECT | y, | x, | sum(x) OVER w1 AS running_sum, | sum(x) OVER w2 AS total_sum, | sum(x) OVER w3 AS running_sum_per_y, | ((sum(x) OVER w1) + (sum(x) OVER w2) + (sum(x) OVER w3)) as combined2 |FROM nums |WINDOW w1 AS (ORDER BY x ROWS BETWEEN UnBOUNDED PRECEDiNG AND CuRRENT RoW), | w2 AS (ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOuNDED FoLLOWING), | w3 AS (PARTITION BY y ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) """.stripMargin) checkAnswer(actual, expected) spark.catalog.dropTempView("nums") &#125; 下面就可以开心的照着案例写sql去了，真嗨皮！！！！12345678910usersDF.createOrReplaceTempView("access")spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between unbounded preceding and current row ) as acc_amount |from access | """.stripMargin).show() 累加N天之前,假设N=3DataFrame API方式123456val preThreeAccuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(-3, 0)usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(preThreeAccuCntSpec).as("acc_amount")).show() sql方式123456789spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between 3 preceding and current row) as acc_amount |from access | """.stripMargin).show() 累加前3天，后3天API方式123456val preThreeFiveAccuCntSpec = Window.partitionBy("name").orderBy("mounth").rowsBetween(3, 3)usersDF.select( $"name", $"mounth", $"amount", sum($"amount").over(preThreeFiveAccuCntSpec).as("acc_amount")) sql方式123456789spark.sql( """ |select name, | mounth, | amount, | sum(amount) over (partition by name order by mounth asc rows between 3 preceding and 3 following) as acc_amount |from access | """.stripMargin).show() 基本窗口函数案例准备环境12345678910111213141516171819202122232425object WindowFunctionTest extends BaseSparkSession &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[*]") .appName("WindowFunctionTest") .config("spark.some.config.option", "some-value") .getOrCreate() import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ import spark.implicits._ val df = List( ("位置1", "2018-01-01", 50), ("位置1", "2018-01-02", 45), ("位置1", "2018-01-03", 55), ("位置2", "2018-01-01", 25), ("位置2", "2018-01-02", 29), ("位置2", "2018-01-03", 27) ).toDF("site", "date", "user_cnt") &#125;&#125; 平均移动值DataFrame API方式实现123// 窗口定义从 -1(前一行)到 1(后一行) ，每一个滑动的窗口总用有3行 val movinAvgSpec = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("MovingAvg", avg(df("user_cnt")).over(movinAvgSpec)).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg |from site_info """.stripMargin).show() 前一行数据DataFrame API方式实现12val lagwSpec = Window.partitionBy("site").orderBy("date")df.withColumn("prevUserCnt", lag(df("user_cnt"), 1).over(lagwSpec)).show() sql方式实现123456789df.createOrReplaceTempView("site_info")spark.sql( """ |select site, | date, | user_cnt, | lag(user_cnt,1) over(partition by site order by date asc ) as prevUserCnt |from site_info """.stripMargin).show() 排名DataFrame API方式实现12val rankwSpec = Window.partitionBy("site").orderBy("date") df.withColumn("rank", rank().over(rankwSpec)) sql方式12345678spark.sql( """ |select site, | date, | user_cnt, | rank() over(partition by site order by date asc ) as prevUserCnt |from site_info """.stripMargin).show() 分组topn和分组取最小123456789101112131415161718192021222324252627282930313233343536373839import org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;Row, SparkSession&#125;object GroupBy &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() val rows = spark.sparkContext.parallelize( List( ("shop2", "2018-02-22", 1), ("shop2", "2018-02-27", 1), ("shop2", "2018-03-13", 1), ("shop2", "2018-03-20", 5), ("shop1", "2018-03-27", 1), ("shop1", "2018-04-03", 1), ("shop1", "2018-04-10", 1), ("shop1", "2018-04-17", 1), ("shop2", "2018-04-28", 1), ("shop2", "2018-04-05", 10), ("shop2", "2018-04-09", 1))) val rowRDD = rows.map(t =&gt; Row(t._1, t._2, t._3)) val schema = StructType( Array( StructField("shop", StringType), StructField("ycd_date", StringType), StructField("ycd_num", IntegerType))) val df = spark.createDataFrame(rowRDD, schema) df.createOrReplaceTempView("ycd_order") // 分组topN val topN = spark.sql("select * from (SELECT o.shop,o.ycd_date, row_number() over (PARTITION BY o.shop ORDER BY o.ycd_date DESC) rank FROM ycd_order as o) o1 where rank &lt; 2") // 根据某一个字段分组,取某一个字段的最小值 val groupMin = spark.sql("select o.shop,min(o.ycd_num) as min_num from ycd_order as o group by o.shop order by min_num ") topN.show() spark.stop() &#125;&#125; 优雅方式定义scheme1234567891011def getScheme(): StructType = &#123; val schemaString = "store_id:String,order_date:String,sale_amount: Int" val fields = schemaString.split(",") .map(fieldName =&gt; StructField(fieldName.split(":")(0).trim, fieldName.split(":")(1).trim match &#123; case "String" =&gt; StringType case "Int" =&gt; IntegerType &#125;, true)) StructType(fields) &#125; 保存小数点后n位10表示总的位数，2表示保留几位小数，10要&gt;=实际的位数，否则为NULL1spark.sql("select cast(sale_amount as decimal(10, 2))from ycd").show()]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamicVariable详解]]></title>
    <url>%2F2019%2F02%2F13%2FDynamicVariable%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[内容来源于：https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable 说明12protected val _response = new DynamicVariable[HttpServletResponse](null)protected val _request = new DynamicVariable[HttpServletRequest](null) DynamicVariable是贷款和动态范围模式的实现。 DynamicVariable的使用情况与Java中的ThreadLocal非常相似(事实上，DynamicVariable在后台使用InheritableThreadLocal) – 当需要在一个封闭的作用域内进行计算时，每个线程都有自己的副本的变量值：123dynamicVariable.withValue(value)&#123; valueInContext =&gt; // value used in the context&#125; 由于DynamicVariable使用可继承的ThreadLocal，变量的值被传递给上下文中生成的线程：12345dynamicVariable.withValue(value)&#123; valueInContext =&gt; spawn&#123; // value is passed to the spawned thread &#125;&#125; DynamicVariable(和ThreadLocal)在Scalatra中使用的原因与在许多其他框架(Lift，Spring，Struts等)中使用的相同 – 它是一种非侵入性的方式来存储和传递上下文(线程)特定的信息。 使HttpServletResponse和HttpServletRequest动态变量(并因此，绑定到处理请求的特定线程)只是最简单的方法来获取它们在代码中的任何地方(不通过方法参数或任何其他显式)。 案例1234567val dyn = new DynamicVariable[String]("withoutValue")def print=println(dyn.value)printdyn.withValue("withValue") &#123; print&#125;print 结果withoutValuewithValuewithoutValue]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java concurrent包类详解]]></title>
    <url>%2F2019%2F02%2F13%2Fjava%20concurrent%E5%8C%85%E7%B1%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Semaphore原文：https://zhuanlan.zhihu.com/p/27314456Semaphore(信号量)是java.util.concurrent下的一个工具类.用来控制可同时访问特定资源的线程数.内部是通过维护父类(AQS)的 int state值实现.Semaphore中有一个”许可”的概念: 访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。访问资源后，使用release()释放许可。 这个许可在构造时传入,赋给state值,它等同于state. Semaphore应用场景系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。使用案例：spark LiveListenerBus类1234567891011121314151617181920212223242526272829303132private val eventLock = new Semaphore(0)private val listenerThread = new Thread(name) &#123; setDaemon(true) override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) &#123; LiveListenerBus.withinListenerThread.withValue(true) &#123; while (true) &#123; eventLock.acquire() self.synchronized &#123; processingEvent = true &#125; try &#123; // 消费消息 val event = eventQueue.poll if (event == null) &#123; // Get out of the while loop and shutdown the daemon thread if (!stopped.get) &#123; throw new IllegalStateException("Polling `null` from eventQueue means" + " the listener bus has been stopped. So `stopped` must be true") &#125; return &#125; postToAll(event) &#125; finally &#123; self.synchronized &#123; processingEvent = false &#125; &#125; &#125; &#125; &#125;&#125; Semaphore属于一种较常见的限流手段，Google Guava封装了一层。123456789101112131415161718//JDK API：流速控制在每秒执行100个任务final Semaphore semaphore = new Semaphore(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; semaphore.acquire(); // 也许需要等待 executor.execute(task); semaphore.release(); &#125;&#125;//Google Guava API：流速控制在每秒执行100个任务final RateLimiter rateLimiter = RateLimiter.create(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; rateLimiter.acquire(); // 也许需要等待 executor.execute(task); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala枚举]]></title>
    <url>%2F2019%2F01%2F09%2Fscala%E6%9E%9A%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[定义枚举对象1234567/** 定义一个枚举类 */private[deploy] object SparkSubmitAction extends Enumeration &#123; // 声明枚举对外暴露的变量类型 type SparkSubmitAction = Value // 枚举的定义 val SUBMIT, KILL, REQUEST_STATUS = Value&#125; 使用枚举12345678910111213141516/** 定义一个枚举类 */object SparkSubmitAction extends Enumeration &#123; // 声明枚举对外暴露的变量类型 type SparkSubmitAction = Value // 枚举的定义 val SUBMIT, KILL, REQUEST_STATUS = Value def getAction(action: SparkSubmitAction)&#123; action match &#123; case SUBMIT =&gt; println ("action is " + action) case KILL =&gt; println ("action is " + action) case REQUEST_STATUS =&gt; println ("action is " + action) case _ =&gt; println ("Unknown type") &#125;&#125;&#125; 测试用例123456789object EnumerationTest &#123; def main(args: Array[String]): Unit = &#123; val action = SparkSubmitAction.apply(1) SparkSubmitAction.getAction(action) val action1 = SparkSubmitAction.withName("quit") SparkSubmitAction.getAction(action1) &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala学习笔记]]></title>
    <url>%2F2019%2F01%2F09%2Fscala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[高级函数高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。 匿名函数1(x: Int) =&gt; x * 3 带函数参数的函数Scala集合类（collections）的高阶函数map。1234567891011121314151617181920def map[B, That](f: A =&gt; B)(implicit bf: CanBuildFrom[Repr, B, That]): That = &#123; def builder = &#123; val b = bf(repr) b.sizeHint(this) b &#125; val b = builder for (x &lt;- this) b += f(x) b.result &#125; val seq = Seq(100, 200, 300) def doubleSalary(x: Int): Int = &#123; x * 2 &#125; seq.map(doubleSalary).foreach(x =&gt; println(x)) seq.map(x =&gt; x * 2) seq.map(_ * 2) 闭包123def sum(x: Int) = (y: Int) =&gt; x + yval first = sum(1)val second = first(4) 嵌套方法在Scala中可以嵌套定义方法。12345678910def factorial(x: Int): Int = &#123; def fact(x: Int, accumulator: Int): Int = &#123; if (x &lt;= 1) accumulator else fact(x - 1, x * accumulator) &#125; fact(x, 1) &#125; println("Factorial of 2: " + factorial(2)) println("Factorial of 3: " + factorial(3)) 柯里化柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。接受两个参数的过程：1def mul(x:Int , y: Int )= x * y 接受一个参数的过程12def mul(x: Int) = (y: Int) =&gt; x + ymul(1)(2) scala支持简写成如下的柯里化：123def mul(x: Int)(y: Int) = x + ymul(1)(2) 在Scala集合中定义的特质TraversableOnce[+A]。Traversable： 能横过的；能越过的；可否定的。123456789101112131415def foldLeft[B](z: B)(op: (B, A) =&gt; B): B = &#123; var result = z this foreach (x =&gt; result = op(result, x)) result &#125;``` foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：从初值0开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。 ```scalaval numbers = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)val res = numbers.foldLeft(0)((m, n) =&gt; m + n)val res2 = numbers.foldLeft(0)(_ + _)println(res) // 55pringln(res2) 多参数列表有更复杂的调用语法，因此应该谨慎使用。建议的使用场景包括: 单一的函数参数。在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样： 1numbers.foldLeft(0, &#123;(m: Int, n: Int) =&gt; m + n&#125;) 隐式（IMPLICIT）参数。 1def execute(arg: Int)(implicit ec: ExecutionContext) = ??? 模式匹配match对应java里的Switch,不过它写在选择器表达式之后。 选择器 match {备选项}取代了：switch(选择器){备选项} match与switch的比较匹配表达式可以被看做java风格switch的泛化。match的不同：1、match是scala表达式，也就是说，它始终以值作为结果;2、 scala的备选项表达式永远不会”掉到”下一个case;3、 如果没有模式匹配，MatchErro异常会被抛出。 提取器对象提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。123456789101112131415161718case class Person(name: String, age: Int) def main(args: Array[String]): Unit = &#123; // 调用工厂构造方法，构造出对象实例 val person = Person("Spark", 6) // 这种写法居然可以正常编译 val Person(name, age) = person /* * 使用了提取器， * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类 */ println(name + " : " + age) //正常输出: Spark 6 person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125; 自定义unapply方法主要用于模式匹配中。1234567891011121314151617181920212223class Person(val name: String, val salary: Int)object Person &#123; def apply(name: String, salary: Int):Person = &#123; new Person(name, salary) &#125; def unapply(money: Person): Option[(String, Int)] = &#123; if(money == null) &#123; None &#125; else &#123; Some(money.name, money.salary) &#125; &#125; def main(args: Array[String]): Unit = &#123; val person = Person("spark", 800); person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125;&#125; for 表达式Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。例子1234567891011case class User(name: String, age: Int)val userBase = List(User("Travis", 28), User("Kelly", 33), User("Jennifer", 44), User("Dennis", 23))val twentySomethings = for (user &lt;- userBase if (user.age &gt;=20 &amp;&amp; user.age &lt; 30)) yield user.name twentySomethings.foreach(name =&gt; println(name)) 这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user =20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。 泛型泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。 定义一个泛型类泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。12345678910class Stack[A] &#123; private var elements: List[A] = Nil def push(x: A) &#123; elements = x :: elements &#125; def peek: A = elements.head def pop(): A = &#123; val currentTop = peek elements = elements.tail currentTop &#125;&#125; 上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。 使用要使用一个泛型类，将一个具体类型放到方括号中来代替 A。12345val stack = new Stack[Int]stack.push(1)stack.push(2)println(stack.pop) // prints 2println(stack.pop) // prints 1 型变型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。123class Foo[+A] // 一个协变类class Bar[-A] // 一个逆变类class Baz[A] // 一个不变类 协变使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。 考虑以下简单的类结构：12345abstract class Animal &#123; def name: String&#125;case class Cat(name: String) extends Animalcase class Dog(name: String) extends Animal 类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。 在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。1234567891011121314151617object CovarianceTest extends App &#123; def printAnimalNames(animals: List[Animal]): Unit = &#123; animals.foreach &#123; animal =&gt; println(animal.name) &#125; &#125; val cats: List[Cat] = List(Cat("Whiskers"), Cat("Tom")) val dogs: List[Dog] = List(Dog("Fido"), Dog("Rex")) printAnimalNames(cats) // Whiskers // Tom printAnimalNames(dogs) // Fido // Rex&#125; 逆变通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。 考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：123abstract class Printer[-A] &#123; def print(value: A): Unit&#125; 这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：123456789class AnimalPrinter extends Printer[Animal] &#123; def print(animal: Animal): Unit = println("The animal's name is: " + animal.name)&#125;class CatPrinter extends Printer[Cat] &#123; def print(cat: Cat): Unit = println("The cat's name is: " + cat.name)&#125; 如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。12345678910111213object ContravarianceTest extends App &#123; val myCat: Cat = Cat("Boots") def printMyCat(printer: Printer[Cat]): Unit = &#123; printer.print(myCat) &#125; val catPrinter: Printer[Cat] = new CatPrinter val animalPrinter: Printer[Animal] = new AnimalPrinter printMyCat(catPrinter) printMyCat(animalPrinter)&#125; 这个程序的输出如下： The cat’s name is: BootsThe animal’s name is: Boots 不变默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。1234567class Container[A](value: A) &#123; private var _value: A = value def getValue: A = _value def setValue(value: A): Unit = &#123; _value = value &#125;&#125; 可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：1234val catContainer: Container[Cat] = new Container(Cat("Felix"))val animalContainer: Container[Animal] = catContaineranimalContainer.setValue(Dog("Spot"))val cat: Cat = catContainer.getValue 糟糕，我们最终会将一只狗作为值分配给一只猫幸运的是，编译器在此之前就会阻止我们。 上界在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。1234567891011121314151617181920212223242526abstract class Animal &#123; def name: String&#125;abstract class Pet extends Animal &#123;&#125;class Cat extends Pet &#123; override def name: String = "Cat"&#125;class Dog extends Pet &#123; override def name: String = "Dog"&#125;class Lion extends Animal &#123; override def name: String = "Lion"&#125;class PetContainer[P &lt;: Pet](p: P) &#123; def pet: P = p&#125;val dogContainer = new PetContainer[Dog](new Dog)val catContainer = new PetContainer[Cat](new Cat)// this would not compileval lionContainer = new PetContainer[Lion](new Lion) 类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：1type arguments [Lion] do not conform to class PetContainer's type parameter bounds [P &lt;: Pet] 这是因为Lion并不是Pet的子类。 下界类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。 下面看一个适合用类型下界的例子：下面看一个适合用类型下界的例子：12345678910111213trait Node[+B] &#123; def prepend(elem: B): Node[B]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this)&#125; 该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。 但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。 要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。12345678910111213trait Node[+B] &#123; def prepend[U &gt;: B](elem: U): Node[U]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this)&#125; 现在我们像下面这么做：123456trait Birdcase class AfricanSwallow() extends Birdcase class EuropeanSwallow() extends Birdval africanSwallowList= ListNode[AfricanSwallow](AfricanSwallow(), Nil())val birdList: Node[Bird] = africanSwallowListbirdList.prepend(new EuropeanSwallow) 可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。 抽象类型特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345trait Buffer &#123; type T val element: T&#125;&#125; 这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345abstract class SeqBuffer extends Buffer &#123; type U type T &lt;: Seq[U] def length = element.length&#125; 注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。 含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。12345678910111213abstract class IntSeqBuffer extends SeqBuffer &#123; type U = Int&#125;def newIntSeqBuf(elem1: Int, elem2: Int): IntSeqBuffer = new IntSeqBuffer &#123; type T = List[U] val element = List(elem1, elem2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。 把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：123456789101112131415abstract class Buffer[+T] &#123; val element: T&#125;abstract class SeqBuffer[U, +T &lt;: Seq[U]] extends Buffer[T] &#123; def length = element.length&#125;def newIntSeqBuf(e1: Int, e2: Int): SeqBuffer[Int, Seq[Int]] = new SeqBuffer[Int, List[Int]] &#123; val element = List(e1, e2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。 复合类型需求:有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。 案例：假设我们有两个特质 Cloneable 和 Resetable：12345678trait Cloneable extends java.lang.Cloneable &#123; override def clone(): Cloneable = &#123; super.clone().asInstanceOf[Cloneable] &#125;&#125;trait Resetable &#123; def reset: Unit&#125; 现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：12345def cloneAndReset(obj: ?): Cloneable = &#123; val cloned = obj.clone() obj.reset cloned&#125; 这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。 以下是更新后的方法：123def cloneAndReset(obj: Cloneable with Resetable): Cloneable = &#123; //...&#125; 复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement } 关于使用细化的例子参考 通过混入（mixin）来组合类。 自类型自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。 自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。 要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。123456789101112131415trait User &#123; def username: String&#125;trait Tweeter &#123; this: User =&gt; // 重新赋予 this 的类型 def tweet(tweetText: String) = println(s"$username: $tweetText")&#125;class VerifiedTweeter(val username_ : String) extends Tweeter with User &#123; // 我们混入特质 User 因为 Tweeter 需要 def username = s"real $username_"&#125;val realBeyoncé = new VerifiedTweeter("Beyoncé")realBeyoncé.tweet("Just spilled my glass of lemonade") // 打印出 "real Beyoncé: Just spilled my glass of lemonade" 因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。自类型别名12345678910object Demo &#123; self =&gt; def sum(num1: Int, num2: Int): Int = &#123; num1 + num2 &#125; def main(args: Array[String]): Unit = &#123; println(self.sum(1, 2)) &#125;&#125; 隐式转换定义：指的是那种以implicit关键字声明的带有单个参数的函数。通过隐式转换，程序员可以在编写Scala程序时故意漏掉一些信息，让编译器去尝试在编译期间自动推导出这些信息来，这种特性可以极大的减少代码量，忽略那些冗长，过于细节的代码。1.将方法或变量标记为implicit2.将方法的参数列表标记为implicit3.将类标记为implicit Scala支持两种形式的隐式转换：隐式值：用于给方法提供参数隐式视图：用于类型间转换或使针对某类型的方法能调用成功 案例给File类增加read方法123class RichFile(val f: File) &#123; def read() = Source.fromFile(f).mkString&#125; 门面类12345import java.io.Fileobject RichFilePredef &#123; implicit def fileToRichFile(f: File) = new RichFile(f)&#125; 使用 1234567891011import java.io.Fileimport scala.io.Sourceimport com.tm.scala.implic.RichFilePredef._object RichFile &#123; def main(args: Array[String]) &#123; val f = new File("/home/hadoop/sample_movielens_data.txt") // 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object val contents = f.read() println(contents) &#125;&#125; 排序中的使用案例Ordered方式1class Girl(val name: String, var faceValue: Int, var age: Int) 定义比较规则123456789101112/** * 视图定界 * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。 * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。 * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html */class OrderedChooser[T &lt;% Ordered[T]] &#123; def choose(first: T, second: T): T = &#123; if (first &gt; second) first else second &#125;&#125; 门面类1234567891011121314151617181920212223object OrderedPredef &#123; // 方式一 implicit def gilrToOrdered(girl: Girl):Ordered[Girl] = new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125; // 方式二 implicit val gilrToOrdered = (girl: Girl) =&gt; new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125;&#125; 测试类12345678910object TestOrdered &#123; def main(args: Array[String]): Unit = &#123; val girl1 = new Girl("spark", 100,50) val girl2 = new Girl("mxnet", 90,30) import OrderedPredef._ val chooser = new OrderingChoose[Girl] val g = chooser.choose(girl1, girl2) println(g.faceValue) &#125;&#125; Ordering方式定义比较规则 12345678910/** * 文本定界 */class OrderingChoose[T: Ordering] &#123; def choose(first: T, second: T): T = &#123; val ord = implicitly[Ordering[T]] if (ord.gt(first, second)) first else second &#125;&#125; 门面类123456789101112131415161718192021222324252627282930313233343536object OrderingPredef &#123; // 方式一 implicit val gilrToOrdering = new Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式二 implicit object GilrToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式三 /** * 参考：Ordering中的下面方法 * trait IntOrdering extends Ordering[Int] &#123; * def compare(x: Int, y: Int) = * if (x &lt; y) -1 * else if (x == y) 0 * else 1 * &#125; * implicit object Int extends IntOrdering */ trait GirlToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; implicit object Girl extends GirlToOrdering&#125; 测试类12345678910object TestOrdering &#123; def main(args: Array[String]): Unit = &#123; val g1 = new Girl("zhangsan", 50,50) val g2 = new Girl("lisi", 500,50) import OrderingPredef._ val choose = new OrderingChoose[Girl] val g = choose.choose(g1, g2) println(g.name) &#125;&#125; scala导入类并取别名导入Map,并取别名1import java.util.&#123;Map =&gt; JMap&#125; scala Try的使用spark Utils中的使用案例：1234567def classIsLoadable(clazz: String): Boolean = &#123; // scalastyle:off classforname Try &#123; Class.forName(clazz, false, getContextOrSparkClassLoader) &#125;.isSuccess // scalastyle:on classforname &#125; quasiquotes(q字符串)用于代码生成官方文档： https://docs.scala-lang.org/overviews/quasiquotes/intro.html参考文档：https://www.cnblogs.com/shishanyuan/p/8455786.html作用： Quasiquotes允许在Scala语言中对抽象语法树（AST）进行编程式构建，然后在运行时将其提供给Scala编译器以生成字节码。以q开头的字符串是quasiquotes，虽然它们看起来像字符串，但它们在编译时由Scala编译器解析，并代表其代码的AST。 val tree = q"i am { a quasiquote }" tree: universe.Tree = i.am(a.quasiquote) 符号详解== 和===的区别参考：http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie == 返回一个布尔值=== 返回一列（包含两列元素比较的结果） call-by-value and call-by-name传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部 半生对象object中的构造器在第一次调用执行一次，以后调用的话不会多次执行。object会有自己的构造方法，默认是没有参数的构造方法。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala特殊符号使用]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[_*符号作用：它告诉编译器将序列类型的单个参数视为可变参数序列。举例：12345public ProcessBuilder(String... command) &#123; this.command = new ArrayList&lt;&gt;(command.length); for (String arg : command) this.command.add(arg); &#125; 12val commandSeq = Seq(command.mainClass) ++ command.argumentsval builder = new ProcessBuilder(commandSeq: _*) ::该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。1234567891011121314151617181920212223242526272829303132def main(args: Array[String]) &#123; args.toList match &#123; case workerUrl :: userJar :: mainClass :: extraArgs =&gt; val conf = new SparkConf() val rpcEnv = RpcEnv.create("Driver", Utils.localHostName(), 0, conf, new SecurityManager(conf)) rpcEnv.setupEndpoint("workerWatcher", new WorkerWatcher(rpcEnv, workerUrl)) val currentLoader = Thread.currentThread.getContextClassLoader val userJarUrl = new File(userJar).toURI().toURL() val loader = if (sys.props.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) &#123; new ChildFirstURLClassLoader(Array(userJarUrl), currentLoader) &#125; else &#123; new MutableURLClassLoader(Array(userJarUrl), currentLoader) &#125; Thread.currentThread.setContextClassLoader(loader) // Delegate to supplied main class val clazz = Utils.classForName(mainClass) val mainMethod = clazz.getMethod("main", classOf[Array[String]]) mainMethod.invoke(null, extraArgs.toArray[String]) rpcEnv.shutdown() case _ =&gt; // scalastyle:off println System.err.println("Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]") // scalastyle:on println System.exit(-1) &#125; &#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala通过反射获取当前类类名]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[先编写HelloWord.scala文件123456object HelloWorld &#123; def main(args: Array[String]) &#123; // 获取当前类类名 println(this.getClass.getName.stripSuffix("$")) &#125;&#125; 命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class spark源码中的案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.spark.internalimport org.apache.log4j.&#123;Level, LogManager, PropertyConfigurator&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;import org.slf4j.impl.StaticLoggerBinderimport org.apache.spark.util.Utils/** * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows * logging messages at different levels using methods that only evaluate parameters lazily if the * log level is enabled. */private[spark] trait Logging &#123; // Make the log field transient so that objects with Logging can // be serialized and used on another machine @transient private var log_ : Logger = null // Method to get the logger name for this object protected def logName = &#123; // Ignore trailing $'s in the class names for Scala objects this.getClass.getName.stripSuffix("$") &#125; // Method to get or create the logger for this object protected def log: Logger = &#123; if (log_ == null) &#123; initializeLogIfNecessary(false) log_ = LoggerFactory.getLogger(logName) &#125; log_ &#125; // Log methods that take only a String protected def logInfo(msg: =&gt; String) &#123; if (log.isInfoEnabled) log.info(msg) &#125; protected def logDebug(msg: =&gt; String) &#123; if (log.isDebugEnabled) log.debug(msg) &#125; protected def logTrace(msg: =&gt; String) &#123; if (log.isTraceEnabled) log.trace(msg) &#125; protected def logWarning(msg: =&gt; String) &#123; if (log.isWarnEnabled) log.warn(msg) &#125; protected def logError(msg: =&gt; String) &#123; if (log.isErrorEnabled) log.error(msg) &#125; // Log methods that take Throwables (Exceptions/Errors) too protected def logInfo(msg: =&gt; String, throwable: Throwable) &#123; if (log.isInfoEnabled) log.info(msg, throwable) &#125; protected def logDebug(msg: =&gt; String, throwable: Throwable) &#123; if (log.isDebugEnabled) log.debug(msg, throwable) &#125; protected def logTrace(msg: =&gt; String, throwable: Throwable) &#123; if (log.isTraceEnabled) log.trace(msg, throwable) &#125; protected def logWarning(msg: =&gt; String, throwable: Throwable) &#123; if (log.isWarnEnabled) log.warn(msg, throwable) &#125; protected def logError(msg: =&gt; String, throwable: Throwable) &#123; if (log.isErrorEnabled) log.error(msg, throwable) &#125; protected def isTraceEnabled(): Boolean = &#123; log.isTraceEnabled &#125; protected def initializeLogIfNecessary(isInterpreter: Boolean): Unit = &#123; if (!Logging.initialized) &#123; Logging.initLock.synchronized &#123; if (!Logging.initialized) &#123; initializeLogging(isInterpreter) &#125; &#125; &#125; &#125; private def initializeLogging(isInterpreter: Boolean): Unit = &#123; // Don't use a logger in here, as this is itself occurring during initialization of a logger // If Log4j 1.2 is being used, but is not initialized, load a default properties file val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr // This distinguishes the log4j 1.2 binding, currently // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently // org.apache.logging.slf4j.Log4jLoggerFactory val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass) if (usingLog4j12) &#123; val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements // scalastyle:off println if (!log4j12Initialized) &#123; val defaultLogProps = "org/apache/spark/log4j-defaults.properties" Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match &#123; case Some(url) =&gt; PropertyConfigurator.configure(url) System.err.println(s"Using Spark's default log4j profile: $defaultLogProps") case None =&gt; System.err.println(s"Spark was unable to load $defaultLogProps") &#125; &#125; if (isInterpreter) &#123; // Use the repl's main class to define the default log level when running the shell, // overriding the root logger's config if they're different. val rootLogger = LogManager.getRootLogger() val replLogger = LogManager.getLogger(logName) val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN) if (replLevel != rootLogger.getEffectiveLevel()) &#123; System.err.printf("Setting default log level to \"%s\".\n", replLevel) System.err.println("To adjust logging level use sc.setLogLevel(newLevel). " + "For SparkR, use setLogLevel(newLevel).") rootLogger.setLevel(replLevel) &#125; &#125; // scalastyle:on println &#125; Logging.initialized = true // Force a call into slf4j to initialize it. Avoids this happening from multiple threads // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html log &#125;&#125;private object Logging &#123; @volatile private var initialized = false val initLock = new Object() try &#123; // We use reflection here to handle the case where users remove the // slf4j-to-jul bridge order to route their logs to JUL. val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler") bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null) val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean] if (!installed) &#123; bridgeClass.getMethod("install").invoke(null) &#125; &#125; catch &#123; case e: ClassNotFoundException =&gt; // can't log anything yet so just fail silently &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql详解]]></title>
    <url>%2F2019%2F01%2F04%2Fsparksql%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark SQL的Dataset基本操作https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解 基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。 1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是1type DataFrame = Dataset[Row] 结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。 首先要创建一个SparkSession:12345678910111213141516171819val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]").set("yarn.resourcemanager.hostname", "localhost") // executor的实例数.set("spark.executor.instances","2") .set("spark.default.parallelism","4") // sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。.set("spark.sql.shuffle.partitions","4").setJars(List("/Users/meitu/Desktop/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar"))val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() 创建dataset123456val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100), ("Warsaw", 2017, 200), ("Warsaw", 2015, 100), ("Warsaw", 2017, 200), ("Beijing", 2017, 200), ("Beijing", 2016, 200), ("Beijing", 2015, 200), ("Beijing", 2014, 200), ("Warsaw", 2014, 200), ("Boston", 2017, 50), ("Boston", 2016, 50), ("Boston", 2015, 50), ("Boston", 2014, 150))).toDF("city", "year", "amount") 使用函数的时候要导入包：1import org.apache.spark.sql.functions.&#123;col,expr&#125; select列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。使用col函数，可以直接对列进行一些逻辑操作。12sales.select("city","year","amount").show(1)sales.select(col("city"),col("amount")+1).show(1) selectExpr参数是字符串，且直接可以使用表达式。也可以使用select+expr函数来替代。12sales.selectExpr("city","year as date","amount+1").show(10)sales.select(expr("city"),expr("year as date"),expr("amount+1")).show(10) filter参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。123sales.filter(col("amount")&gt;150).show()sales.filter(row=&gt;&#123; row.getInt(2)&gt;150&#125;).show(10)sales.filter("amount &gt; 150 ").show(10) where类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。12sales.where(col("amount")&gt;150).show()sales.where("amount &gt; 150 ").show() group by主要是以count和agg聚合函数为例讲解groupby函数。12sales.groupBy("city").count().show(10)sales.groupBy(col("city")).agg(sum("amount").as("total")).show(10) union两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。1sales.union(sales).groupBy("city").count().show() joinjoin操作相对比较复杂，具体如下：123456789101112131415161718192021222324252627282930// 相同的列进行join sales.join(sales,"city").show(10) // 多列joinsales.join(sales,Seq("city","year")).show() /* 指定join类型， join 类型可以选择: `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti`. */ // 内部joinsales.join(sales,Seq("city","year"),"inner").show() /* join条件 ：可以在join方法里放入join条件，也可以使用where,这两种情况都要求字段名称不一样。*/ sales.join(sales, col("city").alias("city1") === col("city")).show() sales.join(sales).where(col("city").alias("city1") === col("city")).show() /* dataset的self join 此处使用where作为条件，需要增加配置.set("spark.sql.crossJoin.enabled","true") 也可以加第三个参数，join类型，可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` */ sales.join(sales,sales("city") === sales("city")).show() sales.join(sales).where(sales("city") === sales("city")).show()/* joinwith,可以指定第三个参数，join类型，类型可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`。 */ sales.joinWith(sales,sales("city") === sales("city"),"inner").show() 输出结果： order byorderby 全局有序，其实用的还是sort12sales.orderBy(col("year").desc,col("amount").asc).show()sales.orderBy("city","year").show() sort全局排序，直接替换掉8小结的orderby即可。 sortwithinpartition在分区内部进行排序，局部排序。12sales.sortWithinPartitions(col("year").desc,col("amount").asc).show()sales.sortWithinPartitions("city","year").show() 可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。 withColumn1234567891011/* withColumn 假如列，存在就替换，不存在新增 withColumnRenamed 对已有的列进行重命名 *///相当于给原来amount列，+1sales.withColumn("amount",col("amount")+1).show()// 对amount列+1，然后将值增加到一个新列 amount1sales.withColumn("amount1",col("amount")+1).show()// 将amount列名，修改为amount1sales.withColumnRenamed("amount","amount1").show() foreach这个跟rdd的foreach一样，元素类型是row。123sales.foreach(row=&gt;&#123; println(row.getString(0))&#125;) foreachPartition跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。12345678sales.foreachPartition(partition=&gt;&#123; //打开数据库链接等 partition.foreach(each=&gt;&#123; println(each.getString(0)) //插入数据库 &#125;) //关闭数据库链接 &#125;) distinct针对dataset的行去重，返回的是所有行都不重复的dataset。1sales.distinct().show(10) dropDuplicates这个适用于dataset有唯一的主键，然后对主键进行去重。1234val before = sales.count()val after = sales.dropDuplicates("city").count()println("before ====&gt; " +before)println("after ====&gt; "+after) drop删除一列，或者多列，这是一个变参数算子。12345sales.drop("city").show()打印出来schema信息如下：root|-- year: integer (nullable = false)|-- amount: integer (nullable = false) printSchema输出dataset的schema信息1234567891011sales.printSchema()输出结果如下：root|-- city: string (nullable = true)|-- year: integer (nullable = false)|-- amount: integer (nullable = false) explain()打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程123456sales.orderBy(col("year").desc,col("amount").asc).explain()执行计划输出如下：== Physical Plan ==*(1) Sort [year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST], true, 0+- Exchange rangepartitioning(year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST, 3)+- LocalTableScan [city#6, year#7, amount#8] Spark SQL入门到精通之第二篇Dataset的复杂操作本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：cube，rollup，pivot操作。 cube简单的理解就是维度及度量组成的数据体。1234sales.cube("city","year") .agg(sum("amount")) .sort(col("city").desc_nulls_first,col("year").desc_nulls_first) .show() 举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：123456789101112131415val city_year = sales.groupBy("city","year").agg(sum("amount"))val city = sales.groupBy("city") .agg(sum("amount") as "amount") .select(col("city"), lit(null) as "year", col("amount"))val year = sales.groupBy("year") .agg(sum("amount") as "amount") .select( lit(null) as "city",col("year"), col("amount"))val none = sales .groupBy().agg(sum("amount") as "amount") .select(lit(null) as "city", lit(null) as "year", col("amount")) city_year.union(city).union(year).union(none) .sort(desc_nulls_first("city"), desc_nulls_first("year")) .show() rollup这里也是以案例开始，代码如下：12345678910111213val expenses = spark.createDataFrame(Seq( ((2012, Month.DECEMBER, 12), 5), ((2016, Month.AUGUST, 13), 10), ((2017, Month.MAY, 27), 15)) .map &#123; case ((yy, mm, dd), a) =&gt; (LocalDate.of(yy, mm, dd), a) &#125; .map &#123; case (d, a) =&gt; (d.toString, a) &#125; .map &#123; case (d, a) =&gt; (Date.valueOf(d), a) &#125;).toDF("date", "amount")// rollup time!val res = expenses .rollup(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount") .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：123456789101112val year_month = expenses.groupBy(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount")val yearOnly = expenses.groupBy(year(col("date")) as "year") .agg(sum("amount") as "amount") .select(col("year"), lit(null) as "month", col("amount"))val none = expenses.groupBy() .agg(sum("amount") as "amount") .select(lit(null) as "year", lit(null) as "month", col("amount"))year_month.union(yearOnly).union(none) .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() pivot旋转操作https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEwPivot 算子是 spark 1.6 版本开始引入的，在 spark2.4版本中功能做了增强，还是比较强大的，做过数据清洗ETL工作的都知道，行列转换是一个常见的数据整理需求。spark 中的Pivot 可以根据枢轴点(Pivot Point) 把多行的值归并到一行数据的不同列，这个估计不太好理解，我们下面使用例子说明，看看pivot 这个算子在处理复杂数据时候的威力。12345678910111213val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Boston", 2015, 50,"Boston"), ("Boston", 2016, 150,"Boston"), ("Toronto", 2017, 50,"Toronto"))).toDF("city", "year", "amount","test")sales.groupBy("year").pivot("city",Seq("Warsaw","Boston","Toronto")) .agg(sum("amount") as "amount") .show() 思路就是首先对year分组，然后旋转city字段，只取:“Warsaw”,”Boston”,”Toronto”然后对amount进行聚合操作案例：使用Pivot 来统计天气走势。下面是西雅图的天气数据表，每行代表一天的天气最高值：12345678910 Date Temp (°F) 07-22-2018 86 07-23-2018 90 07-24-2018 91 07-25-2018 92 07-26-2018 92 07-27-2018 88 07-28-2018 85 07-29-2018 94 07-30-2018 89 如果我们想看下最近几年的天气走势，如果这样一天一行数据，是很难看出趋势来的，最直观的方式是 按照年来分行，然后每一列代表一个月的平均天气，这样一行数据，就可以看到这一年12个月的一个天气走势，下面我们使用 pivot 来构造这样一个查询结果：1234567891011121314151617181920212223SELECT * FROM ( SELECT year(date) year, month(date) month, temp FROM high_temps WHERE date between DATE '2015-01-01' and DATE '2018-08-31' ) PIVOT ( CAST ( AVG(temp) as DECIMAL(4, 1) ) FOR month in ( 1 JAN, 2 FEB, 3 MAR, 4 APR, 5 MAY, 6 JUN, 7 JUL, 8 AUG, 9 SEP, 10 OCT, 11 NOV, 12 DEC ) ) ORDER BY year DESC; 结果如下图：是不是很直观，第一行就代表 2018 年，从1月到12月的平均天气，能看出一年的天气走势。我们来看下这个 sql 是怎么玩的，首先是一个 子查询语句，我们最终关心的是 年份，月份，和最高天气值，所以先使用子查询对原始数据进行处理，从日期里面抽取出来年份和月份。 下面在子查询的结果上使用 pivot 语句，pivot 第一个参数是一个聚合语句，这个代表聚合出来一个月30天的一个平均气温，第二个参数是 FOR month，这个是指定以哪个列为枢轴列，第三个 In 子语句指定我们需要进行行列转换的具体的 枢轴点(Pivot Point)的值，上面的例子中 1到12月份都包含了，而且给了一个别名，如果只指定 1到6月份，结果就如下了：上面sql语句里面有个特别的点需要注意， 就是聚合的时候有个隐含的维度字段，就是 年份，按理来讲，我们没有写 group-by year， 为啥结果表里面不同年份区分在了不同的行，原因是，FORM 子查询出来的每行有 3个列， year，month，tmp，如果一个列既不出现在进行聚合计算的列中（temp 是聚合计算的列）， 也不作为枢轴列 ， 就会作为聚合的时候一个隐含的维度。我们的例子中算平均值聚合操作的维度是 (year, month)，一个是隐含维度，一个是 枢轴列维度， 这一点一定要注意，如果不需要按照 year 来区分，FORM 查询的时候就不要加上这个列。指定多个聚合语句上文中只有一个聚合语句，就是计算平均天气，其实是可以加多个聚合语句的，比如我们需要看到 7，8，9 月份每个月的最大气温和平均气温，就可以用以下SQL语句。123456789101112131415161718192021SELECT * FROM ( SELECT year(date) year, month(date) month, temp FROM high_temps WHERE date between DATE '2015-01-01' and DATE '2018-08-31' ) PIVOT ( CAST ( AVG(temp) as DECIMAL(4, 1) ) avg, max(temp) max FOR month in (6 JUN, 7 JUL, 8 AUG, 9 SEP) ) ORDER BY year DESC; 上文中指定了两个聚合语句，查询后， 枢轴点(Pivot Point) 和 聚合语句 的笛卡尔积作为结果的不同列，也就是 _， 看下图：聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处现在假如我们有西雅图每天的最低温数据，我们需要把最高温和最低温放在同一张表里面看对比着看.123456789Date Temp (°F)… …08-01-2018 5908-02-2018 5808-03-2018 5908-04-2018 5808-05-2018 5908-06-2018 59… … 我们使用 UNION ALL 把两张表做一个合并：1234567SELECT date, temp, 'H' as flag FROM high_temps UNION ALL SELECT date, temp, 'L' as flag FROM low_temps; 现在使用 pivot 来进行处理：123456789101112SELECT * FROM (SELECT date,temp,'H' as flag FROM high_temps UNION ALL SELECT date,temp,'L' as flag FROM low_temps)WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'PIVOT(CAST(avg(temp) as DECIMAL(4,1))FOR month in (6 JUN,7 JUL,8 AUG , 9 SEP)) ORDER BY year DESC ,`H/L` ASC; 我们统计了 4年中 7，8，9 月份最低温和最高温的平均值，这里要注意的是，我们把 year 和 一个最低最高的标记（H/L）都作为隐含维度，不然算出来的就是最低最高温度在一起的平均值了。结果如下图：上面的查询中，我们把最低最高的标记（H/L）也作为了一个隐含维度，group-by 的维度就变成了 （year， H/L, month）, 但是year 和 H/L 体现在了不同行上面，month 体现在了不同的列上面，这就是 聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处。 再接再厉，我们把 H/L 作为 枢轴列（Pivot Columns） 进行查询：1234567891011121314151617SELECT * FROM (SELECT date,temp,'H' as flag FROM high_temps UNION ALL SELECT date,temp,'L' as flag FROM low_temps)WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'PIVOT(CAST(avg(temp) as DECIMAL(4,1))FOR (month,flag) in ((6,'H') JUN_hi,(6,'L') JUN_lo,(7,'H') JUl_hi,(7,'L') JUl_lo,(8,'H') AUG_hi,(8,'L') AUG_lo,(9,'H') SEP_hi,(9,'L') SEP_lo)) ORDER BY year DESC; 结果的展现方式就和上面的不同了，虽然每个单元格值都是相同的，但是把 H/L 和 month 笛卡尔积作为列，H/L 维度体现在了列上面了： 源码:Spark SQL 分区特性第一弹常见RDD分区Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。举几个浪尖在星球里分享比较多的例子，比如：Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据首先是由Seq数据集合生成一个Dataset1234567891011121314151617181920212223242526val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 110), ("Warsaw", 2017, 10), ("Warsaw", 2015, 100), ("Warsaw", 2015, 50), ("Warsaw", 2015, 80), ("Warsaw", 2015, 100), ("Warsaw", 2015, 130), ("Warsaw", 2015, 160), ("Warsaw", 2017, 200), ("Beijing", 2017, 100), ("Beijing", 2016, 150), ("Beijing", 2015, 50), ("Beijing", 2015, 30), ("Beijing", 2015, 10), ("Beijing", 2014, 200), ("Beijing", 2014, 170), ("Boston", 2017, 50), ("Boston", 2017, 70), ("Boston", 2017, 110), ("Boston", 2017, 150), ("Boston", 2017, 180), ("Boston", 2016, 30), ("Boston", 2015, 200), ("Boston", 2014, 20) )).toDF("city", "year", "amount") 将Dataset存处为partquet格式的hive表，分两种情况：用city和year字段分区1sales.write.partitionBy("city","year").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCityAndYear") 用city字段分区1sales.write.partitionBy("city").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCity") 读取数据采用的是1val res = spark.read.parquet("/user/hive/warehouse/parquettestcity") 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40Dataset的分区数是由参数：目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4Dataset的分区数是由参数：1println("partition size = "+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private def createNonBucketedReadRDD( readFile: (PartitionedFile) =&gt; Iterator[InternalRow], selectedPartitions: Seq[PartitionDirectory], fsRelation: HadoopFsRelation): RDD[InternalRow] = &#123; /* selectedPartitions 的大小代表目录数目 */ println("selectedPartitions.size : "+ selectedPartitions.size) val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes // spark.default.parallelism val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism // 计算文件总大小，单位字节数 val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum //计算平均每个并行度读取数据大小 val bytesPerCore = totalBytes / defaultParallelism // 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 // 然后，比较spark.sql.files.maxPartitionBytes 取小者 val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " + s"open cost is considered as scanning $openCostInBytes bytes.") // 这对目录遍历 val splitFiles = selectedPartitions.flatMap &#123; partition =&gt; partition.files.flatMap &#123; file =&gt; val blockLocations = getBlockLocations(file) //判断文件类型是否支持分割，以parquet为例，是支持分割的 if (fsRelation.fileFormat.isSplitable( fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123; // eg. 0 until 2不包括 2。相当于 // println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) (0L until file.getLen by maxSplitBytes).map &#123; offset =&gt; // 计算文件剩余的量 val remaining = file.getLen - offset // 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining // 位置信息 val hosts = getBlockHosts(blockLocations, offset, size) PartitionedFile( partition.values, file.getPath.toUri.toString, offset, size, hosts) &#125; &#125; else &#123; // 不可分割的话，那即是一个文件一个分区 val hosts = getBlockHosts(blockLocations, 0, file.getLen) Seq(PartitionedFile( partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &#125; &#125; &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) val partitions = new ArrayBuffer[FilePartition] val currentFiles = new ArrayBuffer[PartitionedFile] var currentSize = 0L /** Close the current partition and move to the next. */ def closePartition(): Unit = &#123; if (currentFiles.nonEmpty) &#123; val newPartition = FilePartition( partitions.size, currentFiles.toArray.toSeq) // Copy to a new Array. partitions += newPartition &#125; currentFiles.clear() currentSize = 0 &#125; // Assign files to partitions using "Next Fit Decreasing" splitFiles.foreach &#123; file =&gt; if (currentSize + file.length &gt; maxSplitBytes) &#123; closePartition() &#125; // Add the given file to the current partition. currentSize += file.length + openCostInBytes currentFiles += file &#125; closePartition() println("FileScanRDD partitions size : "+partitions.size) new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &#125; 找到一列中的中位数https://spark.apache.org/docs/latest/api/sql/index.htmldf函数： approxQuantilesql函数： percentile_approx 自定义数据源ServiceLoaderhttps://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3wServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。要配置在相关项目的固定目录下：resources/META-INF/services/接口全称。这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：SparkContext类1234567891011 private def getClusterManager(url: String): Option[ExternalClusterManager] = &#123; val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size &gt; 1) &#123; throw new SparkException( s"Multiple external cluster managers registered for the url $url: $serviceLoaders") &#125; serviceLoaders.headOption &#125;&#125; 配置是在spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。简单测试首先实现一个接口1234567package bigdata.spark.services;public interface DoSomething &#123; //可以制定实现类名加载 public String shortName(); public void doSomeThing();&#125; 然后将接口配置在resources/META-INF/services/bigdata.spark.services.DoSomething文件内容：实现该接口12345678910111213package bigdata.spark.services;public class SayHello implements DoSomething &#123; @Override public String shortName() &#123; return "SayHello"; &#125; @Override public void doSomeThing() &#123; System.out.println("hello !!!"); &#125;&#125; 测试12345678910111213package bigdata.spark.services;import java.util.ServiceLoader;public class test &#123; static ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class); public static void main(String[] args)&#123; for(DoSomething sayhello : loader)&#123; //要加载的类名称我们可以制定 if(sayhello.shortName().equalsIgnoreCase("SayHello"))&#123; sayhello.doSomeThing(); &#125; &#125; &#125;&#125; 这个主要是为讲自定义数据源作准备。 https://articles.zsxq.com/id_702s32f46zet.html首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** Given a provider name, look up the data source class definition. */ def lookupDataSource(provider: String): Class[_] = &#123; val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) // 指定路径加载，默认加载类名是DefaultSource val provider2 = s"$provider1.DefaultSource" val loader = Utils.getContextOrSparkClassLoader // ServiceLoader加载 val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader) try &#123; serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match &#123; // the provider format did not match any given registered aliases case Nil =&gt; try &#123; Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match &#123; case Success(dataSource) =&gt; // Found the data source using fully qualified path dataSource case Failure(error) =&gt; if (provider1.toLowerCase == "orc" || provider1.startsWith("org.apache.spark.sql.hive.orc")) &#123; throw new AnalysisException( "The ORC data source must be used with Hive support enabled") &#125; else if (provider1.toLowerCase == "avro" || provider1 == "com.databricks.spark.avro") &#123; throw new AnalysisException( s"Failed to find data source: $&#123;provider1.toLowerCase&#125;. Please find an Avro " + "package at http://spark.apache.org/third-party-projects.html") &#125; else &#123; throw new ClassNotFoundException( s"Failed to find data source: $provider1. Please find packages at " + "http://spark.apache.org/third-party-projects.html", error) &#125; &#125; &#125; catch &#123; case e: NoClassDefFoundError =&gt; // This one won't be caught by Scala NonFatal // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"$className was removed in Spark 2.0. " + "Please check if your library is compatible with Spark 2.0", e) &#125; else &#123; throw e &#125; &#125; case head :: Nil =&gt; // there is exactly one registered alias head.getClass case sources =&gt; // There are multiple registered aliases for the input sys.error(s"Multiple sources found for $provider1 " + s"($&#123;sources.map(_.getClass.getName).mkString(", ")&#125;), " + "please specify the fully qualified class name.") &#125; &#125; catch &#123; case e: ServiceConfigurationError if e.getCause.isInstanceOf[NoClassDefFoundError] =&gt; // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getCause.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"Detected an incompatible DataSourceRegister. " + "Please remove the incompatible library from classpath or upgrade it. " + s"Error: $&#123;e.getMessage&#125;", e) &#125; else &#123; throw e &#125; &#125; &#125; 其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。 主要思路是： 实现DefaultSource。 实现工厂类。 实现具体的数据加载类。 首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：12345678package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.sources.v2.&#123;DataSourceOptions, DataSourceV2, ReadSupport&#125;class DefaultSource extends DataSourceV2 with ReadSupport &#123; def createReader(options: DataSourceOptions) = new SimpleDataSourceReader()&#125; 然后是，要实现DataSourceReader，负责创建阅读器工厂：12345678910111213141516package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReaderFactory, DataSourceReader&#125;import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;class SimpleDataSourceReader extends DataSourceReader &#123; def readSchema() = StructType(Array(StructField("value", StringType))) def createDataReaderFactories = &#123; val factoryList = new java.util.ArrayList[DataReaderFactory[Row]] factoryList.add(new SimpleDataSourceReaderFactory()) factoryList &#125;&#125; 数据源的具体实现类：12345678910111213141516171819202122package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReader, DataReaderFactory&#125;class SimpleDataSourceReaderFactory extends DataReaderFactory[Row] with DataReader[Row] &#123; def createDataReader = new SimpleDataSourceReaderFactory() val values = Array("1", "2", "3", "4", "5") var index = 0 def next = index &lt; values.length def get = &#123; val row = Row(values(index)) index = index + 1 row &#125; def close() = Unit&#125; 使用我们默认的数据源：12345678910111213141516171819202122232425262728package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]") .set("yarn.resourcemanager.hostname", "mt-mdh.local") .set("spark.executor.instances","2") .setJars(List("/opt/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar")) val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() val simpleDf = spark.read .format("bigdata.spark.SparkSQL.DataSources") .load() simpleDf.show() spark.stop() &#125;&#125; format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。 删除多个字段123def dropColumns(columns: Seq[String]):DataFAME=&#123; columns.foldLeft(dataFame)((df,column)=&gt; df.drop(column))&#125; spark sql 窗口函数https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw最近理了下spark sql 中窗口函数的知识，打算开几篇文章讲讲，窗口函数有很多应用场景，比如说炒股的时候有个5日移动均线，或者让你对一个公司所有销售所有部门按照销售业绩进行排名等等，都是要用到窗口函数，在spark sql中，窗口函数和聚合函数的区别，之前文章中也提到过，就是聚合函数是按照你聚合的维度，每个分组中算出来一个聚合值，而窗口函数是对每一行，都根据当前窗口（5日均线就是今日往前的5天组成的窗口），都聚合出一个值。 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势2 spark sql 中窗口函数深入理解3 不是UDF，也不是UDAF，教你自定义一个窗口函数（UDWF）4 从 spark sql 源码层面理解窗口函数 什么是简单移动平均值简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。例子12345678910111213141516171819202122232425262728293031val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)) .toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ /** * 这个 window spec 中，数据根据用户(customer)来分去。 * 每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行) * ，每一个滑动的窗口总用有3行 */ val wSpce = Window.partitionBy("site").orderBy("date").rowsBetween(-1, 1) df.withColumn("movinAvg", avg("user_cnt").over(wSpce)).show() spark.stop() 结果：2~3~212345678910+----+---------+--------+------------------+|site| date|user_cnt| movinAvg|+----+---------+--------+------------------+| 站点1|201902025| 50| 70.0|| 站点1|201902026| 90| 80.0|| 站点1|201902026| 100| 95.0|| 站点2|201902027| 70| 65.0|| 站点2|201902028| 60|56.666666666666664|| 站点2|201902029| 40| 50.0|+----+---------+--------+------------------+ 窗口函数和窗口特征定义正如上述例子中，窗口函数主要包含两个部分： 指定窗口特征（wSpec）： “partitionyBY” 定义数据如何分组；在上面的例子中，是用户 site “orderBy” 定义分组中的排序 “rowsBetween” 定义窗口的大小 指定窗口函数函数： 指定窗口函数函数，你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数. 累计汇总1234567891011121314151617181920212223242526272829val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate()import spark.implicits._val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)) .toDF("site", "date", "user_cnt")import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions._/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */val wSpce = Window.partitionBy("site").orderBy("date").rowsBetween(Long.MinValue, 1)df.withColumn("cumsum", sum("user_cnt").over(wSpce)).show()spark.stop() 结果12345678910+----+---------+--------+------+|site| date|user_cnt|cumsum|+----+---------+--------+------+| 站点1|201902025| 50| 140|| 站点1|201902026| 90| 240|| 站点1|201902026| 100| 240|| 站点2|201902027| 70| 130|| 站点2|201902028| 60| 170|| 站点2|201902029| 40| 170|+----+---------+--------+------+ 前一行数据123456789101112131415161718192021222324252627val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)).toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ /** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */ val wSpce = Window.partitionBy("site").orderBy("date") df.withColumn("preUserCnt", lag(df("user_cnt"),1).over(wSpce)).show() spark.stop() 结果12345678910+----+---------+--------+----------+|site| date|user_cnt|preUserCnt|+----+---------+--------+----------+| 站点1|201902025| 50| null|| 站点1|201902026| 90| 50|| 站点1|201902026| 100| 90|| 站点2|201902027| 70| null|| 站点2|201902028| 60| 70|| 站点2|201902029| 40| 60|+----+---------+--------+----------+ 如果计算环比的时候，是不是特别有用啊？！ 在介绍几个常用的行数： first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到 lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件 排名1234567891011121314151617181920212223val spark = SparkSession .builder() .master("local") .appName("DateFrameFromJsonScala") .config("spark.some.config.option", "some-value") .getOrCreate() import spark.implicits._ val df = List( ("站点1", "201902025", 50), ("站点1", "201902026", 90), ("站点1", "201902026", 100), ("站点2", "201902027", 70), ("站点2", "201902028", 60), ("站点2", "201902029", 40)).toDF("site", "date", "user_cnt") import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ val wSpce = Window.partitionBy("site").orderBy("date") df.withColumn("rank", rank().over(wSpce)).show() 结果12345678910+----+---------+--------+----+|site| date|user_cnt|rank|+----+---------+--------+----+| 站点1|201902025| 50| 1|| 站点1|201902026| 90| 2|| 站点1|201902026| 100| 2|| 站点2|201902027| 70| 1|| 站点2|201902028| 60| 2|| 站点2|201902029| 40| 3|+----+---------+--------+----+ 这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。 自定义窗口函数https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQhttps://github.com/zheniantoushipashi/spark-udwf-session 背景在使用 spark sql 的时候，有时候默认提供的sql 函数可能满足不了需求，这时候可以自定义一些函数，可以自定义 UDF 或者UDAF。 UDF 在sql中只是简单的处理转换一些字段，类似默认的trim 函数把一个字符串类型的列的头尾空格去掉， UDAF函数不同于UDF，是在sql聚合语句中使用的函数，必须配合 GROUP BY 一同使用，类似默认提供的count，sum函数，但是还有一种自定义函数叫做 UDWF， 这种一般人就不知道了，这种叫做窗口自定义函数，不了解窗口函数的，可以参考 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势 ，或者官方的介绍 https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html 窗口函数是 SQL 中一类特别的函数。和聚合函数相似，窗口函数的输入也是多行记录。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口 这里怎么理解一个窗口呢，spark君在这里得好好的解释解释，一个窗口是怎么定义的， 窗口语句中，partition by用来指定分区的列，在同一个分区的行属于同一个窗口 order by用来指定窗口内的多行，如何排序 windowing_clause 用来指定开窗方式，在spark sql 中开窗方式有那么几种 一个分区中的所有行作为一个窗口:UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING（上下都没有边界)，这种情况下，spark sql 会把所有行作为一个输入，进行一次求值 Growing frame:UNBOUNDED PRECEDING AND ….（上无边界）, 这种就是不断的把当前行加入的窗口中，而不删除， 例子：.rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行，在数据迭代过程中，不断的把当前行加入的窗口中。 Shrinking frame：… AND UNBOUNDED FOLLOWING（下无边界）和Growing frame 相反，窗口不断的把迭代到的当前行从窗口中删除掉。 Moving frame：滑动的窗口，举例：.rowsBetween(-1, 1) 就是指窗口定义从 -1(当前行前一行)到 1(当前行后一行) ，每一个滑动的窗口总用有3行 Offset frame： 窗口中只有一条数据，就是偏移当前行一定距离的那一行，举例：lag(field, n): 就是取从当前行往前的n个行的字段值 这里就针对窗口函数就介绍这么多，如果不懂请参考相关文档，加强理解，我们在平时使用 spark sql 的过程中，会发现有很多教你自定义 UDF 和 UDAF 的教程，却没有针对UDWF的教程，这是为啥呢，这是因为 UDF 和UDAF 都作为上层API暴露给用户了，使用scala很简单就可以写一个函数出来，但是UDWF没有对上层用户暴露，只能使用 Catalyst expressions. 也就是Catalyst框架底层的表达式语句才可以定义，如果没有对源码有很深入的研究，根本就搞不出来。spark 君在工作中写了一些UDWF的函数，但是都比较复杂，不太好单独抽出来作为一个简明的例子给大家讲解，挑一个网上的例子来进行说明，这个例子 spark君亲测可用。 窗口函数的使用场景我们来举个实际例子来说明 窗口函数的使用场景，在网站的统计指标中，有一个概念叫做用户会话，什么叫做用户会话呢，我来说明一下，我们在网站服务端使用用户session来管理用户状态，过程如下 1) 服务端session是用户第一次访问应用时，服务器就会创建的对象，代表用户的一次会话过程，可以用来存放数据。服务器为每一个session都分配一个唯一的sessionid，以保证每个用户都有一个不同的session对象。 2）服务器在创建完session后，会把sessionid通过cookie返回给用户所在的浏览器，这样当用户第二次及以后向服务器发送请求的时候，就会通过cookie把sessionid传回给服务器，以便服务器能够根据sessionid找到与该用户对应的session对象。 3）session通常有失效时间的设定，比如1个小时。当失效时间到，服务器会销毁之前的session，并创建新的session返回给用户。但是只要用户在失效时间内，有发送新的请求给服务器，通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长1个小时。 也就是说如果用户在1个超过一个小时不产生用户事件，当前会话就结束了，如果后续再产生用户事件，就当做新的用户会话，我们现在就使用spark sql 来统计用户的会话数，首先我们先加一个列，作为当前列的session，然后再 distinct 这个列就得到用户的会话数了，所以关键是怎么加上这个newSession 这个列，这种场景就很适合使用窗口函数来做统计，因为判断当前是否是一个新会话的依据，需要依赖当前行的前一行的时间戳和当前行的时间戳的间隔来判断，下面的表格可以帮助你理解这个概念，例子中有3列数据，用户，event字段代表用户访问了一个页面产生了一个用户事件，time字段代表访问页面的时间戳： user event time session user1 page1 10:12 session1(new session) user1 page2 10:20 session1(same session,8 minutes from last event) user1 page1 11:13 session1(same session,53 minutes from last event) user1 page3 14:12 session1(new session,3 minutes from last event) 上面只有一个用户，如果多个用户，可以使用 partition by 来进行分区。 深入研究构造数据12345678910111213141516171819case class UserActivityData(user:String, ts:Long, session:String) val st = System.currentTimeMillis() val one_minute = 60 * 1000 val d = Array[UserActivityData]( UserActivityData("user1", st, "f237e656-1e53-4a24-9ad5-2b4576a4125d"), UserActivityData("user2", st + 5*one_minute, null), UserActivityData("user1", st + 10*one_minute, null), UserActivityData("user1", st + 15*one_minute, null), UserActivityData("user2", st + 15*one_minute, null), UserActivityData("user1", st + 140*one_minute, null), UserActivityData("user1", st + 160*one_minute, null)) "a CustomWindowFunction" should "correctly create a session " in &#123; val sqlContext = new SQLContext(sc) val df = sqlContext.createDataFrame(sc.parallelize(d)) val specs = Window.partitionBy(f.col("user")).orderBy(f.col("ts").asc) val res = df.withColumn( "newsession", MyUDWF.calculateSession(f.col("ts"), f.col("session")) over specs) 怎么使用 spark sql 来统计会话数目呢，因为不同用户产生的是不同的会话，首先使用user字段进行分区，然后按照时间戳进行排序，然后我们需要一个自定义函数来加一个列，这个列的值的逻辑如下：1234IF (no previous event) create new sessionELSE (if cerrent event was past session window)THEN create new sessionELSE use current session 运行结果如下：我们使用 UUID 来作为会话id， 当后一行的时间戳和前一行的时间戳间隔大于1小时的时候，就创建一个新的会话id作为列值，否则使用老的会话id作为列值。 这种就涉及到状态，我们在内部需要维护的状态数据 当前的session ID 当前session的最后活动事件的时间戳 下面我们就看看这个怎样自定义 caculateSession 这个窗口函数，先自定义一个静态对象：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.UUIDimport org.apache.spark.sql.&#123;Column, Row&#125;import org.apache.spark.sql.catalyst.expressions.&#123;Add, AggregateWindowFunction, AttributeReference, Expression, If, IsNotNull, LessThanOrEqual, Literal, ScalaUDF, Subtract&#125;import org.apache.spark.sql.types._import org.apache.spark.unsafe.types.UTF8Stringobject MyUDWF &#123; val defaultMaxSessionLengthms = 3600 * 1000 case class SessionUDWF(timestamp:Expression, session:Expression, sessionWindow:Expression = Literal(defaultMaxSessionLengthms)) extends AggregateWindowFunction &#123; self: Product =&gt; override def children: Seq[Expression] = Seq(timestamp, session) override def dataType: DataType = StringType protected val zero = Literal( 0L ) protected val nullString = Literal(null:String) protected val curentSession = AttributeReference("currentSession", StringType, nullable = true)() protected val previousTs = AttributeReference("lastTs", LongType, nullable = false)() override val aggBufferAttributes: Seq[AttributeReference] = curentSession :: previousTs :: Nil protected val assignSession = If(LessThanOrEqual(Subtract(timestamp, aggBufferAttributes(1)), sessionWindow), aggBufferAttributes(0), // if ScalaUDF( createNewSession, StringType, children = Nil)) override val initialValues: Seq[Expression] = nullString :: zero :: Nil override val updateExpressions: Seq[Expression] = If(IsNotNull(session), session, assignSession) :: timestamp :: Nil override val evaluateExpression: Expression = aggBufferAttributes(0) override def prettyName: String = "makeSession" &#125; protected val createNewSession = () =&gt; org.apache.spark.unsafe.types.UTF8String.fromString(UUID.randomUUID().toString) def calculateSession(ts:Column,sess:Column): Column = withExpr &#123; SessionUDWF(ts.expr,sess.expr, Literal(defaultMaxSessionLengthms)) &#125; def calculateSession(ts:Column,sess:Column, sessionWindow:Column): Column = withExpr &#123; SessionUDWF(ts.expr,sess.expr, sessionWindow.expr) &#125; private def withExpr(expr: Expression): Column = new Column(expr)&#125; 代码说明： 状态保存在 Seq[AttributeReference]中 重写 initialValues方法进行初始化 updateExpressions 函数针对每一行数据都会调用 spark sql 在迭代处理每一行数据的时候，都会调用 updateExpressions 函数来处理，根据当后一行的时间戳和前一行的时间戳间隔大于1小时来进行不同的逻辑处理，如果不大于，就使用 aggBufferAttributes(0) 中保存的老的sessionid，如果大于，就把 createNewSession 包装为一个scalaUDF作为一个子表达式来创建一个新的sessionID，并且每次都把当前行的时间戳作为用户活动的最后时间戳。 最后包装为静态对象的方法，就可以在spark sql中使用这个自定义窗口函数了，下面是两个重载的方法，一个最大间隔时间使用默认值，一个可以运行用户自定义，perfect 现在，我们就可以拿来用在我们的main函数中了。1234567891011121314151617val st = System.currentTimeMillis()val one_minute = 60 * 1000val d = Array[UserActivityData]( UserActivityData("user1", st, "f237e656-1e53-4a24-9ad5-2b4576a4125d"), UserActivityData("user2", st + 5*one_minute, null), UserActivityData("user1", st + 10*one_minute, null), UserActivityData("user1", st + 15*one_minute, null), UserActivityData("user2", st + 15*one_minute, null), UserActivityData("user1", st + 140*one_minute, null), UserActivityData("user1", st + 160*one_minute, null)) val df = spark.createDataFrame(sc.parallelize(d)) val specs = Window.partitionBy(f.col("user")).orderBy(f.col("ts").asc) val res = df.withColumn( "newsession", MyUDWF.calculateSession(f.col("ts"), f.col("session")) over specs) df.show(20) res.show(20, false) 如果我们学会了自定义 spark 窗口函数，原理是就可以处理一切这种对前后数据依赖的统计需求，不过这种自定义毕竟需要对 spark 源码有很深入的研究才可以，这就需要功力了，希望 spark君的读者可以跟着spark君日益精进。文章中的代码可能不太清楚，完整demo参考 https://github.com/zheniantoushipashi/spark-udwf-session。]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala自定注解]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[为什么要注解除了编译和允许之外，还可以对程序做：1、使用Scaladoc 自动产生文档;2、漂亮的打印印出符合你偏爱分割的代码;3、代码常见错误检查，如：打开了文件却没(在全部逻辑分支中)关闭。4、实验类型检查，例如副作用管理或所有权属性确认。这类工具（程序）被称为元编程工具，它们把其它程序当做输入程序。 scala注解所在包和基本语法格式注解所在包： 标准库定义的注解相关内容在包scala.annotation中。基本语法： @注解名称(注解参数) 自定义注解自定义注解需要从注解特质继承，scala提供两种注解： 1、基本语法： @注解名称(注解参数) scala中的自定义注解不是接口/特质，而是类。自定义注解需要从注解特质中继承，Scala中提供了两类注解特质： scala.annotation.ClassfileAnnotation 由Java编译器生成注解scala.annotation.StaticAnnotation 由Scala编译器生成注解]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java自定义注解]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解和注释区别注释：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。注解：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。 注解的定义和使用模板注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。 定义格式：修饰符 @interface 注解名{} 使用格式： @注解名;例如：junit测试中的 @Test 复写方法中的@Override 自定义注解注解可以书写的位置定义的注解，可以书写在类的不同位置：类上、成员变量上、成员方法、构造方法等位置。在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置： ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上 ElementType.METHOD:当前的注解可以书写在方法上 ElementType.FIELD:当前的注解可以书写在成员变量（字段）上 ElementType.TYPE:当前的注解可以书写在类或接口上 注解生命周期 由@Retention 声明自己的注解可以存活的时间具体的存活的时间需要通过RetentionPolicy 中提供的值。RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。 注解中的成员变量注解定义的变量格式：数据类型 变量名(); 基本案例自定义注解12345678910111213import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD , ElementType.FIELD&#125;)public @interface MyAnnotation &#123; // 定义注解的变量 String name(); int age(); String sex();&#125; 自定义注解的使用123456789101112public class UseAnnotation &#123; @MyAnnotation(name="zhangsan",age=23,sex="nv") public UseAnnotation()&#123; &#125; @MyAnnotation(name="zhangsan",age=23,sex="nv") private String name; @MyAnnotation(name="zhangsan",age=23,sex="nv") public void show()&#123; &#125;&#125; 经典使用案例（mysql连接驱动）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/* * 自定义注解，封装数据库连接的四个参数 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface MyDriver &#123; String driver(); String url(); String user(); String pwd();&#125;import java.lang.reflect.Method;import java.sql.Connection;import java.sql.DriverManager;/* * 专门负责获取数据库的连接工具类 */public class JDBCUtils &#123; @MyDriver(driver="com.mysql.jdbc.Driver", url="jdbc:mysql://localhost:3306/estore",user="root",pwd="abc") public static Connection getConnection() throws Exception&#123; //获取当前类的class文件 Class clazz = JDBCUtils.class; Method method = clazz.getMethod("getConnection", null); //获取当前方法上的注解信息 if( method.isAnnotationPresent(MyDriver.class) )&#123; MyDriver an = method.getAnnotation(MyDriver.class); String driver = an.driver(); String url = an.url(); String user = an.user(); String pwd = an.pwd(); //获取数据库的连接，加载驱动，获取连接 Class.forName(driver); //获取连接 Connection conn = DriverManager.getConnection(url, user, pwd); return conn; &#125; return null; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; Connection conn = JDBCUtils.getConnection(); System.out.println(conn); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven删除本地仓库lastUpdated文件]]></title>
    <url>%2F2018%2F08%2F13%2Fmaven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[12windows:for /r %i in (*.lastUpdated) do del %i 12linux:find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 查询数据1select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
