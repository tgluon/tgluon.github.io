<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java concurrent包类详解]]></title>
    <url>%2F2019%2F02%2F13%2Fjava%20concurrent%E5%8C%85%E7%B1%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Semaphore原文：https://zhuanlan.zhihu.com/p/27314456Semaphore(信号量)是java.util.concurrent下的一个工具类.用来控制可同时访问特定资源的线程数.内部是通过维护父类(AQS)的 int state值实现.Semaphore中有一个”许可”的概念: 访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。访问资源后，使用release()释放许可。 这个许可在构造时传入,赋给state值,它等同于state. Semaphore应用场景系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。使用案例：spark LiveListenerBus类1234567891011121314151617181920212223242526272829303132private val eventLock = new Semaphore(0)private val listenerThread = new Thread(name) &#123; setDaemon(true) override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) &#123; LiveListenerBus.withinListenerThread.withValue(true) &#123; while (true) &#123; eventLock.acquire() self.synchronized &#123; processingEvent = true &#125; try &#123; // 消费消息 val event = eventQueue.poll if (event == null) &#123; // Get out of the while loop and shutdown the daemon thread if (!stopped.get) &#123; throw new IllegalStateException("Polling `null` from eventQueue means" + " the listener bus has been stopped. So `stopped` must be true") &#125; return &#125; postToAll(event) &#125; finally &#123; self.synchronized &#123; processingEvent = false &#125; &#125; &#125; &#125; &#125;&#125; Semaphore属于一种较常见的限流手段，Google Guava封装了一层。123456789101112131415161718//JDK API：流速控制在每秒执行100个任务final Semaphore semaphore = new Semaphore(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; semaphore.acquire(); // 也许需要等待 executor.execute(task); semaphore.release(); &#125;&#125;//Google Guava API：流速控制在每秒执行100个任务final RateLimiter rateLimiter = RateLimiter.create(100);void submitTasks(List&lt;Runnable&gt; tasks, Executor executor) &#123; for (Runnable task : tasks) &#123; rateLimiter.acquire(); // 也许需要等待 executor.execute(task); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala学习笔记]]></title>
    <url>%2F2019%2F01%2F09%2Fscala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[高级函数高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。 匿名函数1(x: Int) =&gt; x * 3 带函数参数的函数Scala集合类（collections）的高阶函数map。1234567891011121314151617181920def map[B, That](f: A =&gt; B)(implicit bf: CanBuildFrom[Repr, B, That]): That = &#123; def builder = &#123; val b = bf(repr) b.sizeHint(this) b &#125; val b = builder for (x &lt;- this) b += f(x) b.result &#125; val seq = Seq(100, 200, 300) def doubleSalary(x: Int): Int = &#123; x * 2 &#125; seq.map(doubleSalary).foreach(x =&gt; println(x)) seq.map(x =&gt; x * 2) seq.map(_ * 2) 闭包123def sum(x: Int) = (y: Int) =&gt; x + yval first = sum(1)val second = first(4) 嵌套方法在Scala中可以嵌套定义方法。12345678910def factorial(x: Int): Int = &#123; def fact(x: Int, accumulator: Int): Int = &#123; if (x &lt;= 1) accumulator else fact(x - 1, x * accumulator) &#125; fact(x, 1) &#125; println("Factorial of 2: " + factorial(2)) println("Factorial of 3: " + factorial(3)) 柯里化柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。接受两个参数的过程：1def mul(x:Int , y: Int )= x * y 接受一个参数的过程12def mul(x: Int) = (y: Int) =&gt; x + ymul(1)(2) scala支持简写成如下的柯里化：123def mul(x: Int)(y: Int) = x + ymul(1)(2) 在Scala集合中定义的特质TraversableOnce[+A]。Traversable： 能横过的；能越过的；可否定的。123456789101112131415def foldLeft[B](z: B)(op: (B, A) =&gt; B): B = &#123; var result = z this foreach (x =&gt; result = op(result, x)) result &#125;``` foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：从初值0开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。 ```scalaval numbers = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)val res = numbers.foldLeft(0)((m, n) =&gt; m + n)val res2 = numbers.foldLeft(0)(_ + _)println(res) // 55pringln(res2) 多参数列表有更复杂的调用语法，因此应该谨慎使用。建议的使用场景包括: 单一的函数参数。在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样： 1numbers.foldLeft(0, &#123;(m: Int, n: Int) =&gt; m + n&#125;) 隐式（IMPLICIT）参数。 1def execute(arg: Int)(implicit ec: ExecutionContext) = ??? 模式匹配match对应java里的Switch,不过它写在选择器表达式之后。 选择器 match {备选项}取代了：switch(选择器){备选项} match与switch的比较匹配表达式可以被看做java风格switch的泛化。match的不同：1、match是scala表达式，也就是说，它始终以值作为结果;2、 scala的备选项表达式永远不会”掉到”下一个case;3、 如果没有模式匹配，MatchErro异常会被抛出。 提取器对象提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。123456789101112131415161718case class Person(name: String, age: Int) def main(args: Array[String]): Unit = &#123; // 调用工厂构造方法，构造出对象实例 val person = Person("Spark", 6) // 这种写法居然可以正常编译 val Person(name, age) = person /* * 使用了提取器， * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类 */ println(name + " : " + age) //正常输出: Spark 6 person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125; 自定义unapply方法主要用于模式匹配中。1234567891011121314151617181920212223class Person(val name: String, val salary: Int)object Person &#123; def apply(name: String, salary: Int):Person = &#123; new Person(name, salary) &#125; def unapply(money: Person): Option[(String, Int)] = &#123; if(money == null) &#123; None &#125; else &#123; Some(money.name, money.salary) &#125; &#125; def main(args: Array[String]): Unit = &#123; val person = Person("spark", 800); person match &#123; // match过程就是调用提取器的过程 case Person(name, age) =&gt; println("Wow, " + name + " : " + age) &#125; &#125;&#125; for 表达式Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。例子1234567891011case class User(name: String, age: Int)val userBase = List(User("Travis", 28), User("Kelly", 33), User("Jennifer", 44), User("Dennis", 23))val twentySomethings = for (user &lt;- userBase if (user.age &gt;=20 &amp;&amp; user.age &lt; 30)) yield user.name twentySomethings.foreach(name =&gt; println(name)) 这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user =20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。 泛型泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。 定义一个泛型类泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。12345678910class Stack[A] &#123; private var elements: List[A] = Nil def push(x: A) &#123; elements = x :: elements &#125; def peek: A = elements.head def pop(): A = &#123; val currentTop = peek elements = elements.tail currentTop &#125;&#125; 上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。 使用要使用一个泛型类，将一个具体类型放到方括号中来代替 A。12345val stack = new Stack[Int]stack.push(1)stack.push(2)println(stack.pop) // prints 2println(stack.pop) // prints 1 型变型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。123class Foo[+A] // 一个协变类class Bar[-A] // 一个逆变类class Baz[A] // 一个不变类 协变使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。 考虑以下简单的类结构：12345abstract class Animal &#123; def name: String&#125;case class Cat(name: String) extends Animalcase class Dog(name: String) extends Animal 类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。 在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。1234567891011121314151617object CovarianceTest extends App &#123; def printAnimalNames(animals: List[Animal]): Unit = &#123; animals.foreach &#123; animal =&gt; println(animal.name) &#125; &#125; val cats: List[Cat] = List(Cat("Whiskers"), Cat("Tom")) val dogs: List[Dog] = List(Dog("Fido"), Dog("Rex")) printAnimalNames(cats) // Whiskers // Tom printAnimalNames(dogs) // Fido // Rex&#125; 逆变通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。 考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：123abstract class Printer[-A] &#123; def print(value: A): Unit&#125; 这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：123456789class AnimalPrinter extends Printer[Animal] &#123; def print(animal: Animal): Unit = println("The animal's name is: " + animal.name)&#125;class CatPrinter extends Printer[Cat] &#123; def print(cat: Cat): Unit = println("The cat's name is: " + cat.name)&#125; 如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。12345678910111213object ContravarianceTest extends App &#123; val myCat: Cat = Cat("Boots") def printMyCat(printer: Printer[Cat]): Unit = &#123; printer.print(myCat) &#125; val catPrinter: Printer[Cat] = new CatPrinter val animalPrinter: Printer[Animal] = new AnimalPrinter printMyCat(catPrinter) printMyCat(animalPrinter)&#125; 这个程序的输出如下： The cat’s name is: BootsThe animal’s name is: Boots 不变默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。1234567class Container[A](value: A) &#123; private var _value: A = value def getValue: A = _value def setValue(value: A): Unit = &#123; _value = value &#125;&#125; 可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：1234val catContainer: Container[Cat] = new Container(Cat("Felix"))val animalContainer: Container[Animal] = catContaineranimalContainer.setValue(Dog("Spot"))val cat: Cat = catContainer.getValue 糟糕，我们最终会将一只狗作为值分配给一只猫幸运的是，编译器在此之前就会阻止我们。 上界在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。1234567891011121314151617181920212223242526abstract class Animal &#123; def name: String&#125;abstract class Pet extends Animal &#123;&#125;class Cat extends Pet &#123; override def name: String = "Cat"&#125;class Dog extends Pet &#123; override def name: String = "Dog"&#125;class Lion extends Animal &#123; override def name: String = "Lion"&#125;class PetContainer[P &lt;: Pet](p: P) &#123; def pet: P = p&#125;val dogContainer = new PetContainer[Dog](new Dog)val catContainer = new PetContainer[Cat](new Cat)// this would not compileval lionContainer = new PetContainer[Lion](new Lion) 类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：1type arguments [Lion] do not conform to class PetContainer's type parameter bounds [P &lt;: Pet] 这是因为Lion并不是Pet的子类。 下界类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。 下面看一个适合用类型下界的例子：下面看一个适合用类型下界的例子：12345678910111213trait Node[+B] &#123; def prepend(elem: B): Node[B]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend(elem: B): ListNode[B] = ListNode(elem, this)&#125; 该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。 但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。 要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。12345678910111213trait Node[+B] &#123; def prepend[U &gt;: B](elem: U): Node[U]&#125;case class ListNode[+B](h: B, t: Node[B]) extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this) def head: B = h def tail: Node[B] = t&#125;case class Nil[+B]() extends Node[B] &#123; def prepend[U &gt;: B](elem: U): ListNode[U] = ListNode(elem, this)&#125; 现在我们像下面这么做：123456trait Birdcase class AfricanSwallow() extends Birdcase class EuropeanSwallow() extends Birdval africanSwallowList= ListNode[AfricanSwallow](AfricanSwallow(), Nil())val birdList: Node[Bird] = africanSwallowListbirdList.prepend(new EuropeanSwallow) 可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。 抽象类型特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345trait Buffer &#123; type T val element: T&#125;&#125; 这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：12345abstract class SeqBuffer extends Buffer &#123; type U type T &lt;: Seq[U] def length = element.length&#125; 注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。 含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。12345678910111213abstract class IntSeqBuffer extends SeqBuffer &#123; type U = Int&#125;def newIntSeqBuf(elem1: Int, elem2: Int): IntSeqBuffer = new IntSeqBuffer &#123; type T = List[U] val element = List(elem1, elem2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。 把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：123456789101112131415abstract class Buffer[+T] &#123; val element: T&#125;abstract class SeqBuffer[U, +T &lt;: Seq[U]] extends Buffer[T] &#123; def length = element.length&#125;def newIntSeqBuf(e1: Int, e2: Int): SeqBuffer[Int, Seq[Int]] = new SeqBuffer[Int, List[Int]] &#123; val element = List(e1, e2) &#125;val buf = newIntSeqBuf(7, 8)println("length = " + buf.length)println("content = " + buf.element) 需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。 复合类型需求:有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。 案例：假设我们有两个特质 Cloneable 和 Resetable：12345678trait Cloneable extends java.lang.Cloneable &#123; override def clone(): Cloneable = &#123; super.clone().asInstanceOf[Cloneable] &#125;&#125;trait Resetable &#123; def reset: Unit&#125; 现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：12345def cloneAndReset(obj: ?): Cloneable = &#123; val cloned = obj.clone() obj.reset cloned&#125; 这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。 以下是更新后的方法：123def cloneAndReset(obj: Cloneable with Resetable): Cloneable = &#123; //...&#125; 复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement } 关于使用细化的例子参考 通过混入（mixin）来组合类。 自类型自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。 自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。 要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。123456789101112131415trait User &#123; def username: String&#125;trait Tweeter &#123; this: User =&gt; // 重新赋予 this 的类型 def tweet(tweetText: String) = println(s"$username: $tweetText")&#125;class VerifiedTweeter(val username_ : String) extends Tweeter with User &#123; // 我们混入特质 User 因为 Tweeter 需要 def username = s"real $username_"&#125;val realBeyoncé = new VerifiedTweeter("Beyoncé")realBeyoncé.tweet("Just spilled my glass of lemonade") // 打印出 "real Beyoncé: Just spilled my glass of lemonade" 因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。自类型别名12345678910object Demo &#123; self =&gt; def sum(num1: Int, num2: Int): Int = &#123; num1 + num2 &#125; def main(args: Array[String]): Unit = &#123; println(self.sum(1, 2)) &#125;&#125; 隐式转换定义：指的是那种以implicit关键字声明的带有单个参数的函数。 案例给File类增加read方法123class RichFile(val f: File) &#123; def read() = Source.fromFile(f).mkString&#125; 门面类12345import java.io.Fileobject RichFilePredef &#123; implicit def fileToRichFile(f: File) = new RichFile(f)&#125; 使用1234567891011import java.io.Fileimport scala.io.Sourceimport com.tm.scala.implic.RichFilePredef._object RichFile &#123; def main(args: Array[String]) &#123; val f = new File("/home/hadoop/sample_movielens_data.txt") // 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object val contents = f.read() println(contents) &#125;&#125; 排序中的使用案例Ordered方式1class Girl(val name: String, var faceValue: Int, var age: Int) 定义比较规则123456789101112/** * 视图定界 * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。 * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。 * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html */class OrderedChooser[T &lt;% Ordered[T]] &#123; def choose(first: T, second: T): T = &#123; if (first &gt; second) first else second &#125;&#125; 门面类1234567891011121314151617181920212223object OrderedPredef &#123; // 方式一 implicit def gilrToOrdered(girl: Girl):Ordered[Girl] = new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125; // 方式二 implicit val gilrToOrdered = (girl: Girl) =&gt; new Ordered[Girl] &#123; override def compare(that: Girl): Int = &#123; if (girl.faceValue == that.faceValue) &#123; girl.age - that.age &#125; else &#123; girl.faceValue - that.faceValue &#125; &#125; &#125;&#125; 测试类12345678910object TestOrdered &#123; def main(args: Array[String]): Unit = &#123; val girl1 = new Girl("spark", 100,50) val girl2 = new Girl("mxnet", 90,30) import OrderedPredef._ val chooser = new OrderingChoose[Girl] val g = chooser.choose(girl1, girl2) println(g.faceValue) &#125;&#125; Ordering方式定义比较规则12345678910/** * 文本定界 */class OrderingChoose[T: Ordering] &#123; def choose(first: T, second: T): T = &#123; val ord = implicitly[Ordering[T]] if (ord.gt(first, second)) first else second &#125;&#125; 门面类123456789101112131415161718192021222324252627282930313233343536object OrderingPredef &#123; // 方式一 implicit val gilrToOrdering = new Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式二 implicit object GilrToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; // 方式三 /** * 参考：Ordering中的下面方法 * trait IntOrdering extends Ordering[Int] &#123; * def compare(x: Int, y: Int) = * if (x &lt; y) -1 * else if (x == y) 0 * else 1 * &#125; * implicit object Int extends IntOrdering */ trait GirlToOrdering extends Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; x.faceValue - y.faceValue &#125; &#125; implicit object Girl extends GirlToOrdering&#125; 测试类12345678910object TestOrdering &#123; def main(args: Array[String]): Unit = &#123; val g1 = new Girl("zhangsan", 50,50) val g2 = new Girl("lisi", 500,50) import OrderingPredef._ val choose = new OrderingChoose[Girl] val g = choose.choose(g1, g2) println(g.name) &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala自定注解]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[为什么要注解除了编译和允许之外，还可以对程序做：1、使用Scaladoc 自动产生文档;2、漂亮的打印印出符合你偏爱分割的代码;3、代码常见错误检查，如：打开了文件却没(在全部逻辑分支中)关闭。4、实验类型检查，例如副作用管理或所有权属性确认。这类工具（程序）被称为元编程工具，它们把其它程序当做输入程序。 scala注解所在包和基本语法格式注解所在包： 标准库定义的注解相关内容在包scala.annotation中。基本语法： @注解名称(注解参数) 自定义注解自定义注解需要从注解特质继承，scala提供两种注解： 1、基本语法： @注解名称(注解参数) scala中的自定义注解不是接口/特质，而是类。自定义注解需要从注解特质中继承，Scala中提供了两类注解特质： scala.annotation.ClassfileAnnotation 由Java编译器生成注解scala.annotation.StaticAnnotation 由Scala编译器生成注解]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala特殊符号使用]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[_*符号作用：它告诉编译器将序列类型的单个参数视为可变参数序列。举例：12345public ProcessBuilder(String... command) &#123; this.command = new ArrayList&lt;&gt;(command.length); for (String arg : command) this.command.add(arg); &#125; 12val commandSeq = Seq(command.mainClass) ++ command.argumentsval builder = new ProcessBuilder(commandSeq: _*) ::该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。1234567891011121314151617181920212223242526272829303132def main(args: Array[String]) &#123; args.toList match &#123; case workerUrl :: userJar :: mainClass :: extraArgs =&gt; val conf = new SparkConf() val rpcEnv = RpcEnv.create("Driver", Utils.localHostName(), 0, conf, new SecurityManager(conf)) rpcEnv.setupEndpoint("workerWatcher", new WorkerWatcher(rpcEnv, workerUrl)) val currentLoader = Thread.currentThread.getContextClassLoader val userJarUrl = new File(userJar).toURI().toURL() val loader = if (sys.props.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) &#123; new ChildFirstURLClassLoader(Array(userJarUrl), currentLoader) &#125; else &#123; new MutableURLClassLoader(Array(userJarUrl), currentLoader) &#125; Thread.currentThread.setContextClassLoader(loader) // Delegate to supplied main class val clazz = Utils.classForName(mainClass) val mainMethod = clazz.getMethod("main", classOf[Array[String]]) mainMethod.invoke(null, extraArgs.toArray[String]) rpcEnv.shutdown() case _ =&gt; // scalastyle:off println System.err.println("Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]") // scalastyle:on println System.exit(-1) &#125; &#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala通过反射获取当前类类名]]></title>
    <url>%2F2019%2F01%2F04%2Fscala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[先编写HelloWord.scala文件123456object HelloWorld &#123; def main(args: Array[String]) &#123; // 获取当前类类名 println(this.getClass.getName.stripSuffix("$")) &#125;&#125; 命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class spark源码中的案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.spark.internalimport org.apache.log4j.&#123;Level, LogManager, PropertyConfigurator&#125;import org.slf4j.&#123;Logger, LoggerFactory&#125;import org.slf4j.impl.StaticLoggerBinderimport org.apache.spark.util.Utils/** * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows * logging messages at different levels using methods that only evaluate parameters lazily if the * log level is enabled. */private[spark] trait Logging &#123; // Make the log field transient so that objects with Logging can // be serialized and used on another machine @transient private var log_ : Logger = null // Method to get the logger name for this object protected def logName = &#123; // Ignore trailing $'s in the class names for Scala objects this.getClass.getName.stripSuffix("$") &#125; // Method to get or create the logger for this object protected def log: Logger = &#123; if (log_ == null) &#123; initializeLogIfNecessary(false) log_ = LoggerFactory.getLogger(logName) &#125; log_ &#125; // Log methods that take only a String protected def logInfo(msg: =&gt; String) &#123; if (log.isInfoEnabled) log.info(msg) &#125; protected def logDebug(msg: =&gt; String) &#123; if (log.isDebugEnabled) log.debug(msg) &#125; protected def logTrace(msg: =&gt; String) &#123; if (log.isTraceEnabled) log.trace(msg) &#125; protected def logWarning(msg: =&gt; String) &#123; if (log.isWarnEnabled) log.warn(msg) &#125; protected def logError(msg: =&gt; String) &#123; if (log.isErrorEnabled) log.error(msg) &#125; // Log methods that take Throwables (Exceptions/Errors) too protected def logInfo(msg: =&gt; String, throwable: Throwable) &#123; if (log.isInfoEnabled) log.info(msg, throwable) &#125; protected def logDebug(msg: =&gt; String, throwable: Throwable) &#123; if (log.isDebugEnabled) log.debug(msg, throwable) &#125; protected def logTrace(msg: =&gt; String, throwable: Throwable) &#123; if (log.isTraceEnabled) log.trace(msg, throwable) &#125; protected def logWarning(msg: =&gt; String, throwable: Throwable) &#123; if (log.isWarnEnabled) log.warn(msg, throwable) &#125; protected def logError(msg: =&gt; String, throwable: Throwable) &#123; if (log.isErrorEnabled) log.error(msg, throwable) &#125; protected def isTraceEnabled(): Boolean = &#123; log.isTraceEnabled &#125; protected def initializeLogIfNecessary(isInterpreter: Boolean): Unit = &#123; if (!Logging.initialized) &#123; Logging.initLock.synchronized &#123; if (!Logging.initialized) &#123; initializeLogging(isInterpreter) &#125; &#125; &#125; &#125; private def initializeLogging(isInterpreter: Boolean): Unit = &#123; // Don't use a logger in here, as this is itself occurring during initialization of a logger // If Log4j 1.2 is being used, but is not initialized, load a default properties file val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr // This distinguishes the log4j 1.2 binding, currently // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently // org.apache.logging.slf4j.Log4jLoggerFactory val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass) if (usingLog4j12) &#123; val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements // scalastyle:off println if (!log4j12Initialized) &#123; val defaultLogProps = "org/apache/spark/log4j-defaults.properties" Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match &#123; case Some(url) =&gt; PropertyConfigurator.configure(url) System.err.println(s"Using Spark's default log4j profile: $defaultLogProps") case None =&gt; System.err.println(s"Spark was unable to load $defaultLogProps") &#125; &#125; if (isInterpreter) &#123; // Use the repl's main class to define the default log level when running the shell, // overriding the root logger's config if they're different. val rootLogger = LogManager.getRootLogger() val replLogger = LogManager.getLogger(logName) val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN) if (replLevel != rootLogger.getEffectiveLevel()) &#123; System.err.printf("Setting default log level to \"%s\".\n", replLevel) System.err.println("To adjust logging level use sc.setLogLevel(newLevel). " + "For SparkR, use setLogLevel(newLevel).") rootLogger.setLevel(replLevel) &#125; &#125; // scalastyle:on println &#125; Logging.initialized = true // Force a call into slf4j to initialize it. Avoids this happening from multiple threads // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html log &#125;&#125;private object Logging &#123; @volatile private var initialized = false val initLock = new Object() try &#123; // We use reflection here to handle the case where users remove the // slf4j-to-jul bridge order to route their logs to JUL. val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler") bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null) val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean] if (!installed) &#123; bridgeClass.getMethod("install").invoke(null) &#125; &#125; catch &#123; case e: ClassNotFoundException =&gt; // can't log anything yet so just fail silently &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparksql详解]]></title>
    <url>%2F2019%2F01%2F04%2Fsparksql%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark SQL的Dataset基本操作说明：文章来自https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解 1. 基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。 1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是1type DataFrame = Dataset[Row] 结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。 首先要创建一个SparkSession:12345678910111213141516171819val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]").set("yarn.resourcemanager.hostname", "localhost") // executor的实例数.set("spark.executor.instances","2") .set("spark.default.parallelism","4") // sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。.set("spark.sql.shuffle.partitions","4").setJars(List("/Users/meitu/Desktop/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar"))val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() 创建dataset123456val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100), ("Warsaw", 2017, 200), ("Warsaw", 2015, 100), ("Warsaw", 2017, 200), ("Beijing", 2017, 200), ("Beijing", 2016, 200), ("Beijing", 2015, 200), ("Beijing", 2014, 200), ("Warsaw", 2014, 200), ("Boston", 2017, 50), ("Boston", 2016, 50), ("Boston", 2015, 50), ("Boston", 2014, 150))).toDF("city", "year", "amount") 使用函数的时候要导入包：1import org.apache.spark.sql.functions.&#123;col,expr&#125; 2.select列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。使用col函数，可以直接对列进行一些逻辑操作。12sales.select("city","year","amount").show(1)sales.select(col("city"),col("amount")+1).show(1) 3. selectExpr参数是字符串，且直接可以使用表达式。也可以使用select+expr函数来替代。12sales.selectExpr("city","year as date","amount+1").show(10)sales.select(expr("city"),expr("year as date"),expr("amount+1")).show(10) 4. filter参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。123sales.filter(col("amount")&gt;150).show()sales.filter(row=&gt;&#123; row.getInt(2)&gt;150&#125;).show(10)sales.filter("amount &gt; 150 ").show(10) 5. where类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。12sales.where(col("amount")&gt;150).show()sales.where("amount &gt; 150 ").show() 6. group by主要是以count和agg聚合函数为例讲解groupby函数。12sales.groupBy("city").count().show(10)sales.groupBy(col("city")).agg(sum("amount").as("total")).show(10) 7.union两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。1sales.union(sales).groupBy("city").count().show() 8.joinjoin操作相对比较复杂，具体如下：123456789101112131415161718192021222324252627282930// 相同的列进行join sales.join(sales,"city").show(10) // 多列joinsales.join(sales,Seq("city","year")).show() /* 指定join类型， join 类型可以选择: `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti`. */ // 内部joinsales.join(sales,Seq("city","year"),"inner").show() /* join条件 ：可以在join方法里放入join条件，也可以使用where,这两种情况都要求字段名称不一样。*/ sales.join(sales, col("city").alias("city1") === col("city")).show() sales.join(sales).where(col("city").alias("city1") === col("city")).show() /* dataset的self join 此处使用where作为条件，需要增加配置.set("spark.sql.crossJoin.enabled","true") 也可以加第三个参数，join类型，可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` */ sales.join(sales,sales("city") === sales("city")).show() sales.join(sales).where(sales("city") === sales("city")).show()/* joinwith,可以指定第三个参数，join类型，类型可以选择如下： `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`。 */ sales.joinWith(sales,sales("city") === sales("city"),"inner").show() 输出结果： 9. order byorderby 全局有序，其实用的还是sort12sales.orderBy(col("year").desc,col("amount").asc).show()sales.orderBy("city","year").show() 10.sort全局排序，直接替换掉8小结的orderby即可。 11.sortwithinpartition在分区内部进行排序，局部排序。12sales.sortWithinPartitions(col("year").desc,col("amount").asc).show()sales.sortWithinPartitions("city","year").show() 可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。 12. withColumn1234567891011/* withColumn 假如列，存在就替换，不存在新增 withColumnRenamed 对已有的列进行重命名 *///相当于给原来amount列，+1sales.withColumn("amount",col("amount")+1).show()// 对amount列+1，然后将值增加到一个新列 amount1sales.withColumn("amount1",col("amount")+1).show()// 将amount列名，修改为amount1sales.withColumnRenamed("amount","amount1").show() 13. foreach这个跟rdd的foreach一样，元素类型是row。123sales.foreach(row=&gt;&#123; println(row.getString(0))&#125;) 14. foreachPartition跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。12345678sales.foreachPartition(partition=&gt;&#123; //打开数据库链接等 partition.foreach(each=&gt;&#123; println(each.getString(0)) //插入数据库 &#125;) //关闭数据库链接 &#125;) 15. distinct针对dataset的行去重，返回的是所有行都不重复的dataset。1sales.distinct().show(10) 16. dropDuplicates这个适用于dataset有唯一的主键，然后对主键进行去重。1234val before = sales.count()val after = sales.dropDuplicates("city").count()println("before ====&gt; " +before)println("after ====&gt; "+after) 17. drop删除一列，或者多列，这是一个变参数算子。12345sales.drop("city").show()打印出来schema信息如下：root|-- year: integer (nullable = false)|-- amount: integer (nullable = false) 18.printSchema输出dataset的schema信息1234567891011sales.printSchema()输出结果如下：root|-- city: string (nullable = true)|-- year: integer (nullable = false)|-- amount: integer (nullable = false) 17.explain()打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程123456sales.orderBy(col("year").desc,col("amount").asc).explain()执行计划输出如下：== Physical Plan ==*(1) Sort [year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST], true, 0+- Exchange rangepartitioning(year#7 DESC NULLS LAST, amount#8 ASC NULLS FIRST, 3)+- LocalTableScan [city#6, year#7, amount#8] Spark SQL入门到精通之第二篇Dataset的复杂操作本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：cube，rollup，pivot操作。 1. cube简单的理解就是维度及度量组成的数据体。1234sales.cube("city","year") .agg(sum("amount")) .sort(col("city").desc_nulls_first,col("year").desc_nulls_first) .show() 举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：123456789101112131415val city_year = sales.groupBy("city","year").agg(sum("amount"))val city = sales.groupBy("city") .agg(sum("amount") as "amount") .select(col("city"), lit(null) as "year", col("amount"))val year = sales.groupBy("year") .agg(sum("amount") as "amount") .select( lit(null) as "city",col("year"), col("amount"))val none = sales .groupBy().agg(sum("amount") as "amount") .select(lit(null) as "city", lit(null) as "year", col("amount")) city_year.union(city).union(year).union(none) .sort(desc_nulls_first("city"), desc_nulls_first("year")) .show() 2. rollup这里也是以案例开始，代码如下：12345678910111213val expenses = spark.createDataFrame(Seq( ((2012, Month.DECEMBER, 12), 5), ((2016, Month.AUGUST, 13), 10), ((2017, Month.MAY, 27), 15)) .map &#123; case ((yy, mm, dd), a) =&gt; (LocalDate.of(yy, mm, dd), a) &#125; .map &#123; case (d, a) =&gt; (d.toString, a) &#125; .map &#123; case (d, a) =&gt; (Date.valueOf(d), a) &#125;).toDF("date", "amount")// rollup time!val res = expenses .rollup(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount") .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：123456789101112val year_month = expenses.groupBy(year(col("date")) as "year", month(col("date")) as "month") .agg(sum("amount") as "amount")val yearOnly = expenses.groupBy(year(col("date")) as "year") .agg(sum("amount") as "amount") .select(col("year"), lit(null) as "month", col("amount"))val none = expenses.groupBy() .agg(sum("amount") as "amount") .select(lit(null) as "year", lit(null) as "month", col("amount"))year_month.union(yearOnly).union(none) .sort(col("year").asc_nulls_last, col("month").asc_nulls_last) .show() 3. pivot旋转操作12345678910111213val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Warsaw", 2016, 100,"Warsaw"), ("Warsaw", 2017, 200,"Warsaw"), ("Boston", 2015, 50,"Boston"), ("Boston", 2016, 150,"Boston"), ("Toronto", 2017, 50,"Toronto"))).toDF("city", "year", "amount","test")sales.groupBy("year").pivot("city",Seq("Warsaw","Boston","Toronto")) .agg(sum("amount") as "amount") .show() 思路就是首先对year分组，然后旋转city字段，只取:“Warsaw”,”Boston”,”Toronto”然后对amount进行聚合操作 源码:Spark SQL 分区特性第一弹常见RDD分区Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。举几个浪尖在星球里分享比较多的例子，比如：Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据首先是由Seq数据集合生成一个Dataset1234567891011121314151617181920212223242526val sales = spark.createDataFrame(Seq( ("Warsaw", 2016, 110), ("Warsaw", 2017, 10), ("Warsaw", 2015, 100), ("Warsaw", 2015, 50), ("Warsaw", 2015, 80), ("Warsaw", 2015, 100), ("Warsaw", 2015, 130), ("Warsaw", 2015, 160), ("Warsaw", 2017, 200), ("Beijing", 2017, 100), ("Beijing", 2016, 150), ("Beijing", 2015, 50), ("Beijing", 2015, 30), ("Beijing", 2015, 10), ("Beijing", 2014, 200), ("Beijing", 2014, 170), ("Boston", 2017, 50), ("Boston", 2017, 70), ("Boston", 2017, 110), ("Boston", 2017, 150), ("Boston", 2017, 180), ("Boston", 2016, 30), ("Boston", 2015, 200), ("Boston", 2014, 20) )).toDF("city", "year", "amount") 将Dataset存处为partquet格式的hive表，分两种情况：用city和year字段分区1sales.write.partitionBy("city","year").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCityAndYear") 用city字段分区1sales.write.partitionBy("city").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCity") 读取数据采用的是1val res = spark.read.parquet("/user/hive/warehouse/parquettestcity") 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40Dataset的分区数是由参数：目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4Dataset的分区数是由参数：1println("partition size = "+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。那么数据源生成的Dataset的分区数到底是如何决定的呢？我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private def createNonBucketedReadRDD( readFile: (PartitionedFile) =&gt; Iterator[InternalRow], selectedPartitions: Seq[PartitionDirectory], fsRelation: HadoopFsRelation): RDD[InternalRow] = &#123; /* selectedPartitions 的大小代表目录数目 */ println("selectedPartitions.size : "+ selectedPartitions.size) val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes // spark.default.parallelism val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism // 计算文件总大小，单位字节数 val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum //计算平均每个并行度读取数据大小 val bytesPerCore = totalBytes / defaultParallelism // 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 // 然后，比较spark.sql.files.maxPartitionBytes 取小者 val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " + s"open cost is considered as scanning $openCostInBytes bytes.") // 这对目录遍历 val splitFiles = selectedPartitions.flatMap &#123; partition =&gt; partition.files.flatMap &#123; file =&gt; val blockLocations = getBlockLocations(file) //判断文件类型是否支持分割，以parquet为例，是支持分割的 if (fsRelation.fileFormat.isSplitable( fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123; // eg. 0 until 2不包括 2。相当于 // println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) (0L until file.getLen by maxSplitBytes).map &#123; offset =&gt; // 计算文件剩余的量 val remaining = file.getLen - offset // 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining // 位置信息 val hosts = getBlockHosts(blockLocations, offset, size) PartitionedFile( partition.values, file.getPath.toUri.toString, offset, size, hosts) &#125; &#125; else &#123; // 不可分割的话，那即是一个文件一个分区 val hosts = getBlockHosts(blockLocations, 0, file.getLen) Seq(PartitionedFile( partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &#125; &#125; &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) val partitions = new ArrayBuffer[FilePartition] val currentFiles = new ArrayBuffer[PartitionedFile] var currentSize = 0L /** Close the current partition and move to the next. */ def closePartition(): Unit = &#123; if (currentFiles.nonEmpty) &#123; val newPartition = FilePartition( partitions.size, currentFiles.toArray.toSeq) // Copy to a new Array. partitions += newPartition &#125; currentFiles.clear() currentSize = 0 &#125; // Assign files to partitions using "Next Fit Decreasing" splitFiles.foreach &#123; file =&gt; if (currentSize + file.length &gt; maxSplitBytes) &#123; closePartition() &#125; // Add the given file to the current partition. currentSize += file.length + openCostInBytes currentFiles += file &#125; closePartition() println("FileScanRDD partitions size : "+partitions.size) new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &#125; 找到一列中的中位数https://spark.apache.org/docs/latest/api/sql/index.htmldf函数： approxQuantilesql函数： percentile_approx 自定义数据源ServiceLoaderhttps://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3wServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。要配置在相关项目的固定目录下：resources/META-INF/services/接口全称。这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：SparkContext类1234567891011 private def getClusterManager(url: String): Option[ExternalClusterManager] = &#123; val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size &gt; 1) &#123; throw new SparkException( s"Multiple external cluster managers registered for the url $url: $serviceLoaders") &#125; serviceLoaders.headOption &#125;&#125; 配置是在spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。简单测试首先实现一个接口1234567package bigdata.spark.services;public interface DoSomething &#123; //可以制定实现类名加载 public String shortName(); public void doSomeThing();&#125; 然后将接口配置在resources/META-INF/services/bigdata.spark.services.DoSomething文件内容：实现该接口12345678910111213package bigdata.spark.services;public class SayHello implements DoSomething &#123; @Override public String shortName() &#123; return "SayHello"; &#125; @Override public void doSomeThing() &#123; System.out.println("hello !!!"); &#125;&#125; 测试12345678910111213package bigdata.spark.services;import java.util.ServiceLoader;public class test &#123; static ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class); public static void main(String[] args)&#123; for(DoSomething sayhello : loader)&#123; //要加载的类名称我们可以制定 if(sayhello.shortName().equalsIgnoreCase("SayHello"))&#123; sayhello.doSomeThing(); &#125; &#125; &#125;&#125; 这个主要是为讲自定义数据源作准备。 https://articles.zsxq.com/id_702s32f46zet.html首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** Given a provider name, look up the data source class definition. */ def lookupDataSource(provider: String): Class[_] = &#123; val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) // 指定路径加载，默认加载类名是DefaultSource val provider2 = s"$provider1.DefaultSource" val loader = Utils.getContextOrSparkClassLoader // ServiceLoader加载 val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader) try &#123; serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match &#123; // the provider format did not match any given registered aliases case Nil =&gt; try &#123; Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match &#123; case Success(dataSource) =&gt; // Found the data source using fully qualified path dataSource case Failure(error) =&gt; if (provider1.toLowerCase == "orc" || provider1.startsWith("org.apache.spark.sql.hive.orc")) &#123; throw new AnalysisException( "The ORC data source must be used with Hive support enabled") &#125; else if (provider1.toLowerCase == "avro" || provider1 == "com.databricks.spark.avro") &#123; throw new AnalysisException( s"Failed to find data source: $&#123;provider1.toLowerCase&#125;. Please find an Avro " + "package at http://spark.apache.org/third-party-projects.html") &#125; else &#123; throw new ClassNotFoundException( s"Failed to find data source: $provider1. Please find packages at " + "http://spark.apache.org/third-party-projects.html", error) &#125; &#125; &#125; catch &#123; case e: NoClassDefFoundError =&gt; // This one won't be caught by Scala NonFatal // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"$className was removed in Spark 2.0. " + "Please check if your library is compatible with Spark 2.0", e) &#125; else &#123; throw e &#125; &#125; case head :: Nil =&gt; // there is exactly one registered alias head.getClass case sources =&gt; // There are multiple registered aliases for the input sys.error(s"Multiple sources found for $provider1 " + s"($&#123;sources.map(_.getClass.getName).mkString(", ")&#125;), " + "please specify the fully qualified class name.") &#125; &#125; catch &#123; case e: ServiceConfigurationError if e.getCause.isInstanceOf[NoClassDefFoundError] =&gt; // NoClassDefFoundError's class name uses "/" rather than "." for packages val className = e.getCause.getMessage.replaceAll("/", ".") if (spark2RemovedClasses.contains(className)) &#123; throw new ClassNotFoundException(s"Detected an incompatible DataSourceRegister. " + "Please remove the incompatible library from classpath or upgrade it. " + s"Error: $&#123;e.getMessage&#125;", e) &#125; else &#123; throw e &#125; &#125; &#125; 其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。 主要思路是： 实现DefaultSource。 实现工厂类。 实现具体的数据加载类。 首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：12345678package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.sources.v2.&#123;DataSourceOptions, DataSourceV2, ReadSupport&#125;class DefaultSource extends DataSourceV2 with ReadSupport &#123; def createReader(options: DataSourceOptions) = new SimpleDataSourceReader()&#125; 然后是，要实现DataSourceReader，负责创建阅读器工厂：12345678910111213141516package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReaderFactory, DataSourceReader&#125;import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;class SimpleDataSourceReader extends DataSourceReader &#123; def readSchema() = StructType(Array(StructField("value", StringType))) def createDataReaderFactories = &#123; val factoryList = new java.util.ArrayList[DataReaderFactory[Row]] factoryList.add(new SimpleDataSourceReaderFactory()) factoryList &#125;&#125; 数据源的具体实现类：12345678910111213141516171819202122package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.sql.Rowimport org.apache.spark.sql.sources.v2.reader.&#123;DataReader, DataReaderFactory&#125;class SimpleDataSourceReaderFactory extends DataReaderFactory[Row] with DataReader[Row] &#123; def createDataReader = new SimpleDataSourceReaderFactory() val values = Array("1", "2", "3", "4", "5") var index = 0 def next = index &lt; values.length def get = &#123; val row = Row(values(index)) index = index + 1 row &#125; def close() = Unit&#125; 使用我们默认的数据源：12345678910111213141516171819202122232425262728package bigdata.spark.SparkSQL.DataSourcesimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]") .set("yarn.resourcemanager.hostname", "mt-mdh.local") .set("spark.executor.instances","2") .setJars(List("/opt/sparkjar/bigdata.jar" ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar" ,"/opt/jars/kafka-clients-0.10.2.2.jar" ,"/opt/jars/kafka_2.11-0.10.2.2.jar")) val spark = SparkSession .builder() .config(sparkConf) .getOrCreate() val simpleDf = spark.read .format("bigdata.spark.SparkSQL.DataSources") .load() simpleDf.show() spark.stop() &#125;&#125; format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。]]></content>
      <categories>
        <category>sparksql</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java自定义注解]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解和注释区别注释：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。注解：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。 注解的定义和使用模板注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。 定义格式：修饰符 @interface 注解名{} 使用格式： @注解名;例如：junit测试中的 @Test 复写方法中的@Override 自定义注解注解可以书写的位置定义的注解，可以书写在类的不同位置：类上、成员变量上、成员方法、构造方法等位置。在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置： ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上 ElementType.METHOD:当前的注解可以书写在方法上 ElementType.FIELD:当前的注解可以书写在成员变量（字段）上 ElementType.TYPE:当前的注解可以书写在类或接口上 注解生命周期 由@Retention 声明自己的注解可以存活的时间具体的存活的时间需要通过RetentionPolicy 中提供的值。RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。 注解中的成员变量注解定义的变量格式：数据类型 变量名(); 基本案例自定义注解12345678910111213import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD , ElementType.FIELD&#125;)public @interface MyAnnotation &#123; // 定义注解的变量 String name(); int age(); String sex();&#125; 自定义注解的使用123456789101112public class UseAnnotation &#123; @MyAnnotation(name="zhangsan",age=23,sex="nv") public UseAnnotation()&#123; &#125; @MyAnnotation(name="zhangsan",age=23,sex="nv") private String name; @MyAnnotation(name="zhangsan",age=23,sex="nv") public void show()&#123; &#125;&#125; 经典使用案例（mysql连接驱动）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/* * 自定义注解，封装数据库连接的四个参数 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface MyDriver &#123; String driver(); String url(); String user(); String pwd();&#125;import java.lang.reflect.Method;import java.sql.Connection;import java.sql.DriverManager;/* * 专门负责获取数据库的连接工具类 */public class JDBCUtils &#123; @MyDriver(driver="com.mysql.jdbc.Driver", url="jdbc:mysql://localhost:3306/estore",user="root",pwd="abc") public static Connection getConnection() throws Exception&#123; //获取当前类的class文件 Class clazz = JDBCUtils.class; Method method = clazz.getMethod("getConnection", null); //获取当前方法上的注解信息 if( method.isAnnotationPresent(MyDriver.class) )&#123; MyDriver an = method.getAnnotation(MyDriver.class); String driver = an.driver(); String url = an.url(); String user = an.user(); String pwd = an.pwd(); //获取数据库的连接，加载驱动，获取连接 Class.forName(driver); //获取连接 Connection conn = DriverManager.getConnection(url, user, pwd); return conn; &#125; return null; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; Connection conn = JDBCUtils.getConnection(); System.out.println(conn); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven删除本地仓库lastUpdated文件]]></title>
    <url>%2F2018%2F08%2F13%2Fmaven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[12windows:for /r %i in (*.lastUpdated) do del %i 12linux:find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 查询数据1select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
