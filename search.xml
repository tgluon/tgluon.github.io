<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java自定义注解]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解和注释区别注释：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。注解：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。 注解的定义和使用模板注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。 定义格式：修饰符 @interface 注解名{} 使用格式： @注解名;例如：junit测试中的 @Test 复写方法中的@Override 自定义注解注解可以书写的位置定义的注解，可以书写在类的不同位置：类上、成员变量上、成员方法、构造方法等位置。在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置： ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上 ElementType.METHOD:当前的注解可以书写在方法上 ElementType.FIELD:当前的注解可以书写在成员变量（字段）上 ElementType.TYPE:当前的注解可以书写在类或接口上 注解生命周期 由@Retention 声明自己的注解可以存活的时间具体的存活的时间需要通过RetentionPolicy 中提供的值。RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。 注解中的成员变量注解定义的变量格式：数据类型 变量名(); 基本案例自定义注解12345678910111213import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD , ElementType.FIELD&#125;)public @interface MyAnnotation &#123; // 定义注解的变量 String name(); int age(); String sex();&#125; 自定义注解的使用123456789101112public class UseAnnotation &#123; @MyAnnotation(name="zhangsan",age=23,sex="nv") public UseAnnotation()&#123; &#125; @MyAnnotation(name="zhangsan",age=23,sex="nv") private String name; @MyAnnotation(name="zhangsan",age=23,sex="nv") public void show()&#123; &#125;&#125; 经典使用案例（mysql连接驱动）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/* * 自定义注解，封装数据库连接的四个参数 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface MyDriver &#123; String driver(); String url(); String user(); String pwd();&#125;import java.lang.reflect.Method;import java.sql.Connection;import java.sql.DriverManager;/* * 专门负责获取数据库的连接工具类 */public class JDBCUtils &#123; @MyDriver(driver="com.mysql.jdbc.Driver", url="jdbc:mysql://localhost:3306/estore",user="root",pwd="abc") public static Connection getConnection() throws Exception&#123; //获取当前类的class文件 Class clazz = JDBCUtils.class; Method method = clazz.getMethod("getConnection", null); //获取当前方法上的注解信息 if( method.isAnnotationPresent(MyDriver.class) )&#123; MyDriver an = method.getAnnotation(MyDriver.class); String driver = an.driver(); String url = an.url(); String user = an.user(); String pwd = an.pwd(); //获取数据库的连接，加载驱动，获取连接 Class.forName(driver); //获取连接 Connection conn = DriverManager.getConnection(url, user, pwd); return conn; &#125; return null; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; Connection conn = JDBCUtils.getConnection(); System.out.println(conn); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven删除本地仓库lastUpdated文件]]></title>
    <url>%2F2018%2F08%2F13%2Fmaven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[12windows:for /r %i in (*.lastUpdated) do del %i 12linux:find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。ORCFILE格式的输出包是：1org.apache.hadoop.hive.ql.io.orc 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立ORCFILE格式表12345678create table t_orcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as orcfile; 向表中导入数据注意：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_orcfile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEXTFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FTEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。TEXTFILE格式的输出包是： org.apache.hadoop.mapred.TextfileInputFormatorg.apache.hadoop.mapred.TextfileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立TEXTFILE格式表12345678create table t_textfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as textfile; 装载数据1load data local inpath '/home/hadoop/textfile/a.txt' into/overwrite t_textfile; 查询数据1select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCFILE格式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FRCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，被用于在一个时间点操作多行的场景。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。RCFILE格式的输入输出包是：12org.apache.hadoop.hive.ql.io.RCFileInputFormatorg.apache.hadoop.hive.ql.io.RCFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立RCFILE格式表12345678create table t_rcfile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as rcfile; 向表中导入数据注意：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。1insert overwrite table t_rcfile select * from t_textfile 查询1select * from t_rcfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequencefile方式存储到hive表]]></title>
    <url>%2F2018%2F07%2F28%2FSEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。SEQUENCEFILE格式的输入输入包是：12org.apache.hadoop.mapred.SequenceFileInputFormatorg.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat 注意:本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节 案例建立Sequencefile格式表12345678create table t_sequencefile( c1 string, c2 int, c3 string, c4 string) row format delimited fields terminated by '\t' stored as sequencefile; 向表中导入数据注意：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。1insert overwrite table t_sequencefile select * from t_textfile]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载json文件到hive表]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[struct类型应用准备json文件simple.json1&#123;"foo":"abc","bar":"200901011000000","quux":&#123;"quuxid":1234,"quuxname":"sam"&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456789create database if not exists mytest;use mytest;create table if not exists my_table( foo string, bar string, quux struct&lt;quuxid:int,quuxname:string&gt;) row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/simple.json' into table my_table; 查询1select foo, bar,quux.quuxid,quux.quuname from my_table; sstruct结合array类型应用准备json文件complex.json1&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表12345678910111213use mytest;create table if not exists complex_json( docid string, user struct&lt;id: int, username: string, shippingaddress:struct&lt;address1:string address2: string, city: string, state: string&gt;, orders:array&lt;struct&lt;itemid:int,orderdate:String&gt;&gt;&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 装载数据1load data local inpath '/home/hadoop/data/complex.json' overwrite table complex_json; 查询12select docid,user.id,user.shippingaddress.city as city ,user.orders[0].itemid as order0id,user.orders[1].itemid as order1ib from complex_json; 动态map类型应用json文件a.json12345&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac","result":10000&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789,"proxy":"ac"&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456","media":789&#125;&#125;&#123;"conflict":&#123;"liveid":123,"zhuboid":"456"&#125;&#125;&#123;"conflict":&#123;"liveid":123&#125;&#125; 添加hive-hcatalog-core.jar包1add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar 建立测试表123456use mytest;create table if not exists json_table( conflict map&lt;string,string&gt;)row format serde 'org.apache.hive.hcatlog.data.JsonSerde'stored as textfile; 查询12select * from json_table; select conflict['media'] from json_table;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台监控工具]]></title>
    <url>%2F2018%2F07%2F24%2F%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[大数据平台监控工具Prometheus+grafana环境搭建参考博客： http://blog.51cto.com/youerning/2050543 cerebro作为本地监控，监控elasticsearch https://github.com/lmenezes/cerebro openTSDB+grafana参考 https://blog.csdn.net/u011537073/article/details/54565742 tableau付费 https://www.tableau.com/support/help]]></content>
      <categories>
        <category>大数据平台监控</category>
      </categories>
      <tags>
        <tag>大数据平台监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境变量配置]]></title>
    <url>%2F2018%2F07%2F22%2F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java环境变量配置windows方式一(常用):变量名：JAVA_HOME变量值：D:\Program Files\Java\jdk1.8.0_73变量名：PATH变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin变量名：classpath变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar; %JAVA_HOME%\lib\tools.jar 方法二：path:D:\Program Files\Java\jdk1.8.0_60\binclasspath:D:\Program Files\Java\jdk1.8.0_60\libjavac linuxJAVA_HOME=/home/hadoop/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarPATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH 验证在命令行输入一下命令进行验证：1.java2.javac 查看版本号maven环境变量配置windowsMAVEM_HOMED:\Program Files\mavenpath%MAVEN_HOME%\bin注意：%MAVEN_HOME%\bin 应该放在path的最前面。 linux12MAVEN_HOME=/home/hadoop/maven PATH=$PATH:$MAVEN_HOME/bin]]></content>
      <categories>
        <category>环境变量配置</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
