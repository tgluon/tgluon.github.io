<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml" />






<meta name="description" content="火星度假村追梦程序员。">
<meta property="og:type" content="website">
<meta property="og:title" content="blog">
<meta property="og:url" content="https://tgluon.github.io/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="火星度假村追梦程序员。">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="blog">
<meta name="twitter:description" content="火星度假村追梦程序员。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://tgluon.github.io/"/>





  <title>blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/tgluon"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_orange_ff7600.png" alt="Fork me on GitHub"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">追梦青年</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/04/17/antlr4学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/antlr4学习笔记/" itemprop="url">antlr4学习笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-17T18:46:07+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/antlr/" itemprop="url" rel="index">
                    <span itemprop="name">antlr</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/17/antlr4学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/17/antlr4学习笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  174
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="下载antlr"><a href="#下载antlr" class="headerlink" title="下载antlr"></a>下载antlr</h1><p><a href="https://www.antlr.org/download/antlr-4.7.2-complete.jar" target="_blank" rel="noopener">https://www.antlr.org/download/antlr-4.7.2-complete.jar</a>   </p>
<h1 id="启动antlr"><a href="#启动antlr" class="headerlink" title="启动antlr"></a>启动antlr</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alias antlr4='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.Tool'</span><br><span class="line"></span><br><span class="line">alias grun='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.gui.TestRig'</span><br><span class="line"></span><br><span class="line">antlr4</span><br></pre></td></tr></table></figure>
<h1 id="测试案例"><a href="#测试案例" class="headerlink" title="测试案例"></a>测试案例</h1><h2 id="定义antlr-Hello-g4文件"><a href="#定义antlr-Hello-g4文件" class="headerlink" title="定义antlr Hello.g4文件"></a>定义antlr Hello.g4文件</h2><p><strong>注意：</strong> 逗号必须对齐<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">grammar Hello; //定义一个名为Hello的语法</span><br><span class="line">r  : &apos;hello&apos; ID ; // 匹配一个关键字hello和一个紧随其后的标识符</span><br><span class="line">ID : [a-z]+ ;    // 匹配小写字母组成的标识符</span><br><span class="line">WS : [ \t\r\n]+ -&gt; skip; // 忽略空格 Tab 换行</span><br></pre></td></tr></table></figure></p>
<h2 id="运行生成对应的文件和代码"><a href="#运行生成对应的文件和代码" class="headerlink" title="运行生成对应的文件和代码"></a>运行生成对应的文件和代码</h2><p>antlr4 Hello.g4<br>javac Hello*.java</p>
<h2 id="运行生成树图"><a href="#运行生成树图" class="headerlink" title="运行生成树图"></a>运行生成树图</h2><p>grun Hello r -gui<br><img src="https://github.com/tgluon/tgluon.github.io/blob/master/images/myimages/数据在语言程序中的流动过程.png" alt=""></p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/04/15/spark sql窗口函数实战/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/15/spark sql窗口函数实战/" itemprop="url">spark sql窗口函数实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-15T16:48:42+08:00">
                2019-04-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sparksql/" itemprop="url" rel="index">
                    <span itemprop="name">sparksql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/15/spark sql窗口函数实战/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/15/spark sql窗口函数实战/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3,501
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  19
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p>主要以一些官方文档为参考。<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a>   </p>
<p><a href="https://help.aliyun.com/document_detail/34994.html?spm=a2c4g.11174283.6.650.6f02590e0d209m#h2-url-1" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/34994.html?spm=a2c4g.11174283.6.650.6f02590e0d209m#h2-url-1</a></p>
<p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a> </p>
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html" target="_blank" rel="noopener">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html</a>   </p>
<p><a href="http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html" target="_blank" rel="noopener">http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html</a>    </p>
<p><a href="http://cdn2.hubspot.net/hubfs/438089/notebooks/eBook/Introducing_Window_Functions_in_Spark_SQL_Notebook.html" target="_blank" rel="noopener">http://cdn2.hubspot.net/hubfs/438089/notebooks/eBook/Introducing_Window_Functions_in_Spark_SQL_Notebook.html</a>    </p>
<p><a href="http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/" target="_blank" rel="noopener">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>
<p><a href="https://www.cnblogs.com/piaolingzxh/p/5538783.html" target="_blank" rel="noopener">https://www.cnblogs.com/piaolingzxh/p/5538783.html</a></p>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> <span class="keyword">extends</span> <span class="title">BaseSparkSession</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">      .set(<span class="string">"spark.master"</span>, <span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-01"</span>, <span class="number">500</span>),</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-02"</span>, <span class="number">450</span>),</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-03"</span>, <span class="number">550</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-01"</span>, <span class="number">250</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-02"</span>, <span class="number">290</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-03"</span>, <span class="number">270</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="平均移动值"><a href="#平均移动值" class="headerlink" title="平均移动值"></a>平均移动值</h1><h2 id="DataFrame-API方式实现"><a href="#DataFrame-API方式实现" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)，每一个滑动的窗口总用有3行</span></span><br><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"> df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  avg($<span class="string">"user_cnt"</span>).over(movinAvgSpec).as(<span class="string">"moving_avg_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现"><a href="#sql方式实现" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h1 id="lag函数"><a href="#lag函数" class="headerlink" title="lag函数"></a>lag函数</h1><p>说明：取当前记录的前x条数据的指定列，如果没有返回null，有就返回真实值。</p>
<h2 id="DataFrame-API方式实现-1"><a href="#DataFrame-API方式实现-1" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"prevUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(lagwSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  lag($<span class="string">"user_cnt"</span>).over(movinAvgSpec).as(<span class="string">"lag_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-1"><a href="#sql方式实现-1" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lag(user_cnt,1) over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h1 id="lead函数"><a href="#lead函数" class="headerlink" title="lead函数"></a>lead函数</h1><p>说明：取当前记录的后x条数据的指定列，如果没有返回null，有就返回真实值。 </p>
<h2 id="DataFrame-API方式实现-2"><a href="#DataFrame-API方式实现-2" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> leadwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"lead_user_cnt"</span>, lead(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(leadwSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> leadwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">  df.select(</span><br><span class="line">    $<span class="string">"site"</span>,</span><br><span class="line">    $<span class="string">"date"</span>,</span><br><span class="line">    $<span class="string">"user_cnt"</span>,</span><br><span class="line">    lead($<span class="string">"user_cnt"</span>, <span class="number">1</span>).over(leadwSpec).as(<span class="string">"lead_user_cnt"</span>)</span><br><span class="line">  ).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-2"><a href="#sql方式实现-2" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lead(user_cnt,1) over(partition by site order by date asc ) as lead_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------------+</span><br><span class="line">|site|      date|user_cnt|lead_user_cnt|</span><br><span class="line">+----+----------+--------+-------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|          <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|          <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="literal">null</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|          <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|          <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="literal">null</span>|</span><br><span class="line">+----+----------+--------+-------------+</span><br></pre></td></tr></table></figure>
<h1 id="FIRST-VALUE函数"><a href="#FIRST-VALUE函数" class="headerlink" title="FIRST_VALUE函数"></a>FIRST_VALUE函数</h1><p>说明：该函数用于获取分组排序后最第一条记录的字段值。 </p>
<h2 id="DataFrame-API方式实现-3"><a href="#DataFrame-API方式实现-3" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> firstValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"first_value_user_cnt"</span>, first(<span class="string">"user_cnt"</span>).over(firstValuewSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> firstValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  first($<span class="string">"user_cnt"</span>).over(firstValuewSpec).as(<span class="string">"first_value_user_cnt"</span>)).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-3"><a href="#sql方式实现-3" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       first_value(user_cnt) over(partition by site order by date asc ) as first_value_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+--------------------+</span><br><span class="line">|site|      date|user_cnt|first_value_user_cnt|</span><br><span class="line">+----+----------+--------+--------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                 <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                 <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                 <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                 <span class="number">500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                 <span class="number">500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                 <span class="number">500</span>|</span><br><span class="line">+----+----------+--------+--------------------+</span><br></pre></td></tr></table></figure>
<h1 id="LAST-VALUE函数"><a href="#LAST-VALUE函数" class="headerlink" title="LAST_VALUE函数"></a>LAST_VALUE函数</h1><p>说明：该函数用于获取分组排序后最后一条记录的字段值。 </p>
<h2 id="DataFrame-API方式实现-4"><a href="#DataFrame-API方式实现-4" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"last_value_user_cnt"</span>, last(<span class="string">"user_cnt"</span>).over(lastValuewSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  last($<span class="string">"user_cnt"</span>).over(lastValuewSpec).as(<span class="string">"last_value_user_cnt"</span>)).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-4"><a href="#sql方式实现-4" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       last_value(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following ) as last_value_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------------------+</span><br><span class="line">|site|      date|user_cnt|last_value_user_cnt|</span><br><span class="line">+----+----------+--------+-------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                <span class="number">270</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+-------------------+</span><br></pre></td></tr></table></figure>
<h1 id="COUNT"><a href="#COUNT" class="headerlink" title="COUNT"></a>COUNT</h1><p>说明：该函数用于计算计数值。</p>
<h2 id="不指定order-by"><a href="#不指定order-by" class="headerlink" title="不指定order by"></a>不指定order by</h2><p>###<br><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"count"</span>, count(<span class="string">"user_cnt"</span>).over(counWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  count($<span class="string">"user_cnt"</span>).over(counWSpec).as(<span class="string">"count"</span>)).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-5"><a href="#sql方式实现-5" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">     <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">       | select  site,</span></span><br><span class="line"><span class="string">       |         date,</span></span><br><span class="line"><span class="string">       |         user_cnt,</span></span><br><span class="line"><span class="string">       |         count(user_cnt) over(partition by site) as count</span></span><br><span class="line"><span class="string">       |from     site_info</span></span><br><span class="line"><span class="string">     "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|count|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">3</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">3</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure>
<h2 id="指定order-by"><a href="#指定order-by" class="headerlink" title="指定order by"></a>指定order by</h2><p>指定order by时，返回当前窗口内从开始行到当前行的累计计数值。  </p>
<h2 id="DataFrame-API方式实现-5"><a href="#DataFrame-API方式实现-5" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span>.asc)</span><br><span class="line">df.withColumn(<span class="string">"count"</span>, count(<span class="string">"user_cnt"</span>).over(counWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  count($<span class="string">"user_cnt"</span>).over(counWSpec).as(<span class="string">"count"</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-6"><a href="#sql方式实现-6" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    | select  site,</span></span><br><span class="line"><span class="string">    |         date,</span></span><br><span class="line"><span class="string">    |         user_cnt,</span></span><br><span class="line"><span class="string">    |         count(user_cnt) over(partition by site order by date) as count</span></span><br><span class="line"><span class="string">    |from     site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-4"><a href="#结果-4" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|count|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure>
<h1 id="sum函数"><a href="#sum函数" class="headerlink" title="sum函数"></a>sum函数</h1><p>说明：该函数用于计算汇总值。</p>
<h2 id="不指定order-by-1"><a href="#不指定order-by-1" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-6"><a href="#DataFrame-API方式实现-6" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"sum_user_cnt"</span>, sum(<span class="string">"user_cnt"</span>).over(sumWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  sum($<span class="string">"user_cnt"</span>).over(sumWSpec).as(<span class="string">"sum_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-7"><a href="#sql方式实现-7" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       sum(user_cnt) over(partition by site ) as sum_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-5"><a href="#结果-5" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|sum_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">810</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|        <span class="number">1500</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h2 id="指定order-by-1"><a href="#指定order-by-1" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-7"><a href="#DataFrame-API方式实现-7" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"sum_user_cnt"</span>, sum(<span class="string">"user_cnt"</span>).over(sumWSpec))</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  sum($<span class="string">"user_cnt"</span>).over(sumWSpec).as(<span class="string">"sum_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-8"><a href="#sql方式实现-8" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       sum(user_cnt) over(partition by site order by date asc  rows between unbounded preceding and unbounded following ) as sum_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-6"><a href="#结果-6" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|sum_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">810</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|        <span class="number">1500</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h1 id="min函数"><a href="#min函数" class="headerlink" title="min函数"></a>min函数</h1><h2 id="不指定order-by-2"><a href="#不指定order-by-2" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-8"><a href="#DataFrame-API方式实现-8" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, min(<span class="string">"user_cnt"</span>).over(minWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  min($<span class="string">"user_cnt"</span>).over(minWSpec)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-9"><a href="#sql方式实现-9" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        min(user_cnt) over(partition by site) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-7"><a href="#结果-7" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|site|      date|user_cnt|min(user_cnt) <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> site unspecifiedframe$())|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                                                       <span class="number">450</span>|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h2 id="指定order-by-2"><a href="#指定order-by-2" class="headerlink" title="指定order by"></a>指定order by</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, min(<span class="string">"user_cnt"</span>).over(minWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  min($<span class="string">"user_cnt"</span>).over(minWSpec)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-10"><a href="#sql方式实现-10" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        min(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-8"><a href="#结果-8" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|site|      date|user_cnt|min(user_cnt) <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> site unspecifiedframe$())|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                                                       <span class="number">450</span>|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h1 id="max函数"><a href="#max函数" class="headerlink" title="max函数"></a>max函数</h1><h2 id="不指定order-by-3"><a href="#不指定order-by-3" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-9"><a href="#DataFrame-API方式实现-9" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, max(<span class="string">"user_cnt"</span>).over(maxWSpec))</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  max($<span class="string">"user_cnt"</span>).over(maxWSpec).as(<span class="string">"max_user_cnt"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-11"><a href="#sql方式实现-11" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">   <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">     |select  site,</span></span><br><span class="line"><span class="string">     |        date,</span></span><br><span class="line"><span class="string">     |        user_cnt,</span></span><br><span class="line"><span class="string">     |        max(user_cnt) over(partition by site ) as min_user_cnt</span></span><br><span class="line"><span class="string">     |from    site_info</span></span><br><span class="line"><span class="string">   "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-9"><a href="#结果-9" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|min_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">290</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h2 id="指定order-by-3"><a href="#指定order-by-3" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-10"><a href="#DataFrame-API方式实现-10" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, max(<span class="string">"user_cnt"</span>).over(maxWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  max($<span class="string">"user_cnt"</span>).over(maxWSpec).as(<span class="string">"max_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-12"><a href="#sql方式实现-12" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        max(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-10"><a href="#结果-10" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|min_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">290</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h1 id="avg函数"><a href="#avg函数" class="headerlink" title="avg函数"></a>avg函数</h1><p>说明：该函数用于计算平均值。</p>
<h2 id="不指定order-by-4"><a href="#不指定order-by-4" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-11"><a href="#DataFrame-API方式实现-11" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line"> df.withColumn(<span class="string">"avg_user_cnt"</span>, avg(<span class="string">"user_cnt"</span>).over(avgWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  avg(<span class="string">"user_cnt"</span>).over(avgWSpec).as(<span class="string">"avg_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-13"><a href="#sql方式实现-13" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        avg(user_cnt) over(partition by site ) as avg_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-11"><a href="#结果-11" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|avg_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|       <span class="number">270.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|       <span class="number">500.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h2 id="指定order-by-4"><a href="#指定order-by-4" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-12"><a href="#DataFrame-API方式实现-12" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"avg_user_cnt"</span>, avg(<span class="string">"user_cnt"</span>).over(avgWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  avg(<span class="string">"user_cnt"</span>).over(avgWSpec).as(<span class="string">"avg_user_cnt"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式实现-14"><a href="#sql方式实现-14" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        avg(user_cnt) over(partition by site order by date  asc rows between unbounded preceding and unbounded following ) as avg_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h3 id="结果-12"><a href="#结果-12" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|avg_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|       <span class="number">270.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|       <span class="number">500.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h1 id="rank函数"><a href="#rank函数" class="headerlink" title="rank函数"></a>rank函数</h1><p>说明：该函数用于计算排名。</p>
<h2 id="DataFrame-API方式实现-13"><a href="#DataFrame-API方式实现-13" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'user_cnt</span>.desc)</span><br><span class="line">df.withColumn(<span class="string">"rank"</span>, rank().over(rankWSpec))</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'user_cnt</span>.desc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  rank().over(rankWSpec).as(<span class="string">"rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-15"><a href="#sql方式实现-15" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        rank() over(partition by site order by user_cnt desc) as rank_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-13"><a href="#结果-13" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="row-number-over-函数"><a href="#row-number-over-函数" class="headerlink" title="row_number over 函数"></a>row_number over 函数</h1><h2 id="DataFrame-API方式实现-14"><a href="#DataFrame-API方式实现-14" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowNUmberWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> desc, <span class="symbol">'user_cnt</span> desc)</span><br><span class="line">df.withColumn(<span class="string">"row_num"</span>, row_number().over(rowNUmberWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowNUmberWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> desc, <span class="symbol">'user_cnt</span> desc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  row_number().over(rowNUmberWSpec).as(<span class="string">"row_num"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-16"><a href="#sql方式实现-16" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        row_number() over(partition by site order by date desc , user_cnt desc ) as row_num</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure>
<h2 id="结果-14"><a href="#结果-14" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------+</span><br><span class="line">|site|      date|user_cnt|row_num|</span><br><span class="line">+----+----------+--------+-------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|      <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|      <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|      <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|      <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|      <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|      <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-------+</span><br></pre></td></tr></table></figure>
<h1 id="dense-rank函数"><a href="#dense-rank函数" class="headerlink" title="dense_rank函数"></a>dense_rank函数</h1><p>说明：该函数用于计算连续排名。</p>
<h2 id="DataFrame-API方式实现-15"><a href="#DataFrame-API方式实现-15" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> denseRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"dense_rank"</span>, dense_rank() over (denseRankWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> denseRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  dense_rank().over(denseRankWSpec).as(<span class="string">"dense_rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-17"><a href="#sql方式实现-17" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  spark.sql(</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">      |select site,</span></span><br><span class="line"><span class="string">      |       date,</span></span><br><span class="line"><span class="string">      |       user_cnt,</span></span><br><span class="line"><span class="string">      |       dense_rank() over(partition by site order by date asc ) as dense_rank</span></span><br><span class="line"><span class="string">      |from   site_info</span></span><br><span class="line"><span class="string">      |</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-15"><a href="#结果-15" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------+</span><br><span class="line">|site|      date|user_cnt|dense_rank|</span><br><span class="line">+----+----------+--------+----------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+----------+</span><br></pre></td></tr></table></figure>
<h1 id="percent-rank函数"><a href="#percent-rank函数" class="headerlink" title="percent_rank函数"></a>percent_rank函数</h1><p>说明：该函数用于计算一组数据中某行的相对排名。</p>
<h2 id="DataFrame-API方式实现-16"><a href="#DataFrame-API方式实现-16" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> percentRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"percent_rank"</span>, percent_rank() over (percentRankWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> percentRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  percent_rank().over(percentRankWSpec).as(<span class="string">"percent_rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-18"><a href="#sql方式实现-18" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  spark.sql(</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">      |select site,</span></span><br><span class="line"><span class="string">      |       date,</span></span><br><span class="line"><span class="string">      |       user_cnt,</span></span><br><span class="line"><span class="string">      |       percent_rank() over(partition by site order by date asc ) as percent_rank</span></span><br><span class="line"><span class="string">      |from   site_info</span></span><br><span class="line"><span class="string">      |</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-16"><a href="#结果-16" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|site|      date|user_cnt|percent_rank|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">0.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">0.5</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">1.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">0.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">0.5</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">1.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure>
<h1 id="ntile函数"><a href="#ntile函数" class="headerlink" title="ntile函数"></a>ntile函数</h1><p>说明：用于将分组数据按照顺序切分成n片，并返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。</p>
<h2 id="DataFrame-API方式实现-17"><a href="#DataFrame-API方式实现-17" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ntileRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"ntile"</span>, ntile(<span class="number">2</span>).over(ntileRankWSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ntileRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  ntile(<span class="number">2</span>).over(ntileRankWSpec).as(<span class="string">"ntile"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<h2 id="sql方式实现-19"><a href="#sql方式实现-19" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        ntile(2) over(partition by site order by date) as ntile</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="结果-17"><a href="#结果-17" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|ntile|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">2</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure>
<h1 id="致谢！"><a href="#致谢！" class="headerlink" title="致谢！"></a>致谢！</h1><p>本人能力有限，博客错误难免，有错往将错误发送到邮箱(<a href="mailto:t_spider@aliyun.com" target="_blank" rel="noopener">t_spider@aliyun.com</a>)</p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/04/14/idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/14/idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)/" itemprop="url">idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T23:11:18+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/14/idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/14/idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  24,964
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  154
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>idea 中连接yarn集群debug spark代码,有利于定位线上问题。</p>
<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>默认你hadoop环境已经搞定情况下的说明！！！<br><strong>scala版本：</strong> 2.11.8<br><strong>jdk版本</strong>：JDK1.8<br><strong>hadoop版本：</strong> 2.7.3<br><strong>spark版本：</strong> 2.4.1<br><strong>zookeeper：</strong> 3.4.6<br><strong>高能预警：</strong> 一定要保持yarn集群机器scala版本和项目pom文件中scala版本一致。     </p>
<table>
<thead>
<tr>
<th>linux版本</th>
<th>IP</th>
<th>hostname</th>
<th>进程 </th>
</tr>
</thead>
<tbody>
<tr>
<td>centos7</td>
<td>192.168.8.81</td>
<td>hadoop01</td>
<td>NameNode、DFSZKFailoverController</td>
</tr>
<tr>
<td>centos7</td>
<td>192.168.8.82</td>
<td>hadoop02</td>
<td>NameNode、DFSZKFailoverController、ResourceManager、JournalNode、NodeManager、DataNode、QuorumPeerMain</td>
</tr>
<tr>
<td>centos7</td>
<td>192.168.8.83</td>
<td>hadoop03</td>
<td>JournalNode、NodeManager、DataNode、QuorumPeerMain</td>
</tr>
<tr>
<td>centos7</td>
<td>192.168.8.84</td>
<td>hadoop04</td>
<td>JournalNode、NodeManager、DataNode、QuorumPeerMain</td>
</tr>
</tbody>
</table>
<p><strong>idea所在机器：</strong>   </p>
<table>
<thead>
<tr>
<th>linux版本</th>
<th>IP</th>
<th>hostname</th>
</tr>
</thead>
<tbody>
<tr>
<td>ubuntu18.04</td>
<td>192.168.8.85</td>
<td>hadoop05</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="maven项目中resources目录中配置文件"><a href="#maven项目中resources目录中配置文件" class="headerlink" title="maven项目中resources目录中配置文件"></a>maven项目中resources目录中配置文件</h1><p><strong>高能预警：</strong> 配置文件需要和hadoop集群一致</p>
<p><strong>具体配置文件如下：</strong><br>core-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop临时目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>hdfs-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--修改block块的大小,默认为128M--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.block.seze<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>yarn-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.embedded<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zk集群地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.client.failover-proxy-provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 两个可选值：org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore 以及 默认值org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.8.82:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打开日志聚合功能，这样才能从web界面查看日志 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 聚合日志最长保留时间 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 中间结果存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/data/localdir1,/home/hadoop/hadoop/data/localdir2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 日志存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/data/hdfs/logdir1,/home/hadoop/hadoop/data/hdfs/logdir2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>spark-defaults.conf配置文件：</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the "License"); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line">spark.driver.extraClassPath         /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*</span><br><span class="line"></span><br><span class="line">spark.executor.extraClassPath     /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*</span><br><span class="line">spark.driver.extraJavaOptions     -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005</span><br><span class="line">spark.executor.extraJavaOptions   -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005</span><br><span class="line">spark.driver.memory                512m</span><br><span class="line">spark.executor.memory              512m</span><br><span class="line"># Example:</span><br><span class="line"># spark.master                     spark://master:7077</span><br><span class="line"># spark.eventLog.enabled           true</span><br><span class="line"># spark.eventLog.dir               hdfs://namenode:8021/directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</span><br></pre></td></tr></table></figure></p>
<h1 id="maven-pom配置文件"><a href="#maven-pom配置文件" class="headerlink" title="maven pom配置文件"></a>maven pom配置文件</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.xh.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>sparklearning<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mysql.version</span>&gt;</span>5.1.35<span class="tag">&lt;/<span class="name">mysql.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava.version</span>&gt;</span>26.0-jre<span class="tag">&lt;/<span class="name">guava.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fastjson.version</span>&gt;</span>1.2.40<span class="tag">&lt;/<span class="name">fastjson.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">zookeeper.version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">zookeeper.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--scala--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;zookeeper.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--spark--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-yarn_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;fastjson.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--hive--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--hadoop--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--mysql--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--guava--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin compiles Scala files --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-compile-first<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>process-resources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>add-source<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-test-compile<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>process-test-resources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin compiles Java files --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--跳过test begin--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">skipTests</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skipTests</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin adds all dependencies to JAR file during 'package' command.</span></span><br><span class="line"><span class="comment">            Pay EXTRA attention to the 'mainClass' tag.</span></span><br><span class="line"><span class="comment">            You have to set name of class with entry point to program ('main' method) --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifestFile</span>&gt;</span><span class="tag">&lt;/<span class="name">manifestFile</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.defonds.RsaEncryptor<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>META-INF/spring.handlers<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>META-INF/spring.schemas<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="开发程序"><a href="#开发程序" class="headerlink" title="开发程序"></a>开发程序</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xh.spark.sql.function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">          .set(<span class="string">"spark.master"</span>, <span class="string">"yarn"</span>)</span><br><span class="line">          .set(<span class="string">"spark.submit.deployMode"</span>, <span class="string">"client"</span>) <span class="comment">// 部署模式为client</span></span><br><span class="line">          .set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"hadoop02"</span>) <span class="comment">// resourcemanager主机名</span></span><br><span class="line">          .set(<span class="string">"spark.executor.instances"</span>, <span class="string">"2"</span>) <span class="comment">// Executor实例的数量</span></span><br><span class="line">          .set(<span class="string">"spark.dynamicAllocation.enabled"</span>, <span class="string">"false"</span>)</span><br><span class="line">          .setJars(<span class="type">List</span>(<span class="string">"/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar"</span>,</span><br><span class="line">            <span class="string">"/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar"</span></span><br><span class="line">          ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-01"</span>, <span class="number">50</span>),</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-02"</span>, <span class="number">45</span>),</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-03"</span>, <span class="number">55</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-01"</span>, <span class="number">25</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-02"</span>, <span class="number">29</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-03"</span>, <span class="number">27</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 简单移动平均值</span></span><br><span class="line">    <span class="comment">// API方式</span></span><br><span class="line">    <span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)	，每一个滑动的窗口总用有3行</span></span><br><span class="line">    <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sql方式</span></span><br><span class="line">     df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">        spark.sql(</span><br><span class="line">          <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">            |select site,</span></span><br><span class="line"><span class="string">            |       date,</span></span><br><span class="line"><span class="string">            |       user_cnt,</span></span><br><span class="line"><span class="string">            |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">            |from   site_info</span></span><br><span class="line"><span class="string">          "</span><span class="string">""</span>.stripMargin).show()</span><br><span class="line">    </span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上确认无误之后，直接在idea里面run该程序。   </p>
<h1 id="执行结果如下"><a href="#执行结果如下" class="headerlink" title="执行结果如下"></a>执行结果如下</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/jvm/java-1.8.0-openjdk/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:45315,suspend=y,server=n -javaagent:/home/hadoop/idea/lib/rt/debugger-agent.jar -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-1.8.0-openjdk/jre/lib/charsets.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jce.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jsse.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/management-agent.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/resources.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/rt.jar:/home/hadoop/worker/sparklearning/target/classes:/home/hadoop/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar:/home/hadoop/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar:/home/hadoop/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/hadoop/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/hadoop/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar:/home/hadoop/repository/org/apache/spark/spark-yarn_2.11/2.4.1/spark-yarn_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/hadoop/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/hadoop/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/hadoop/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/hadoop/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.5/hadoop-yarn-server-web-proxy-2.6.5.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/hadoop/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/hadoop/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-core_2.11/2.4.1/spark-core_2.11-2.4.1.jar:/home/hadoop/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/hadoop/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/hadoop/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/hadoop/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/hadoop/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/hadoop/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/hadoop/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/hadoop/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/hadoop/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/hadoop/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/hadoop/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/hadoop/repository/org/apache/spark/spark-launcher_2.11/2.4.1/spark-launcher_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-kvstore_2.11/2.4.1/spark-kvstore_2.11-2.4.1.jar:/home/hadoop/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/hadoop/repository/org/apache/spark/spark-network-common_2.11/2.4.1/spark-network-common_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-network-shuffle_2.11/2.4.1/spark-network-shuffle_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-unsafe_2.11/2.4.1/spark-unsafe_2.11-2.4.1.jar:/home/hadoop/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/hadoop/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/hadoop/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/hadoop/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/hadoop/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/hadoop/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/hadoop/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/hadoop/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/hadoop/repository/org/xerial/snappy/snappy-java/1.1.7.1/snappy-java-1.1.7.1.jar:/home/hadoop/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/hadoop/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/hadoop/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/hadoop/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/hadoop/repository/org/scala-lang/modules/scala-xml_2.11/1.0.6/scala-xml_2.11-1.0.6.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/hadoop/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/hadoop/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/hadoop/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/hadoop/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/hadoop/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/hadoop/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/hadoop/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/hadoop/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/hadoop/repository/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/hadoop/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/hadoop/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/hadoop/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/hadoop/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/hadoop/repository/org/apache/spark/spark-tags_2.11/2.4.1/spark-tags_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-sql_2.11/2.4.1/spark-sql_2.11-2.4.1.jar:/home/hadoop/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/hadoop/repository/org/apache/spark/spark-sketch_2.11/2.4.1/spark-sketch_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-catalyst_2.11/2.4.1/spark-catalyst_2.11-2.4.1.jar:/home/hadoop/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/hadoop/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/hadoop/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/hadoop/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/hadoop/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/hadoop/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/hadoop/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/hadoop/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/hadoop/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/hadoop/repository/org/apache/spark/spark-mllib_2.11/2.4.1/spark-mllib_2.11-2.4.1.jar:/home/hadoop/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/hadoop/repository/org/apache/spark/spark-graphx_2.11/2.4.1/spark-graphx_2.11-2.4.1.jar:/home/hadoop/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/hadoop/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/hadoop/repository/org/apache/spark/spark-mllib-local_2.11/2.4.1/spark-mllib-local_2.11-2.4.1.jar:/home/hadoop/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/hadoop/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/hadoop/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/hadoop/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/hadoop/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/hadoop/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/hadoop/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/hadoop/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/hadoop/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-hive_2.11/2.4.1/spark-hive_2.11-2.4.1.jar:/home/hadoop/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/hadoop/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/hadoop/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/hadoop/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/hadoop/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/hadoop/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/hadoop/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/hadoop/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/hadoop/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/hadoop/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/hadoop/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/hadoop/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/hadoop/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/hadoop/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/hadoop/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/hadoop/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/hadoop/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/hadoop/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/hadoop/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/hadoop/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/hadoop/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/hadoop/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/hadoop/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming_2.11/2.4.1/spark-streaming_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.4.1/spark-streaming-kafka-0-10_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/hadoop/repository/com/alibaba/fastjson/1.2.40/fastjson-1.2.40.jar:/home/hadoop/repository/org/apache/hive/hive-jdbc/2.3.3/hive-jdbc-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-common/2.3.3/hive-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-storage-api/2.4.0/hive-storage-api-2.4.0.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.3.3/orc-core-1.3.3.jar:/home/hadoop/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/home/hadoop/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/home/hadoop/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/home/hadoop/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/home/hadoop/repository/asm/asm/3.1/asm-3.1.jar:/home/hadoop/repository/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-web/2.6.2/log4j-web-2.6.2.jar:/home/hadoop/repository/com/tdunning/json/1.8/json-1.8.jar:/home/hadoop/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/hadoop/repository/org/apache/hive/hive-service/2.3.3/hive-service-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-server/2.3.3/hive-llap-server-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3.jar:/home/hadoop/repository/org/apache/slider/slider-core/0.90.2-incubating/slider-core-0.90.2-incubating.jar:/home/hadoop/repository/com/beust/jcommander/1.30/jcommander-1.30.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-registry/2.7.1/hadoop-yarn-registry-2.7.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/hadoop/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop2-compat/1.1.1/hbase-hadoop2-compat-1.1.1.jar:/home/hadoop/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/hadoop/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-server/1.1.1/hbase-server-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-procedure/1.1.1/hbase-procedure-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-prefix-tree/1.1.1/hbase-prefix-tree-1.1.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop-compat/1.1.1/hbase-hadoop-compat-1.1.1.jar:/home/hadoop/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/hadoop/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/repository/javax/servlet/jsp-api/2.0/jsp-api-2.0.jar:/home/hadoop/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/hadoop/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/repository/javax/servlet/servlet-api/2.4/servlet-api-2.4.jar:/home/hadoop/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/repository/org/apache/hive/hive-serde/2.3.3/hive-serde-2.3.3.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop-bundle/1.8.1/parquet-hadoop-bundle-1.8.1.jar:/home/hadoop/repository/org/apache/hive/hive-metastore/2.3.3/hive-metastore-2.3.3.jar:/home/hadoop/repository/org/apache/hbase/hbase-client/1.1.1/hbase-client-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-annotations/1.1.1/hbase-annotations-1.1.1.jar:/usr/lib/jvm/java-1.8.0-openjdk/lib/tools.jar:/home/hadoop/repository/org/apache/hbase/hbase-protocol/1.1.1/hbase-protocol-1.1.1.jar:/home/hadoop/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/hadoop/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/hadoop/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/repository/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/home/hadoop/repository/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/home/hadoop/repository/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/home/hadoop/repository/co/cask/tephra/tephra-api/0.6.0/tephra-api-0.6.0.jar:/home/hadoop/repository/co/cask/tephra/tephra-core/0.6.0/tephra-core-0.6.0.jar:/home/hadoop/repository/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/hadoop/repository/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/hadoop/repository/org/apache/twill/twill-common/0.6.0-incubating/twill-common-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-core/0.6.0-incubating/twill-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-api/0.6.0-incubating/twill-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-api/0.6.0-incubating/twill-discovery-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-core/0.6.0-incubating/twill-discovery-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-zookeeper/0.6.0-incubating/twill-zookeeper-0.6.0-incubating.jar:/home/hadoop/repository/co/cask/tephra/tephra-hbase-compat-1.0/0.6.0/tephra-hbase-compat-1.0-0.6.0.jar:/home/hadoop/repository/org/apache/hive/hive-shims/2.3.3/hive-shims-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-common/2.3.3/hive-shims-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-0.23/2.3.3/hive-shims-0.23-2.3.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.7.2/hadoop-yarn-server-resourcemanager-2.7.2.jar:/home/hadoop/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.7.2/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6-tests.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-scheduler/2.3.3/hive-shims-scheduler-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-service-rpc/2.3.3/hive-service-rpc-2.3.3.jar:/home/hadoop/repository/org/apache/httpcomponents/httpcore/4.4/httpcore-4.4.jar:/home/hadoop/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/hadoop/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/hadoop/repository/org/apache/hive/hive-exec/2.3.3/hive-exec-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-vector-code-gen/2.3.3/hive-vector-code-gen-2.3.3.jar:/home/hadoop/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/hadoop/repository/org/apache/hive/hive-llap-tez/2.3.3/hive-llap-tez-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-client/2.3.3/hive-llap-client-2.3.3.jar:/home/hadoop/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-1.2-api/2.6.2/log4j-1.2-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar:/home/hadoop/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/hadoop/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/hadoop/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/hadoop/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/hadoop/repository/org/apache/commons/commons-compress/1.9/commons-compress-1.9.jar:/home/hadoop/repository/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar:/home/hadoop/repository/org/apache/calcite/calcite-druid/1.10.0/calcite-druid-1.10.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica/1.8.0/avatica-1.8.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica-metrics/1.8.0/avatica-metrics-1.8.0.jar:/home/hadoop/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/hadoop/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-client/2.7.3/hadoop-client-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-common/2.7.3/hadoop-common-2.7.3.jar:/home/hadoop/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/hadoop/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/hadoop/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-auth/2.7.3/hadoop-auth-2.7.3.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/hadoop/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/hadoop/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-hdfs/2.7.3/hadoop-hdfs-2.7.3.jar:/home/hadoop/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/hadoop/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.3/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.3/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.3/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.3/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.3/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-annotations/2.7.3/hadoop-annotations-2.7.3.jar:/home/hadoop/repository/mysql/mysql-connector-java/5.1.35/mysql-connector-java-5.1.35.jar:/home/hadoop/repository/com/google/guava/guava/26.0-jre/guava-26.0-jre.jar:/home/hadoop/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/home/hadoop/repository/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar:/home/hadoop/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/hadoop/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/hadoop/scala/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-reflect.jar:/home/hadoop/scala/lib/scala-actors-migration_2.11-1.1.0.jar:/home/hadoop/scala/lib/scala-xml_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-library.jar:/home/hadoop/scala/lib/scala-swing_2.11-1.0.2.jar:/home/hadoop/scala/lib/scala-actors-2.11.0.jar:/home/hadoop/idea/lib/idea_rt.jar com.xh.spark.sql.function.WindowFunctionTest</span><br><span class="line">Connected to the target VM, address: '127.0.0.1:45315', transport: 'socket'</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">19/04/12 16:47:02 INFO SparkContext: Running Spark version 2.4.1</span><br><span class="line">19/04/12 16:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/04/12 16:47:04 INFO SparkContext: Submitted application: WindowFunctionTest</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing view acls to: hadoop</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing modify acls to: hadoop</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">19/04/12 16:47:07 INFO Utils: Successfully started service 'sparkDriver' on port 43568.</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/04/12 16:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07913749-f493-4bae-95aa-f9ea019dde51</span><br><span class="line">19/04/12 16:47:07 INFO MemoryStore: MemoryStore started with capacity 447.3 MB</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/04/12 16:47:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.</span><br><span class="line">19/04/12 16:47:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop04:4040</span><br><span class="line">19/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT.jar with timestamp 1555058828687</span><br><span class="line">19/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1555058828703</span><br><span class="line">19/04/12 16:47:13 INFO Client: Requesting a new application from cluster with 3 NodeManagers</span><br><span class="line">19/04/12 16:47:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">19/04/12 16:47:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead</span><br><span class="line">19/04/12 16:47:13 INFO Client: Setting up container launch context for our AM</span><br><span class="line">19/04/12 16:47:13 INFO Client: Setting up the launch environment for our AM container</span><br><span class="line">19/04/12 16:47:14 INFO Client: Preparing resources for our AM container</span><br><span class="line">19/04/12 16:47:14 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">19/04/12 16:47:23 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_libs__8176900855339303501.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_libs__8176900855339303501.zip</span><br><span class="line">19/04/12 16:47:50 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_conf__4747135480716905311.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_conf__.zip</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing view acls to: hadoop</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing modify acls to: hadoop</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">19/04/12 16:47:55 INFO Client: Submitting application application_1555049463744_0005 to ResourceManager</span><br><span class="line">19/04/12 16:47:55 INFO YarnClientImpl: Submitted application application_1555049463744_0005</span><br><span class="line">19/04/12 16:47:55 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1555049463744_0005 and attemptId None</span><br><span class="line">19/04/12 16:47:57 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:47:57 INFO Client: </span><br><span class="line">	 client token: N/A</span><br><span class="line">	 diagnostics: N/A</span><br><span class="line">	 ApplicationMaster host: N/A</span><br><span class="line">	 ApplicationMaster RPC port: -1</span><br><span class="line">	 queue: root.hadoop</span><br><span class="line">	 start time: 1555058875812</span><br><span class="line">	 final status: UNDEFINED</span><br><span class="line">	 tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/</span><br><span class="line">	 user: hadoop</span><br><span class="line">19/04/12 16:47:58 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:47:59 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:00 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:01 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:02 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:03 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:04 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:05 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:06 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:07 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:08 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:09 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:10 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:11 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:12 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:13 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:14 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:15 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:16 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:17 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:18 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:19 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:20 INFO Client: Application report for application_1555049463744_0005 (state: RUNNING)</span><br><span class="line">19/04/12 16:48:20 INFO Client: </span><br><span class="line">	 client token: N/A</span><br><span class="line">	 diagnostics: N/A</span><br><span class="line">	 ApplicationMaster host: 192.168.8.82</span><br><span class="line">	 ApplicationMaster RPC port: -1</span><br><span class="line">	 queue: root.hadoop</span><br><span class="line">	 start time: 1555058875812</span><br><span class="line">	 final status: UNDEFINED</span><br><span class="line">	 tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/</span><br><span class="line">	 user: hadoop</span><br><span class="line">19/04/12 16:48:20 INFO YarnClientSchedulerBackend: Application application_1555049463744_0005 has started running.</span><br><span class="line">19/04/12 16:48:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42715.</span><br><span class="line">19/04/12 16:48:21 INFO NettyBlockTransferService: Server created on hadoop04:42715</span><br><span class="line">19/04/12 16:48:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:42715 with 447.3 MB RAM, BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:23 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)</span><br><span class="line">19/04/12 16:48:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; 192.168.8.82, PROXY_URI_BASES -&gt; http://192.168.8.82:8089/proxy/application_1555049463744_0005, RM_HA_URLS -&gt; hadoop02:8088,hadoop03:8088), /proxy/application_1555049463744_0005</span><br><span class="line">19/04/12 16:48:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill, /metrics/json.</span><br><span class="line">19/04/12 16:48:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)</span><br><span class="line">19/04/12 16:48:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/worker/sparklearning/spark-warehouse').</span><br><span class="line">19/04/12 16:48:32 INFO SharedState: Warehouse path is 'file:/home/hadoop/worker/sparklearning/spark-warehouse'.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.</span><br><span class="line">19/04/12 16:48:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">19/04/12 16:48:55 INFO CodeGenerator: Code generated in 2751.822246 ms</span><br><span class="line">19/04/12 16:52:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.83:53042) with ID 1</span><br><span class="line">19/04/12 16:52:30 INFO BlockManagerMasterEndpoint: Registering block manager hadoop03:45104 with 366.3 MB RAM, BlockManagerId(1, hadoop03, 45104, None)</span><br><span class="line">19/04/12 16:52:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.84:37146) with ID 2</span><br><span class="line">19/04/12 16:52:39 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:38267 with 366.3 MB RAM, BlockManagerId(2, hadoop04, 38267, None)</span><br><span class="line">19/04/12 16:53:38 INFO CodeGenerator: Code generated in 668.087005 ms</span><br><span class="line">19/04/12 16:53:45 INFO CodeGenerator: Code generated in 732.99189 ms</span><br><span class="line">19/04/12 16:53:47 INFO CodeGenerator: Code generated in 1265.744172 ms</span><br><span class="line">19/04/12 16:53:47 INFO CodeGenerator: Code generated in 271.771791 ms</span><br><span class="line">19/04/12 16:53:51 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Registering RDD 2 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Got job 0 (show at WindowFunctionTest.scala:45) with 1 output partitions</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Final stage: ResultStage 1 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:53:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.1 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:42715 (size: 3.1 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:53:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0, 1))</span><br><span class="line">19/04/12 16:53:55 INFO YarnScheduler: Adding task set 0.0 with 2 tasks</span><br><span class="line">19/04/12 16:53:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 8230 bytes)</span><br><span class="line">19/04/12 16:53:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 8230 bytes)</span><br><span class="line">19/04/12 16:55:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop03:45104 (size: 3.1 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:55:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:38267 (size: 3.1 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 137230 ms on hadoop03 (executor 1) (1/2)</span><br><span class="line">19/04/12 16:56:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 147247 ms on hadoop04 (executor 2) (2/2)</span><br><span class="line">19/04/12 16:56:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: ShuffleMapStage 0 (show at WindowFunctionTest.scala:45) finished in 151.743 s</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: looking for newly runnable stages</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: running: Set()</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: waiting: Set(ResultStage 1)</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: failed: Set()</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.4 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0))</span><br><span class="line">19/04/12 16:56:25 INFO YarnScheduler: Adding task set 1.0 with 1 tasks</span><br><span class="line">19/04/12 16:56:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.83:53042</span><br><span class="line">19/04/12 16:56:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2345 ms on hadoop03 (executor 1) (1/1)</span><br><span class="line">19/04/12 16:56:27 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:27 INFO DAGScheduler: ResultStage 1 (show at WindowFunctionTest.scala:45) finished in 3.127 s</span><br><span class="line">19/04/12 16:56:27 INFO DAGScheduler: Job 0 finished: show at WindowFunctionTest.scala:45, took 156.360095 s</span><br><span class="line">19/04/12 16:56:28 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Got job 1 (show at WindowFunctionTest.scala:45) with 4 output partitions</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Final stage: ResultStage 3 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(1, 2, 3, 4))</span><br><span class="line">19/04/12 16:56:28 INFO YarnScheduler: Adding task set 3.0 with 4 tasks</span><br><span class="line">19/04/12 16:56:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:28 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, hadoop03, executor 1, partition 2, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5, hadoop03, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 447 ms on hadoop03 (executor 1) (1/4)</span><br><span class="line">19/04/12 16:56:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6, hadoop03, executor 1, partition 4, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 290 ms on hadoop03 (executor 1) (2/4)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 334 ms on hadoop03 (executor 1) (3/4)</span><br><span class="line">19/04/12 16:56:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.84:37146</span><br><span class="line">19/04/12 16:56:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3241 ms on hadoop04 (executor 2) (4/4)</span><br><span class="line">19/04/12 16:56:31 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:31 INFO DAGScheduler: ResultStage 3 (show at WindowFunctionTest.scala:45) finished in 3.803 s</span><br><span class="line">19/04/12 16:56:31 INFO DAGScheduler: Job 1 finished: show at WindowFunctionTest.scala:45, took 3.848347 s</span><br><span class="line">19/04/12 16:56:32 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Got job 2 (show at WindowFunctionTest.scala:45) with 20 output partitions</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Final stage: ResultStage 5 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))</span><br><span class="line">19/04/12 16:56:32 INFO YarnScheduler: Adding task set 5.0 with 20 tasks</span><br><span class="line">19/04/12 16:56:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7, hadoop03, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:32 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, hadoop04, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 9, hadoop04, executor 2, partition 7, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 726 ms on hadoop04 (executor 2) (1/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 10, hadoop03, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 811 ms on hadoop03 (executor 1) (2/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 11, hadoop03, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 10) in 393 ms on hadoop03 (executor 1) (3/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 12, hadoop04, executor 2, partition 10, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 9) in 523 ms on hadoop04 (executor 2) (4/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 13, hadoop03, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 11) in 317 ms on hadoop03 (executor 1) (5/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 14, hadoop03, executor 1, partition 12, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 15, hadoop03, executor 1, partition 13, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 13) in 851 ms on hadoop03 (executor 1) (6/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 14) in 541 ms on hadoop03 (executor 1) (7/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 16, hadoop04, executor 2, partition 14, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 12) in 1219 ms on hadoop04 (executor 2) (8/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 17, hadoop04, executor 2, partition 15, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 18, hadoop03, executor 1, partition 16, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 16) in 313 ms on hadoop04 (executor 2) (9/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 15) in 596 ms on hadoop03 (executor 1) (10/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 19, hadoop04, executor 2, partition 17, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 20, hadoop03, executor 1, partition 18, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 18) in 158 ms on hadoop03 (executor 1) (11/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 17) in 167 ms on hadoop04 (executor 2) (12/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 21, hadoop03, executor 1, partition 19, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 22, hadoop04, executor 2, partition 20, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 20) in 250 ms on hadoop03 (executor 1) (13/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 19) in 343 ms on hadoop04 (executor 2) (14/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 23, hadoop04, executor 2, partition 21, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 22) in 247 ms on hadoop04 (executor 2) (15/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 24, hadoop03, executor 1, partition 22, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 21) in 481 ms on hadoop03 (executor 1) (16/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 25, hadoop04, executor 2, partition 23, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 23) in 393 ms on hadoop04 (executor 2) (17/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 26, hadoop04, executor 2, partition 24, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 25) in 473 ms on hadoop04 (executor 2) (18/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 24) in 898 ms on hadoop03 (executor 1) (19/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 26) in 524 ms on hadoop04 (executor 2) (20/20)</span><br><span class="line">19/04/12 16:56:36 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:36 INFO DAGScheduler: ResultStage 5 (show at WindowFunctionTest.scala:45) finished in 4.493 s</span><br><span class="line">19/04/12 16:56:36 INFO DAGScheduler: Job 2 finished: show at WindowFunctionTest.scala:45, took 4.566402 s</span><br><span class="line">19/04/12 16:56:37 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Got job 3 (show at WindowFunctionTest.scala:45) with 100 output partitions</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Final stage: ResultStage 7 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Submitting 100 missing tasks from ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39))</span><br><span class="line">19/04/12 16:56:37 INFO YarnScheduler: Adding task set 7.0 with 100 tasks</span><br><span class="line">19/04/12 16:56:37 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 27, hadoop04, executor 2, partition 110, NODE_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:37 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 28, hadoop03, executor 1, partition 102, NODE_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 29, hadoop04, executor 2, partition 25, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 27) in 1607 ms on hadoop04 (executor 2) (1/100)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 30, hadoop04, executor 2, partition 26, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 31, hadoop04, executor 2, partition 27, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 29) in 1083 ms on hadoop04 (executor 2) (2/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 30) in 659 ms on hadoop04 (executor 2) (3/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 32, hadoop04, executor 2, partition 28, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 31) in 473 ms on hadoop04 (executor 2) (4/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 33, hadoop04, executor 2, partition 29, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 34, hadoop03, executor 1, partition 30, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 28) in 3578 ms on hadoop03 (executor 1) (5/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 32) in 505 ms on hadoop04 (executor 2) (6/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 35, hadoop04, executor 2, partition 31, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 33) in 438 ms on hadoop04 (executor 2) (7/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 36, hadoop03, executor 1, partition 32, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 37, hadoop04, executor 2, partition 33, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 35) in 353 ms on hadoop04 (executor 2) (8/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 34) in 583 ms on hadoop03 (executor 1) (9/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 38, hadoop03, executor 1, partition 34, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 36) in 301 ms on hadoop03 (executor 1) (10/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 39, hadoop03, executor 1, partition 35, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 40, hadoop04, executor 2, partition 36, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 38) in 453 ms on hadoop03 (executor 1) (11/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 41, hadoop04, executor 2, partition 37, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 37) in 1017 ms on hadoop04 (executor 2) (12/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 42, hadoop03, executor 1, partition 38, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 39) in 407 ms on hadoop03 (executor 1) (13/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 40) in 486 ms on hadoop04 (executor 2) (14/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 43, hadoop04, executor 2, partition 39, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 41) in 259 ms on hadoop04 (executor 2) (15/100)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 44, hadoop04, executor 2, partition 40, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 43) in 472 ms on hadoop04 (executor 2) (16/100)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 45, hadoop03, executor 1, partition 41, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 42) in 964 ms on hadoop03 (executor 1) (17/100)</span><br><span class="line">19/04/12 16:56:45 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 46, hadoop03, executor 1, partition 42, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 47, hadoop04, executor 2, partition 43, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 44) in 2989 ms on hadoop04 (executor 2) (18/100)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 45) in 3210 ms on hadoop03 (executor 1) (19/100)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 48, hadoop03, executor 1, partition 44, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 46) in 1507 ms on hadoop03 (executor 1) (20/100)</span><br><span class="line">19/04/12 16:56:47 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 49, hadoop04, executor 2, partition 45, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:47 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 47) in 1533 ms on hadoop04 (executor 2) (21/100)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 50, hadoop04, executor 2, partition 46, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 49) in 715 ms on hadoop04 (executor 2) (22/100)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 51, hadoop03, executor 1, partition 47, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 48) in 2132 ms on hadoop03 (executor 1) (23/100)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 52, hadoop04, executor 2, partition 48, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 50) in 1332 ms on hadoop04 (executor 2) (24/100)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 53, hadoop03, executor 1, partition 49, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 51) in 776 ms on hadoop03 (executor 1) (25/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 54, hadoop04, executor 2, partition 50, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 52) in 1066 ms on hadoop04 (executor 2) (26/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 55, hadoop03, executor 1, partition 51, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 53) in 698 ms on hadoop03 (executor 1) (27/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 56, hadoop04, executor 2, partition 52, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 54) in 679 ms on hadoop04 (executor 2) (28/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 57, hadoop03, executor 1, partition 53, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 55) in 1082 ms on hadoop03 (executor 1) (29/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 58, hadoop04, executor 2, partition 54, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 56) in 731 ms on hadoop04 (executor 2) (30/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 59, hadoop03, executor 1, partition 55, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 57) in 592 ms on hadoop03 (executor 1) (31/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 60, hadoop04, executor 2, partition 56, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 58) in 557 ms on hadoop04 (executor 2) (32/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 61, hadoop03, executor 1, partition 57, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 59) in 468 ms on hadoop03 (executor 1) (33/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 62, hadoop04, executor 2, partition 58, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 60) in 241 ms on hadoop04 (executor 2) (34/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 63, hadoop04, executor 2, partition 59, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 62) in 728 ms on hadoop04 (executor 2) (35/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 64, hadoop03, executor 1, partition 60, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 61) in 804 ms on hadoop03 (executor 1) (36/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 65, hadoop04, executor 2, partition 61, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 63) in 392 ms on hadoop04 (executor 2) (37/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 66, hadoop03, executor 1, partition 62, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 64) in 320 ms on hadoop03 (executor 1) (38/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 67, hadoop04, executor 2, partition 63, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 65) in 154 ms on hadoop04 (executor 2) (39/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 68, hadoop03, executor 1, partition 64, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 66) in 297 ms on hadoop03 (executor 1) (40/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 69, hadoop03, executor 1, partition 65, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 68) in 463 ms on hadoop03 (executor 1) (41/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 70, hadoop04, executor 2, partition 66, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 67) in 672 ms on hadoop04 (executor 2) (42/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 71, hadoop03, executor 1, partition 67, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 69) in 165 ms on hadoop03 (executor 1) (43/100)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 72, hadoop03, executor 1, partition 68, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 71) in 1656 ms on hadoop03 (executor 1) (44/100)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 73, hadoop03, executor 1, partition 69, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 72) in 534 ms on hadoop03 (executor 1) (45/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 74, hadoop03, executor 1, partition 70, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 73) in 376 ms on hadoop03 (executor 1) (46/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 75, hadoop04, executor 2, partition 71, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 70) in 2844 ms on hadoop04 (executor 2) (47/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 76, hadoop03, executor 1, partition 72, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 74) in 444 ms on hadoop03 (executor 1) (48/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 77, hadoop03, executor 1, partition 73, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 76) in 341 ms on hadoop03 (executor 1) (49/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 78, hadoop03, executor 1, partition 74, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 77) in 169 ms on hadoop03 (executor 1) (50/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 79, hadoop03, executor 1, partition 75, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 78) in 193 ms on hadoop03 (executor 1) (51/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 80, hadoop04, executor 2, partition 76, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 75) in 1174 ms on hadoop04 (executor 2) (52/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 81, hadoop03, executor 1, partition 77, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 82, hadoop04, executor 2, partition 78, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 79) in 337 ms on hadoop03 (executor 1) (53/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 80) in 310 ms on hadoop04 (executor 2) (54/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 83, hadoop03, executor 1, partition 79, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 81) in 761 ms on hadoop03 (executor 1) (55/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 84, hadoop04, executor 2, partition 80, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 82) in 696 ms on hadoop04 (executor 2) (56/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 85, hadoop03, executor 1, partition 81, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 83) in 691 ms on hadoop03 (executor 1) (57/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 86, hadoop03, executor 1, partition 82, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 85) in 118 ms on hadoop03 (executor 1) (58/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 87, hadoop04, executor 2, partition 83, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 84) in 447 ms on hadoop04 (executor 2) (59/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 88, hadoop03, executor 1, partition 84, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 86) in 240 ms on hadoop03 (executor 1) (60/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 89, hadoop04, executor 2, partition 85, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 87) in 325 ms on hadoop04 (executor 2) (61/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 90, hadoop03, executor 1, partition 86, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 88) in 255 ms on hadoop03 (executor 1) (62/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 91, hadoop04, executor 2, partition 87, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 89) in 147 ms on hadoop04 (executor 2) (63/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 92, hadoop03, executor 1, partition 88, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 90) in 130 ms on hadoop03 (executor 1) (64/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 93, hadoop03, executor 1, partition 89, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 92) in 110 ms on hadoop03 (executor 1) (65/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 94, hadoop04, executor 2, partition 90, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 91) in 264 ms on hadoop04 (executor 2) (66/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 95, hadoop03, executor 1, partition 91, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 93) in 165 ms on hadoop03 (executor 1) (67/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 96, hadoop04, executor 2, partition 92, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 94) in 267 ms on hadoop04 (executor 2) (68/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 97, hadoop03, executor 1, partition 93, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 95) in 222 ms on hadoop03 (executor 1) (69/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 98, hadoop04, executor 2, partition 94, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 96) in 191 ms on hadoop04 (executor 2) (70/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 99, hadoop03, executor 1, partition 95, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 97) in 173 ms on hadoop03 (executor 1) (71/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 100, hadoop04, executor 2, partition 96, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 98) in 165 ms on hadoop04 (executor 2) (72/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 101, hadoop03, executor 1, partition 97, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 99) in 181 ms on hadoop03 (executor 1) (73/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 102, hadoop04, executor 2, partition 98, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 100) in 176 ms on hadoop04 (executor 2) (74/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 103, hadoop04, executor 2, partition 99, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 102) in 103 ms on hadoop04 (executor 2) (75/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 104, hadoop03, executor 1, partition 100, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 101) in 202 ms on hadoop03 (executor 1) (76/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 105, hadoop04, executor 2, partition 101, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 103) in 159 ms on hadoop04 (executor 2) (77/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 106, hadoop04, executor 2, partition 103, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 105) in 378 ms on hadoop04 (executor 2) (78/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 107, hadoop03, executor 1, partition 104, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 104) in 551 ms on hadoop03 (executor 1) (79/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 108, hadoop04, executor 2, partition 105, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 106) in 437 ms on hadoop04 (executor 2) (80/100)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 109, hadoop03, executor 1, partition 106, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 107) in 621 ms on hadoop03 (executor 1) (81/100)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 110, hadoop04, executor 2, partition 107, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 108) in 1254 ms on hadoop04 (executor 2) (82/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 111, hadoop03, executor 1, partition 108, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 109) in 1316 ms on hadoop03 (executor 1) (83/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 112, hadoop04, executor 2, partition 109, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 110) in 793 ms on hadoop04 (executor 2) (84/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 113, hadoop03, executor 1, partition 111, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 111) in 780 ms on hadoop03 (executor 1) (85/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 114, hadoop04, executor 2, partition 112, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 112) in 567 ms on hadoop04 (executor 2) (86/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 115, hadoop03, executor 1, partition 113, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 113) in 689 ms on hadoop03 (executor 1) (87/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 116, hadoop04, executor 2, partition 114, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 114) in 513 ms on hadoop04 (executor 2) (88/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 117, hadoop03, executor 1, partition 115, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 115) in 432 ms on hadoop03 (executor 1) (89/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 118, hadoop04, executor 2, partition 116, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 116) in 271 ms on hadoop04 (executor 2) (90/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 119, hadoop03, executor 1, partition 117, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 120, hadoop04, executor 2, partition 118, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 118) in 129 ms on hadoop04 (executor 2) (91/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 117) in 173 ms on hadoop03 (executor 1) (92/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 121, hadoop04, executor 2, partition 119, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 120) in 139 ms on hadoop04 (executor 2) (93/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 122, hadoop03, executor 1, partition 120, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 119) in 232 ms on hadoop03 (executor 1) (94/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 123, hadoop04, executor 2, partition 121, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 121) in 110 ms on hadoop04 (executor 2) (95/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 124, hadoop03, executor 1, partition 122, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 122) in 115 ms on hadoop03 (executor 1) (96/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 125, hadoop04, executor 2, partition 123, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 123) in 136 ms on hadoop04 (executor 2) (97/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 126, hadoop03, executor 1, partition 124, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 124) in 124 ms on hadoop03 (executor 1) (98/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 125) in 140 ms on hadoop04 (executor 2) (99/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 126) in 145 ms on hadoop03 (executor 1) (100/100)</span><br><span class="line">19/04/12 16:57:04 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:57:04 INFO DAGScheduler: ResultStage 7 (show at WindowFunctionTest.scala:45) finished in 26.819 s</span><br><span class="line">19/04/12 16:57:04 INFO DAGScheduler: Job 3 finished: show at WindowFunctionTest.scala:45, took 27.209729 s</span><br><span class="line">19/04/12 16:57:05 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Got job 4 (show at WindowFunctionTest.scala:45) with 75 output partitions</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Final stage: ResultStage 9 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:57:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Submitting 75 missing tasks from ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139))</span><br><span class="line">19/04/12 16:57:05 INFO YarnScheduler: Adding task set 9.0 with 75 tasks</span><br><span class="line">19/04/12 16:57:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 127, hadoop04, executor 2, partition 125, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:05 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 128, hadoop03, executor 1, partition 126, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 129, hadoop04, executor 2, partition 127, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 127) in 398 ms on hadoop04 (executor 2) (1/75)</span><br><span class="line">19/04/12 16:57:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 130, hadoop04, executor 2, partition 128, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 129) in 259 ms on hadoop04 (executor 2) (2/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 131, hadoop03, executor 1, partition 129, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 128) in 796 ms on hadoop03 (executor 1) (3/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 132, hadoop04, executor 2, partition 130, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 130) in 277 ms on hadoop04 (executor 2) (4/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 133, hadoop03, executor 1, partition 131, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 131) in 188 ms on hadoop03 (executor 1) (5/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 134, hadoop03, executor 1, partition 132, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 133) in 173 ms on hadoop03 (executor 1) (6/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 135, hadoop04, executor 2, partition 133, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 132) in 289 ms on hadoop04 (executor 2) (7/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 136, hadoop03, executor 1, partition 134, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 134) in 206 ms on hadoop03 (executor 1) (8/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 137, hadoop04, executor 2, partition 135, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 135) in 187 ms on hadoop04 (executor 2) (9/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 138, hadoop03, executor 1, partition 136, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 139, hadoop04, executor 2, partition 137, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 136) in 218 ms on hadoop03 (executor 1) (10/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 140, hadoop03, executor 1, partition 138, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 138) in 102 ms on hadoop03 (executor 1) (11/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 137) in 232 ms on hadoop04 (executor 2) (12/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 141, hadoop03, executor 1, partition 139, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 140) in 96 ms on hadoop03 (executor 1) (13/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 142, hadoop04, executor 2, partition 140, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 139) in 200 ms on hadoop04 (executor 2) (14/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 143, hadoop03, executor 1, partition 141, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 141) in 156 ms on hadoop03 (executor 1) (15/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 144, hadoop03, executor 1, partition 142, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 143) in 234 ms on hadoop03 (executor 1) (16/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 145, hadoop04, executor 2, partition 143, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 142) in 453 ms on hadoop04 (executor 2) (17/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 146, hadoop03, executor 1, partition 144, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 144) in 273 ms on hadoop03 (executor 1) (18/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 147, hadoop04, executor 2, partition 145, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 145) in 220 ms on hadoop04 (executor 2) (19/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 148, hadoop03, executor 1, partition 146, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 146) in 178 ms on hadoop03 (executor 1) (20/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 149, hadoop04, executor 2, partition 147, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 147) in 313 ms on hadoop04 (executor 2) (21/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 150, hadoop03, executor 1, partition 148, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 148) in 279 ms on hadoop03 (executor 1) (22/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 151, hadoop04, executor 2, partition 149, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 149) in 371 ms on hadoop04 (executor 2) (23/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 152, hadoop03, executor 1, partition 150, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 150) in 291 ms on hadoop03 (executor 1) (24/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 153, hadoop04, executor 2, partition 151, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 151) in 321 ms on hadoop04 (executor 2) (25/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 154, hadoop03, executor 1, partition 152, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 152) in 385 ms on hadoop03 (executor 1) (26/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 155, hadoop03, executor 1, partition 153, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 156, hadoop04, executor 2, partition 154, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 153) in 386 ms on hadoop04 (executor 2) (27/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 154) in 332 ms on hadoop03 (executor 1) (28/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 157, hadoop03, executor 1, partition 155, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 155) in 199 ms on hadoop03 (executor 1) (29/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 158, hadoop04, executor 2, partition 156, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 156) in 303 ms on hadoop04 (executor 2) (30/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 159, hadoop04, executor 2, partition 157, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 158) in 94 ms on hadoop04 (executor 2) (31/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 160, hadoop03, executor 1, partition 158, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 157) in 276 ms on hadoop03 (executor 1) (32/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 161, hadoop04, executor 2, partition 159, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 159) in 171 ms on hadoop04 (executor 2) (33/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 162, hadoop03, executor 1, partition 160, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 160) in 182 ms on hadoop03 (executor 1) (34/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 163, hadoop03, executor 1, partition 161, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 164, hadoop04, executor 2, partition 162, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 165, hadoop04, executor 2, partition 163, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 164) in 52 ms on hadoop04 (executor 2) (35/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 161) in 191 ms on hadoop04 (executor 2) (36/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 162) in 206 ms on hadoop03 (executor 1) (37/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 166, hadoop03, executor 1, partition 164, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 167, hadoop04, executor 2, partition 165, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 165) in 108 ms on hadoop04 (executor 2) (38/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 168, hadoop04, executor 2, partition 166, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 163) in 218 ms on hadoop03 (executor 1) (39/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 167) in 88 ms on hadoop04 (executor 2) (40/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 169, hadoop03, executor 1, partition 167, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 166) in 112 ms on hadoop03 (executor 1) (41/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 170, hadoop03, executor 1, partition 168, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 169) in 75 ms on hadoop03 (executor 1) (42/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 171, hadoop04, executor 2, partition 169, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 168) in 162 ms on hadoop04 (executor 2) (43/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 172, hadoop03, executor 1, partition 170, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 173, hadoop04, executor 2, partition 171, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 170) in 96 ms on hadoop03 (executor 1) (44/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 174, hadoop03, executor 1, partition 172, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 171) in 75 ms on hadoop04 (executor 2) (45/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 172) in 49 ms on hadoop03 (executor 1) (46/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 175, hadoop03, executor 1, partition 173, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 174) in 85 ms on hadoop03 (executor 1) (47/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 176, hadoop04, executor 2, partition 174, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 177, hadoop03, executor 1, partition 175, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 178, hadoop03, executor 1, partition 176, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 177) in 38 ms on hadoop03 (executor 1) (48/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 173) in 141 ms on hadoop04 (executor 2) (49/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 179, hadoop03, executor 1, partition 177, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 175) in 123 ms on hadoop03 (executor 1) (50/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 178) in 57 ms on hadoop03 (executor 1) (51/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 180, hadoop04, executor 2, partition 178, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 176) in 114 ms on hadoop04 (executor 2) (52/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 181, hadoop03, executor 1, partition 179, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 182, hadoop03, executor 1, partition 180, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 183, hadoop04, executor 2, partition 181, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 179) in 159 ms on hadoop03 (executor 1) (53/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 184, hadoop03, executor 1, partition 182, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 181) in 110 ms on hadoop03 (executor 1) (54/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 185, hadoop04, executor 2, partition 183, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 180) in 165 ms on hadoop04 (executor 2) (55/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 182) in 262 ms on hadoop03 (executor 1) (56/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 186, hadoop03, executor 1, partition 184, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 187, hadoop04, executor 2, partition 185, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 183) in 331 ms on hadoop04 (executor 2) (57/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 184) in 301 ms on hadoop03 (executor 1) (58/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 185) in 289 ms on hadoop04 (executor 2) (59/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 188, hadoop03, executor 1, partition 186, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 186) in 151 ms on hadoop03 (executor 1) (60/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 189, hadoop04, executor 2, partition 187, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 187) in 197 ms on hadoop04 (executor 2) (61/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 190, hadoop03, executor 1, partition 188, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 188) in 114 ms on hadoop03 (executor 1) (62/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 191, hadoop04, executor 2, partition 189, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 189) in 148 ms on hadoop04 (executor 2) (63/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 192, hadoop03, executor 1, partition 190, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 190) in 113 ms on hadoop03 (executor 1) (64/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 193, hadoop03, executor 1, partition 191, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 194, hadoop04, executor 2, partition 192, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 191) in 129 ms on hadoop04 (executor 2) (65/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 192) in 105 ms on hadoop03 (executor 1) (66/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 195, hadoop03, executor 1, partition 193, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 193) in 126 ms on hadoop03 (executor 1) (67/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 196, hadoop04, executor 2, partition 194, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 194) in 131 ms on hadoop04 (executor 2) (68/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 197, hadoop04, executor 2, partition 195, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 196) in 217 ms on hadoop04 (executor 2) (69/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 198, hadoop03, executor 1, partition 196, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 195) in 301 ms on hadoop03 (executor 1) (70/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 199, hadoop04, executor 2, partition 197, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 197) in 270 ms on hadoop04 (executor 2) (71/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 200, hadoop03, executor 1, partition 198, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 198) in 256 ms on hadoop03 (executor 1) (72/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 201, hadoop04, executor 2, partition 199, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 199) in 282 ms on hadoop04 (executor 2) (73/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 200) in 214 ms on hadoop03 (executor 1) (74/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 201) in 104 ms on hadoop04 (executor 2) (75/75)</span><br><span class="line">19/04/12 16:57:11 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:57:11 INFO DAGScheduler: ResultStage 9 (show at WindowFunctionTest.scala:45) finished in 6.626 s</span><br><span class="line">19/04/12 16:57:11 INFO DAGScheduler: Job 4 finished: show at WindowFunctionTest.scala:45, took 6.700226 s</span><br><span class="line">19/04/12 16:57:14 INFO SparkUI: Stopped Spark web UI at http://hadoop04:4040</span><br><span class="line">19/04/12 16:57:14 INFO YarnClientSchedulerBackend: Interrupting monitor thread</span><br><span class="line">19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Shutting down all executors</span><br><span class="line">19/04/12 16:57:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down</span><br><span class="line">19/04/12 16:57:15 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices</span><br><span class="line">(serviceOption=None,</span><br><span class="line"> services=List(),</span><br><span class="line"> started=false)</span><br><span class="line">19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Stopped</span><br><span class="line">19/04/12 16:57:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/04/12 16:57:17 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/04/12 16:57:17 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/04/12 16:57:17 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/04/12 16:57:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/04/12 16:57:18 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/04/12 16:57:19 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/04/12 16:57:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line">| site|      date|user_cnt|MovingAvg|</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line">|站点1|2018-01-01|      50|     47.5|</span><br><span class="line">|站点1|2018-01-02|      45|     50.0|</span><br><span class="line">|站点1|2018-01-03|      55|     50.0|</span><br><span class="line">|站点2|2018-01-01|      25|     27.0|</span><br><span class="line">|站点2|2018-01-02|      29|     27.0|</span><br><span class="line">|站点2|2018-01-03|      27|     28.0|</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line"></span><br><span class="line">Disconnected from the target VM, address: '127.0.0.1:45315', transport: 'socket'</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>至此,IDEA已经能连接上yarn集群。</p>
<h1 id="开启远程yarn-client-debug调试"><a href="#开启远程yarn-client-debug调试" class="headerlink" title="开启远程yarn client debug调试"></a>开启远程yarn client debug调试</h1><h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><p>将我们的应用程序打包(在hadoop05机器)，发送到hadoop01机器，并在hadoop01上启动我们的应用程序,具体如下：  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/hadoop/worker/sparklearning/target/   </span><br><span class="line"></span><br><span class="line">scp -r sparklearning-1.0-SNAPSHOT.jar  hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar   </span><br><span class="line"></span><br><span class="line">scp -r sparklearning-1.0-SNAPSHOT.jar  hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<p><strong>在hadoop01启动</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class com.xh.spark.sql.function.WindowFunctionTest \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 512m \</span><br><span class="line">    --executor-memory 512m \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">	--driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005" \</span><br><span class="line">    /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></p>
<h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><p>在idea中Edit Configurations ——&gt; 点击+号 ——&gt;Remote —— Configuration ——&gt; Debugger Mode (用Attach to remote JVM)——&gt;host(hadoop01)<br>——&gt; Port(默认5005，也可以选择使用未使用的端口，比如5656) ——&gt; Command line arguments for remote JVN (-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005)</p>
<p>然后点击小虫子图标就可以run程序了！！！！</p>
<h1 id="问题解决分享"><a href="#问题解决分享" class="headerlink" title="问题解决分享"></a>问题解决分享</h1><h2 id="Could-not-parse-Master-URL"><a href="#Could-not-parse-Master-URL" class="headerlink" title="Could not parse Master URL"></a>Could not parse Master URL</h2><p><strong>解决：</strong> 在pom.xml中添加如下依赖<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-yarn_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Name-node-is-in-safe-mode"><a href="#Name-node-is-in-safe-mode" class="headerlink" title="Name node is in safe mode"></a>Name node is in safe mode</h2><p><strong>解决：</strong>  在hadoop所在linux环境输入如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure></p>
<h2 id="Exception-in-thread-“main”-org-apache-hadoop-security-AccessControlException"><a href="#Exception-in-thread-“main”-org-apache-hadoop-security-AccessControlException" class="headerlink" title="Exception in thread “main” org.apache.hadoop.security.AccessControlException:"></a>Exception in thread “main” org.apache.hadoop.security.AccessControlException:</h2><blockquote>
<p>Exception in thread “main” org.apache.hadoop.security.AccessControlException: Permission denied: user=deeplearning, access=WRITE, inode=”/user/deeplearning/.sparkStaging/application_1554947367832_0002”:hadoop:supergroup:drwxr-xr-x</p>
</blockquote>
<p><strong>解决：</strong>  在hdfs-site.xml添加如下配置<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Diagnostics-Container-pid-3293-containerID-container-e12-1555047553207-0002-02-000001-is-running-beyond-virtual-memory-limits-Current-usage-116-1-MB-of-1-GB-physical-memory-used-2-3-GB-of-2-1-GB-virtual-memory-used-Killing-container"><a href="#Diagnostics-Container-pid-3293-containerID-container-e12-1555047553207-0002-02-000001-is-running-beyond-virtual-memory-limits-Current-usage-116-1-MB-of-1-GB-physical-memory-used-2-3-GB-of-2-1-GB-virtual-memory-used-Killing-container" class="headerlink" title="Diagnostics: Container [pid=3293,containerID=container_e12_1555047553207_0002_02_000001] is running beyond virtual memory limits. Current usage: 116.1 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container."></a>Diagnostics: Container [pid=3293,containerID=container_e12_1555047553207_0002_02_000001] is running beyond virtual memory limits. Current usage: 116.1 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container.</h2><p><strong>解决：</strong> 在yarn-site.xml文件中添加如下配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="问题解决参考文章"><a href="#问题解决参考文章" class="headerlink" title="问题解决参考文章"></a>问题解决参考文章</h2><p><a href="https://stackoverflow.com/questions/41054700/could-not-parse-master-url" target="_blank" rel="noopener">https://stackoverflow.com/questions/41054700/could-not-parse-master-url</a></p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p>
<p><a href="http://www.cnblogs.com/lisi2016/p/6863923.html" target="_blank" rel="noopener">http://www.cnblogs.com/lisi2016/p/6863923.html</a>  </p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢!"></a>致谢!</h1><p>如有遇到问题，将问题发送至本人邮箱(<a href="mailto:t_spider@aliyun.com" target="_blank" rel="noopener">t_spider@aliyun.com</a>)。欢迎大家一起讨论问题！</p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/04/10/spark sql实战案例/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/10/spark sql实战案例/" itemprop="url">sparksql实战案例</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-10T20:11:36+08:00">
                2019-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sparksql/" itemprop="url" rel="index">
                    <span itemprop="name">sparksql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/10/spark sql实战案例/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/10/spark sql实战案例/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2,053
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  12
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="累计统计"><a href="#累计统计" class="headerlink" title="累计统计"></a>累计统计</h1><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>access.csv<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">15</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">8</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">4</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">6</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">16</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">22</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">10</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">50</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">25</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">5</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">23</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">1</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">50</span></span><br></pre></td></tr></table></figure></p>
<h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AccumulatorCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 读取数据</span></span><br><span class="line">    <span class="keyword">val</span> usersDF = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">","</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .load(<span class="string">"src/main/resources/access.csv"</span>)</span><br><span class="line">      .toDF(<span class="string">"name"</span>, <span class="string">"mounth"</span>, <span class="string">"amount"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="具体实现逻辑"><a href="#具体实现逻辑" class="headerlink" title="具体实现逻辑"></a>具体实现逻辑</h2><h3 id="DataFrame-API-方式"><a href="#DataFrame-API-方式" class="headerlink" title="DataFrame API 方式"></a>DataFrame API 方式</h3><p><strong>方式一:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// rowsBetween(Long.MinValue, 0):窗口的大小是按照排序从最小值到当前行</span></span><br><span class="line"><span class="keyword">val</span> accuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="number">0</span>)</span><br><span class="line">usersDF.withColumn(<span class="string">"acc_amount"</span>, sum(usersDF(<span class="string">"amount"</span>)).over(accuCntSpec)).show()</span><br></pre></td></tr></table></figure></p>
<p><strong>方式二</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(accuCntSpec).as(<span class="string">"acc_amount"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p>
<h3 id="sql方式"><a href="#sql方式" class="headerlink" title="sql方式"></a>sql方式</h3><p><strong>思路</strong>：根据DF算子意思，找到SqlBase.g4文件，看看是否有该类sql支持。<br>在SqlBase.g4文件中刚好找到如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">windowFrame</span><br><span class="line">    : frameType=RANGE start=frameBound</span><br><span class="line">    | frameType=ROWS start=frameBound</span><br><span class="line">    | frameType=RANGE BETWEEN start=frameBound AND end=frameBound</span><br><span class="line">    | frameType=ROWS BETWEEN start=frameBound AND end=frameBound</span><br><span class="line">    ;</span><br><span class="line"></span><br><span class="line">frameBound</span><br><span class="line">    : UNBOUNDED boundType=(PRECEDING | FOLLOWING)</span><br><span class="line">    | boundType=CURRENT ROW</span><br><span class="line">    | expression boundType=(PRECEDING | FOLLOWING)</span><br><span class="line">    ;</span><br></pre></td></tr></table></figure></p>
<p>在spark源码sql模块core项目org.apache.spark.sql.execution包中找到SQLWindowFunctionSuite类找到如下测试方法<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">"window function: multiple window expressions in a single expression"</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> nums = sparkContext.parallelize(<span class="number">1</span> to <span class="number">10</span>).map(x =&gt; (x, x % <span class="number">2</span>)).toDF(<span class="string">"x"</span>, <span class="string">"y"</span>)</span><br><span class="line">   nums.createOrReplaceTempView(<span class="string">"nums"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> expected =</span><br><span class="line">     <span class="type">Row</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">55</span>, <span class="number">1</span>, <span class="number">57</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">55</span>, <span class="number">2</span>, <span class="number">60</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">55</span>, <span class="number">4</span>, <span class="number">65</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">4</span>, <span class="number">10</span>, <span class="number">55</span>, <span class="number">6</span>, <span class="number">71</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">55</span>, <span class="number">9</span>, <span class="number">79</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">6</span>, <span class="number">21</span>, <span class="number">55</span>, <span class="number">12</span>, <span class="number">88</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">7</span>, <span class="number">28</span>, <span class="number">55</span>, <span class="number">16</span>, <span class="number">99</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">55</span>, <span class="number">20</span>, <span class="number">111</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">55</span>, <span class="number">25</span>, <span class="number">125</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">10</span>, <span class="number">55</span>, <span class="number">55</span>, <span class="number">30</span>, <span class="number">140</span>) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> actual = sql(</span><br><span class="line">     <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">       |SELECT</span></span><br><span class="line"><span class="string">       |  y,</span></span><br><span class="line"><span class="string">       |  x,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w1 AS running_sum,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w2 AS total_sum,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w3 AS running_sum_per_y,</span></span><br><span class="line"><span class="string">       |  ((sum(x) OVER w1) + (sum(x) OVER w2) + (sum(x) OVER w3)) as combined2</span></span><br><span class="line"><span class="string">       |FROM nums</span></span><br><span class="line"><span class="string">       |WINDOW w1 AS (ORDER BY x ROWS BETWEEN UnBOUNDED PRECEDiNG AND CuRRENT RoW),</span></span><br><span class="line"><span class="string">       |       w2 AS (ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOuNDED FoLLOWING),</span></span><br><span class="line"><span class="string">       |       w3 AS (PARTITION BY y ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</span></span><br><span class="line"><span class="string">     "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">   checkAnswer(actual, expected)</span><br><span class="line"></span><br><span class="line">   spark.catalog.dropTempView(<span class="string">"nums"</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p>下面就可以开心的照着案例写sql去了，真嗨皮！！！！<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">usersDF.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select name,</span></span><br><span class="line"><span class="string">    |       mounth,</span></span><br><span class="line"><span class="string">    |       amount,</span></span><br><span class="line"><span class="string">    |	      sum(amount) over (partition by name order by mounth asc  rows between unbounded preceding and current row ) as acc_amount</span></span><br><span class="line"><span class="string">    |from   access</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="累加N天之前-假设N-3"><a href="#累加N天之前-假设N-3" class="headerlink" title="累加N天之前,假设N=3"></a>累加N天之前,假设N=3</h2><h3 id="DataFrame-API方式"><a href="#DataFrame-API方式" class="headerlink" title="DataFrame API方式"></a>DataFrame API方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> preThreeAccuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="number">-3</span>, <span class="number">0</span>)</span><br><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(preThreeAccuCntSpec).as(<span class="string">"acc_amount"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="sql方式-1"><a href="#sql方式-1" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select name,</span></span><br><span class="line"><span class="string">    |       mounth,</span></span><br><span class="line"><span class="string">    |       amount,</span></span><br><span class="line"><span class="string">    |	      sum(amount) over (partition by name order by mounth asc rows between 3 preceding and current row) as acc_amount</span></span><br><span class="line"><span class="string">    |from   access</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="累加前3天，后3天"><a href="#累加前3天，后3天" class="headerlink" title="累加前3天，后3天"></a>累加前3天，后3天</h2><h3 id="API方式"><a href="#API方式" class="headerlink" title="API方式"></a>API方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> preThreeFiveAccuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(preThreeFiveAccuCntSpec).as(<span class="string">"acc_amount"</span>))</span><br></pre></td></tr></table></figure>
<h3 id="sql方式-2"><a href="#sql方式-2" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">   <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">     |select name,</span></span><br><span class="line"><span class="string">     |       mounth,</span></span><br><span class="line"><span class="string">     |       amount,</span></span><br><span class="line"><span class="string">     |	      sum(amount) over (partition by name order by mounth asc rows between 3 preceding and 3 following) as acc_amount</span></span><br><span class="line"><span class="string">     |from   access</span></span><br><span class="line"><span class="string">     |</span></span><br><span class="line"><span class="string">   "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h1 id="基本窗口函数案例"><a href="#基本窗口函数案例" class="headerlink" title="基本窗口函数案例"></a>基本窗口函数案例</h1><h2 id="准备环境-1"><a href="#准备环境-1" class="headerlink" title="准备环境"></a>准备环境</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> <span class="keyword">extends</span> <span class="title">BaseSparkSession</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-01"</span>, <span class="number">50</span>),</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-02"</span>, <span class="number">45</span>),</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-03"</span>, <span class="number">55</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-01"</span>, <span class="number">25</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-02"</span>, <span class="number">29</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-03"</span>, <span class="number">27</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="平均移动值"><a href="#平均移动值" class="headerlink" title="平均移动值"></a>平均移动值</h2><h3 id="DataFrame-API方式实现"><a href="#DataFrame-API方式实现" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)	，每一个滑动的窗口总用有3行</span></span><br><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br></pre></td></tr></table></figure>
<h3 id="sql方式实现"><a href="#sql方式实现" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="前一行数据"><a href="#前一行数据" class="headerlink" title="前一行数据"></a>前一行数据</h2><h3 id="DataFrame-API方式实现-1"><a href="#DataFrame-API方式实现-1" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"prevUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(lagwSpec)).show()</span><br></pre></td></tr></table></figure>
<h3 id="sql方式实现-1"><a href="#sql方式实现-1" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lag(user_cnt,1) over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><h3 id="DataFrame-API方式实现-2"><a href="#DataFrame-API方式实现-2" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"> df.withColumn(<span class="string">"rank"</span>, rank().over(rankwSpec))</span><br></pre></td></tr></table></figure>
<h3 id="sql方式-3"><a href="#sql方式-3" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       rank() over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure>
<h1 id="分组topn和分组取最小"><a href="#分组topn和分组取最小" class="headerlink" title="分组topn和分组取最小"></a>分组topn和分组取最小</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupBy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> rows = spark.sparkContext.parallelize(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-02-22"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-02-27"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-03-13"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-03-20"</span>, <span class="number">5</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-03-27"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-03"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-10"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-17"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-28"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-05"</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-09"</span>, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">val</span> rowRDD = rows.map(t =&gt; <span class="type">Row</span>(t._1, t._2, t._3))</span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">Array</span>(</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"shop"</span>, <span class="type">StringType</span>),</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"ycd_date"</span>, <span class="type">StringType</span>),</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"ycd_num"</span>, <span class="type">IntegerType</span>)))</span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"ycd_order"</span>)</span><br><span class="line">    <span class="comment">// 分组topN</span></span><br><span class="line">    <span class="keyword">val</span> topN = spark.sql(<span class="string">"select * from (SELECT o.shop,o.ycd_date, row_number() over (PARTITION BY o.shop ORDER BY o.ycd_date DESC) rank FROM ycd_order as o) o1 where  rank &lt; 2"</span>)</span><br><span class="line">    <span class="comment">// 根据某一个字段分组,取某一个字段的最小值</span></span><br><span class="line">    <span class="keyword">val</span> groupMin = spark.sql(<span class="string">"select o.shop,min(o.ycd_num) as min_num from ycd_order as o group by o.shop order by min_num "</span>)</span><br><span class="line">    topN.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="优雅方式定义scheme"><a href="#优雅方式定义scheme" class="headerlink" title="优雅方式定义scheme"></a>优雅方式定义scheme</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getScheme</span></span>(): <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> schemaString = <span class="string">"store_id:String,order_date:String,sale_amount: Int"</span></span><br><span class="line">    <span class="keyword">val</span> fields = schemaString.split(<span class="string">","</span>)</span><br><span class="line">      .map(fieldName =&gt;</span><br><span class="line">        <span class="type">StructField</span>(fieldName.split(<span class="string">":"</span>)(<span class="number">0</span>).trim,</span><br><span class="line">          fieldName.split(<span class="string">":"</span>)(<span class="number">1</span>).trim <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"String"</span> =&gt; <span class="type">StringType</span></span><br><span class="line">            <span class="keyword">case</span> <span class="string">"Int"</span> =&gt; <span class="type">IntegerType</span></span><br><span class="line">          &#125;, <span class="literal">true</span>))</span><br><span class="line">    <span class="type">StructType</span>(fields)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h1 id="保存小数点后n位"><a href="#保存小数点后n位" class="headerlink" title="保存小数点后n位"></a>保存小数点后n位</h1><p>10表示总的位数，2表示保留几位小数，10要&gt;=实际的位数，否则为NULL<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"select cast(sale_amount as decimal(10, 2))from ycd"</span>).show()</span><br></pre></td></tr></table></figure></p>
<h1 id="重命名行"><a href="#重命名行" class="headerlink" title="重命名行"></a>重命名行</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">SQLContext</span>, <span class="type">Row</span>, <span class="type">DataFrame</span>, <span class="type">Column</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">val</span> firstDF = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">)).toDF()</span><br><span class="line"><span class="keyword">val</span> colNames = <span class="type">Seq</span>(<span class="string">"uid"</span>, <span class="string">"col1"</span>, <span class="string">"col2"</span>, <span class="string">"col3"</span>, <span class="string">"col4"</span>, <span class="string">"col5"</span>, <span class="string">"col6"</span>)</span><br><span class="line"><span class="keyword">val</span> secondDF = firstDF.toDF(colNames: _*)</span><br></pre></td></tr></table></figure>
<h1 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">DataFrame</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;array, col, explode, lit, struct&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">)).toDF(<span class="string">"uid"</span>, <span class="string">"col1"</span>, <span class="string">"col2"</span>, <span class="string">"col3"</span>, <span class="string">"col4"</span>, <span class="string">"col5"</span>, <span class="string">"col6"</span>)</span><br><span class="line"></span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the transpose user defined function.</span></span><br><span class="line"><span class="comment">// Imputs:</span></span><br><span class="line"><span class="comment">//   transDF: The dataframe which will be transposed</span></span><br><span class="line"><span class="comment">//   transBy: The column that the dataframe will be transposed by</span></span><br><span class="line"><span class="comment">// Outputs:</span></span><br><span class="line"><span class="comment">//   Dataframe datatype consisting of three columns:</span></span><br><span class="line"><span class="comment">//     transBy</span></span><br><span class="line"><span class="comment">//     column_name</span></span><br><span class="line"><span class="comment">//     column_value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transposeUDF</span></span>(transDF: <span class="type">DataFrame</span>, transBy: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (cols, types) = transDF.dtypes.filter&#123; <span class="keyword">case</span> (c, _) =&gt; !transBy.contains(c)&#125;.unzip</span><br><span class="line">  require(types.distinct.size == <span class="number">1</span>)      </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kvs = explode(array(</span><br><span class="line">    cols.map(c =&gt; struct(lit(c).alias(<span class="string">"column_name"</span>), col(c).alias(<span class="string">"column_value"</span>))): _*</span><br><span class="line">  ))</span><br><span class="line">  <span class="keyword">val</span> byExprs = transBy.map(col(_))</span><br><span class="line"></span><br><span class="line">  transDF</span><br><span class="line">    .select(byExprs :+ kvs.alias(<span class="string">"_kvs"</span>): _*)</span><br><span class="line">    .select(byExprs ++ <span class="type">Seq</span>($<span class="string">"_kvs.column_name"</span>, $<span class="string">"_kvs.column_value"</span>): _*)</span><br><span class="line">&#125;</span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="type">Output</span>:</span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|uid|col1|col2|col3|col4|col5|col6|</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|<span class="number">1</span>  |<span class="number">1</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">4</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">9</span>   |<span class="number">8</span>   |</span><br><span class="line">|<span class="number">3</span>  |<span class="number">6</span>   |<span class="number">1</span>   |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">6</span>   |</span><br><span class="line">|<span class="number">4</span>  |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">6</span>   |<span class="number">9</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">5</span>  |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">3</span>   |</span><br><span class="line">|<span class="number">6</span>  |<span class="number">1</span>   |<span class="number">1</span>   |<span class="number">4</span>   |<span class="number">2</span>   |<span class="number">8</span>   |<span class="number">4</span>   |</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line"></span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|uid|column_name|column_value|</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|<span class="number">1</span>  |col1       |<span class="number">1</span>           |</span><br><span class="line">|<span class="number">1</span>  |col2       |<span class="number">2</span>           |</span><br><span class="line">|<span class="number">1</span>  |col3       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">1</span>  |col4       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">1</span>  |col5       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">1</span>  |col6       |<span class="number">5</span>           |</span><br><span class="line">|<span class="number">2</span>  |col1       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">2</span>  |col2       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">2</span>  |col3       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">2</span>  |col4       |<span class="number">7</span>           |</span><br><span class="line">|<span class="number">2</span>  |col5       |<span class="number">9</span>           |</span><br><span class="line">|<span class="number">2</span>  |col6       |<span class="number">8</span>           |</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">only showing top <span class="number">12</span> rows</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/02/13/DynamicVariable详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/DynamicVariable详解/" itemprop="url">DynamicVariable详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T23:34:37+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/02/13/DynamicVariable详解/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/02/13/DynamicVariable详解/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  304
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>内容来源于：<a href="https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable" target="_blank" rel="noopener">https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable</a>    </p>
<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> _response   = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">HttpServletResponse</span>](<span class="literal">null</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> _request    = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">HttpServletRequest</span>](<span class="literal">null</span>)</span><br></pre></td></tr></table></figure>
<p>DynamicVariable是贷款和动态范围模式的实现。 DynamicVariable的使用情况与Java中的ThreadLocal非常相似(事实上，DynamicVariable在后台使用InheritableThreadLocal) – 当需要在一个封闭的作用域内进行计算时，每个线程都有自己的副本的变量值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dynamicVariable.withValue(value)&#123; valueInContext =&gt;</span><br><span class="line">  <span class="comment">// value used in the context</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>由于DynamicVariable使用可继承的ThreadLocal，变量的值被传递给上下文中生成的线程：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dynamicVariable.withValue(value)&#123; valueInContext =&gt;</span><br><span class="line">  spawn&#123;</span><br><span class="line">    <span class="comment">// value is passed to the spawned thread</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>DynamicVariable(和ThreadLocal)在Scalatra中使用的原因与在许多其他框架(Lift，Spring，Struts等)中使用的相同 – 它是一种非侵入性的方式来存储和传递上下文(线程)特定的信息。</p>
<p>使HttpServletResponse和HttpServletRequest动态变量(并因此，绑定到处理请求的特定线程)只是最简单的方法来获取它们在代码中的任何地方(不通过方法参数或任何其他显式)。</p>
<h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dyn = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">String</span>](<span class="string">"withoutValue"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print=println</span></span>(dyn.value)</span><br><span class="line">print</span><br><span class="line">dyn.withValue(<span class="string">"withValue"</span>) &#123;</span><br><span class="line">  print</span><br><span class="line">&#125;</span><br><span class="line">print</span><br></pre></td></tr></table></figure>
<p><strong><em>结果</em></strong><br>withoutValue<br>withValue<br>withoutValue   </p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/02/13/java concurrent包类详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/java concurrent包类详解/" itemprop="url">java concurrent包类详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T23:34:37+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/02/13/java concurrent包类详解/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/02/13/java concurrent包类详解/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  549
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h1><p>原文：<a href="https://zhuanlan.zhihu.com/p/27314456" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27314456</a><br>Semaphore(信号量)是java.util.concurrent下的一个工具类.<strong>用来控制可同时访问特定资源的线程数</strong>.内部是通过维护父类(AQS)的 int state值实现.<br>Semaphore中有一个”许可”的概念:</p>
<p>访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。<br>访问资源后，使用release()释放许可。</p>
<p>这个许可在构造时传入,赋给state值,它等同于state.</p>
<p><strong>Semaphore应用场景</strong><br>系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。<br>Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。<br><strong>使用案例：</strong><br>spark LiveListenerBus类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> val eventLock = <span class="keyword">new</span> Semaphore(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> val listenerThread = <span class="keyword">new</span> Thread(name) &#123;</span><br><span class="line">  setDaemon(<span class="keyword">true</span>)</span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryOrStopSparkContext(sparkContext) &#123;</span><br><span class="line">    LiveListenerBus.withinListenerThread.withValue(<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        eventLock.acquire()</span><br><span class="line">        self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">          processingEvent = <span class="keyword">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 消费消息</span></span><br><span class="line">          val event = eventQueue.poll</span><br><span class="line">          <span class="keyword">if</span> (event == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Get out of the while loop and shutdown the daemon thread</span></span><br><span class="line">            <span class="keyword">if</span> (!stopped.get) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Polling `null` from eventQueue means"</span> +</span><br><span class="line">                <span class="string">" the listener bus has been stopped. So `stopped` must be true"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">          &#125;</span><br><span class="line">          postToAll(event)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">            processingEvent = <span class="keyword">false</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Semaphore属于一种较常见的限流手段，Google Guava封装了一层。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//JDK API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> Semaphore semaphore = <span class="keyword">new</span> Semaphore(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        semaphore.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">        semaphore.release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Google Guava API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> RateLimiter rateLimiter = RateLimiter.create(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        rateLimiter.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/01/09/scala学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/09/scala学习笔记/" itemprop="url">scala学习笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-09T23:34:37+08:00">
                2019-01-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/09/scala学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/01/09/scala学习笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6,121
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  27
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="高级函数"><a href="#高级函数" class="headerlink" title="高级函数"></a>高级函数</h1><p>高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。    </p>
<h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x: <span class="type">Int</span>) =&gt; x * <span class="number">3</span></span><br></pre></td></tr></table></figure>
<h2 id="带函数参数的函数"><a href="#带函数参数的函数" class="headerlink" title="带函数参数的函数"></a>带函数参数的函数</h2><p>Scala集合类（collections）的高阶函数map。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">B</span>, <span class="type">That</span>](f: <span class="type">A</span> =&gt; <span class="type">B</span>)(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span> = &#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">builder</span> </span>= &#123;</span><br><span class="line">     <span class="keyword">val</span> b = bf(repr)</span><br><span class="line">     b.sizeHint(<span class="keyword">this</span>)</span><br><span class="line">     b</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">val</span> b = builder</span><br><span class="line">   <span class="keyword">for</span> (x &lt;- <span class="keyword">this</span>) b += f(x)</span><br><span class="line">   b.result</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">doubleSalary</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">     x * <span class="number">2</span></span><br><span class="line">   &#125;</span><br><span class="line">   seq.map(doubleSalary).foreach(x =&gt; println(x))</span><br><span class="line">   seq.map(x =&gt; x * <span class="number">2</span>)</span><br><span class="line">   seq.map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line"><span class="keyword">val</span> first = sum(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> second = first(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="嵌套方法"><a href="#嵌套方法" class="headerlink" title="嵌套方法"></a>嵌套方法</h1><p>在Scala中可以嵌套定义方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fact</span></span>(x: <span class="type">Int</span>, accumulator: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (x &lt;= <span class="number">1</span>) accumulator</span><br><span class="line">      <span class="keyword">else</span> fact(x - <span class="number">1</span>, x * accumulator)</span><br><span class="line">    &#125;  </span><br><span class="line">    fact(x, <span class="number">1</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> println(<span class="string">"Factorial of 2: "</span> + factorial(<span class="number">2</span>))</span><br><span class="line"> println(<span class="string">"Factorial of 3: "</span> + factorial(<span class="number">3</span>))</span><br></pre></td></tr></table></figure></p>
<h1 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h1><p>柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。<br><strong>接受两个参数的过程：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x:<span class="type">Int</span> , y: <span class="type">Int</span> )= x * y</span><br></pre></td></tr></table></figure></p>
<p><strong>接受一个参数的过程</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>scala支持简写成如下的柯里化：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line"></span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>在Scala集合中定义的特质TraversableOnce[+A]。<br>Traversable： 能横过的；能越过的；可否定的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> result = z</span><br><span class="line">    <span class="keyword">this</span> foreach (x =&gt; result = op(result, x))</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br><span class="line">```   </span><br><span class="line">foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：</span><br><span class="line"></span><br><span class="line">从初值<span class="number">0</span>开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。  </span><br><span class="line">```scala</span><br><span class="line"><span class="keyword">val</span> numbers = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> res = numbers.foldLeft(<span class="number">0</span>)((m, n) =&gt; m + n)</span><br><span class="line"><span class="keyword">val</span> res2 = numbers.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">println(res) <span class="comment">// 55</span></span><br><span class="line">pringln(res2)</span><br></pre></td></tr></table></figure></p>
<p>多参数列表有更复杂的调用语法，因此应该谨慎使用。<br><strong>建议的使用场景包括:</strong>   </p>
<ol>
<li><p>单一的函数参数。<br>在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numbers.foldLeft(<span class="number">0</span>, &#123;(m: <span class="type">Int</span>, n: <span class="type">Int</span>) =&gt; m + n&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>隐式（IMPLICIT）参数。     </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(arg: <span class="type">Int</span>)(<span class="keyword">implicit</span> ec: <span class="type">ExecutionContext</span>) = ???</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h1><p>match对应java里的Switch,不过它写在选择器表达式之后。 </p>
<p>选择器 match {备选项}<br>取代了：<br>switch(选择器){备选项}  </p>
<h2 id="match与switch的比较"><a href="#match与switch的比较" class="headerlink" title="match与switch的比较"></a>match与switch的比较</h2><p>匹配表达式可以被看做java风格switch的泛化。<br>match的不同：<br>1、match是scala表达式，也就是说，它始终以值作为结果;<br>2、 scala的备选项表达式永远不会”掉到”下一个case;<br>3、 如果没有模式匹配，MatchErro异常会被抛出。</p>
<h1 id="提取器对象"><a href="#提取器对象" class="headerlink" title="提取器对象"></a>提取器对象</h1><p>提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"> <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// 调用工厂构造方法，构造出对象实例</span></span><br><span class="line">   <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"Spark"</span>, <span class="number">6</span>)</span><br><span class="line">   <span class="comment">// 这种写法居然可以正常编译</span></span><br><span class="line">   <span class="keyword">val</span> <span class="type">Person</span>(name, age) = person</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 使用了提取器，</span></span><br><span class="line"><span class="comment">    * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(name + <span class="string">" : "</span> + age) <span class="comment">//正常输出: Spark 6</span></span><br><span class="line">   </span><br><span class="line">   person <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">     <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>自定义unapply方法</strong><br>主要用于模式匹配中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val salary: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>, salary: <span class="type">Int</span>):<span class="type">Person</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Person</span>(name, salary)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(money: <span class="type">Person</span>): <span class="type">Option</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    <span class="keyword">if</span>(money == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Some</span>(money.name, money.salary)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"spark"</span>, <span class="number">800</span>);</span><br><span class="line">     person <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="for-表达式"><a href="#for-表达式" class="headerlink" title="for 表达式"></a>for 表达式</h1><p>Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">userBase</span> </span>= <span class="type">List</span>(<span class="type">User</span>(<span class="string">"Travis"</span>, <span class="number">28</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Kelly"</span>, <span class="number">33</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Jennifer"</span>, <span class="number">44</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Dennis"</span>, <span class="number">23</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> twentySomethings = <span class="keyword">for</span> (user &lt;- userBase <span class="keyword">if</span> (user.age &gt;=<span class="number">20</span> &amp;&amp; user.age &lt; <span class="number">30</span>))</span><br><span class="line">  <span class="keyword">yield</span> user.name  </span><br><span class="line"></span><br><span class="line">twentySomethings.foreach(name =&gt; println(name))</span><br></pre></td></tr></table></figure></p>
<p>这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user <- userbase="" 是生成器，if="" (user.age="">=20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。</-></p>
<h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><p>泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。<br>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。  </p>
<h2 id="定义一个泛型类"><a href="#定义一个泛型类" class="headerlink" title="定义一个泛型类"></a>定义一个泛型类</h2><p>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> elements: <span class="type">List</span>[<span class="type">A</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">push</span></span>(x: <span class="type">A</span>) &#123; elements = x :: elements &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">peek</span></span>: <span class="type">A</span> = elements.head</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pop</span></span>(): <span class="type">A</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> currentTop = peek</span><br><span class="line">    elements = elements.tail</span><br><span class="line">    currentTop</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。   </p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>要使用一个泛型类，将一个具体类型放到方括号中来代替 A。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stack = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">Int</span>]</span><br><span class="line">stack.push(<span class="number">1</span>)</span><br><span class="line">stack.push(<span class="number">2</span>)</span><br><span class="line">println(stack.pop)  <span class="comment">// prints 2</span></span><br><span class="line">println(stack.pop)  <span class="comment">// prints 1</span></span><br></pre></td></tr></table></figure></p>
<h1 id="型变"><a href="#型变" class="headerlink" title="型变"></a>型变</h1><p>型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>[+<span class="type">A</span>] <span class="title">//</span> <span class="title">一个协变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Bar</span>[-<span class="type">A</span>] <span class="title">//</span> <span class="title">一个逆变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Baz</span>[<span class="type">A</span>]  <span class="title">//</span> <span class="title">一个不变类</span></span></span><br></pre></td></tr></table></figure></p>
<h2 id="协变"><a href="#协变" class="headerlink" title="协变"></a>协变</h2><p>使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。   </p>
<p>考虑以下简单的类结构：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Dog</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br></pre></td></tr></table></figure></p>
<p>类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。</p>
<p>在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CovarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printAnimalNames</span></span>(animals: <span class="type">List</span>[<span class="type">Animal</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    animals.foreach &#123; animal =&gt;</span><br><span class="line">      println(animal.name)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> cats: <span class="type">List</span>[<span class="type">Cat</span>] = <span class="type">List</span>(<span class="type">Cat</span>(<span class="string">"Whiskers"</span>), <span class="type">Cat</span>(<span class="string">"Tom"</span>))</span><br><span class="line">  <span class="keyword">val</span> dogs: <span class="type">List</span>[<span class="type">Dog</span>] = <span class="type">List</span>(<span class="type">Dog</span>(<span class="string">"Fido"</span>), <span class="type">Dog</span>(<span class="string">"Rex"</span>))</span><br><span class="line"></span><br><span class="line">  printAnimalNames(cats)</span><br><span class="line">  <span class="comment">// Whiskers</span></span><br><span class="line">  <span class="comment">// Tom</span></span><br><span class="line">  printAnimalNames(dogs)</span><br><span class="line">  <span class="comment">// Fido</span></span><br><span class="line">  <span class="comment">// Rex</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="逆变"><a href="#逆变" class="headerlink" title="逆变"></a>逆变</h2><p>通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。   </p>
<p>考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Printer</span>[-<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnimalPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Animal</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(animal: <span class="type">Animal</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The animal's name is: "</span> + animal.name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CatPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Cat</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(cat: <span class="type">Cat</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The cat's name is: "</span> + cat.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ContravarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> myCat: <span class="type">Cat</span> = <span class="type">Cat</span>(<span class="string">"Boots"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printMyCat</span></span>(printer: <span class="type">Printer</span>[<span class="type">Cat</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printer.print(myCat)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> catPrinter: <span class="type">Printer</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">CatPrinter</span></span><br><span class="line">  <span class="keyword">val</span> animalPrinter: <span class="type">Printer</span>[<span class="type">Animal</span>] = <span class="keyword">new</span> <span class="type">AnimalPrinter</span></span><br><span class="line"></span><br><span class="line">  printMyCat(catPrinter)</span><br><span class="line">  printMyCat(animalPrinter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个程序的输出如下：   </p>
<blockquote>
<p>The cat’s name is: Boots<br>The animal’s name is: Boots    </p>
</blockquote>
<h1 id="不变"><a href="#不变" class="headerlink" title="不变"></a>不变</h1><p>默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Container</span>[<span class="type">A</span>](<span class="params">value: <span class="type">A</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _value: <span class="type">A</span> = value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">A</span> = _value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setValue</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _value = value</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> catContainer: <span class="type">Container</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">Container</span>(<span class="type">Cat</span>(<span class="string">"Felix"</span>))</span><br><span class="line"><span class="keyword">val</span> animalContainer: <span class="type">Container</span>[<span class="type">Animal</span>] = catContainer</span><br><span class="line">animalContainer.setValue(<span class="type">Dog</span>(<span class="string">"Spot"</span>))</span><br><span class="line"><span class="keyword">val</span> cat: <span class="type">Cat</span> = catContainer.getValue</span><br></pre></td></tr></table></figure></p>
<p>  糟糕，我们最终会将一只狗作为值分配给一只猫<br>幸运的是，编译器在此之前就会阻止我们。</p>
<h2 id="上界"><a href="#上界" class="headerlink" title="上界"></a>上界</h2><p>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Pet</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Cat"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Dog"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lion</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Lion"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PetContainer</span>[<span class="type">P</span> &lt;: <span class="type">Pet</span>](<span class="params">p: <span class="type">P</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pet</span></span>: <span class="type">P</span> = p</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dogContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Dog</span>](<span class="keyword">new</span> <span class="type">Dog</span>)</span><br><span class="line"><span class="keyword">val</span> catContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Cat</span>](<span class="keyword">new</span> <span class="type">Cat</span>)</span><br><span class="line"><span class="comment">// this would not compile</span></span><br><span class="line"><span class="keyword">val</span> lionContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Lion</span>](<span class="keyword">new</span> <span class="type">Lion</span>)</span><br></pre></td></tr></table></figure></p>
<p>类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">arguments</span> [<span class="type">Lion</span>] <span class="title">do</span> <span class="title">not</span> <span class="title">conform</span> <span class="title">to</span> <span class="title">class</span> <span class="title">PetContainer</span>'<span class="title">s</span> <span class="title">type</span> <span class="title">parameter</span> <span class="title">bounds</span> [<span class="type">P</span> &lt;: <span class="type">Pet</span>]</span></span><br></pre></td></tr></table></figure></p>
<p>这是因为Lion并不是Pet的子类。   </p>
<h2 id="下界"><a href="#下界" class="headerlink" title="下界"></a>下界</h2><p>类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。  </p>
<p>下面看一个适合用类型下界的例子：<br>下面看一个适合用类型下界的例子：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">Node</span>[<span class="type">B</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。</p>
<p>但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。</p>
<p>要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">Node</span>[<span class="type">U</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>现在我们像下面这么做：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">AfricanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">EuropeanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">africanSwallowList=</span> <span class="title">ListNode</span>[<span class="type">AfricanSwallow</span>](<span class="params"><span class="type">AfricanSwallow</span>(</span>), <span class="title">Nil</span>(<span class="params"></span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">birdList</span></span>: <span class="type">Node</span>[<span class="type">Bird</span>] = africanSwallowList</span><br><span class="line">birdList.prepend(<span class="keyword">new</span> <span class="type">EuropeanSwallow</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。</p>
<h1 id="抽象类型"><a href="#抽象类型" class="headerlink" title="抽象类型"></a>抽象类型</h1><p>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">T</span></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">element</span></span>: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。<br>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span> <span class="keyword">extends</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span></span></span><br><span class="line"><span class="class">  <span class="title">type</span> <span class="title">T</span> <span class="title">&lt;</span></span>: <span class="type">Seq</span>[<span class="type">U</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。</p>
<p>含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSeqBuffer</span> <span class="keyword">extends</span> <span class="title">SeqBuffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span> </span>= <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(elem1: <span class="type">Int</span>, elem2: <span class="type">Int</span>): <span class="type">IntSeqBuffer</span> =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">IntSeqBuffer</span> &#123;</span><br><span class="line">       <span class="class"><span class="keyword">type</span> <span class="title">T</span> </span>= <span class="type">List</span>[<span class="type">U</span>]</span><br><span class="line">       <span class="keyword">val</span> element = <span class="type">List</span>(elem1, elem2)</span><br><span class="line">     &#125;</span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p>
<p>这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。</p>
<p>把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span>[+<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> element: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span>[<span class="type">U</span>, +<span class="type">T</span> &lt;: <span class="type">Seq</span>[<span class="type">U</span>]] <span class="keyword">extends</span> <span class="title">Buffer</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(e1: <span class="type">Int</span>, e2: <span class="type">Int</span>): <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]] &#123;</span><br><span class="line">    <span class="keyword">val</span> element = <span class="type">List</span>(e1, e2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p>
<p>需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。</p>
<h1 id="复合类型"><a href="#复合类型" class="headerlink" title="复合类型"></a>复合类型</h1><p><strong>需求</strong>:<br>有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。</p>
<p><strong>案例</strong>：<br>假设我们有两个特质 Cloneable 和 Resetable：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Cloneable</span> <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Cloneable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clone</span></span>(): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.clone().asInstanceOf[<span class="type">Cloneable</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Resetable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>: <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: ?): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> cloned = obj.clone()</span><br><span class="line">  obj.reset</span><br><span class="line">  cloned</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。</p>
<p>以下是更新后的方法：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: <span class="type">Cloneable</span> <span class="keyword">with</span> <span class="type">Resetable</span>): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement }</p>
<p>关于使用细化的例子参考 通过混入（mixin）来组合类。</p>
<h1 id="自类型"><a href="#自类型" class="headerlink" title="自类型"></a>自类型</h1><p>自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。</p>
<p>自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。</p>
<p>要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">username</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Tweeter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>: <span class="type">User</span> =&gt;  <span class="comment">// 重新赋予 this 的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tweet</span></span>(tweetText: <span class="type">String</span>) = println(<span class="string">s"<span class="subst">$username</span>: <span class="subst">$tweetText</span>"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VerifiedTweeter</span>(<span class="params">val username_ : <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Tweeter</span> <span class="keyword">with</span> <span class="title">User</span> </span>&#123;  <span class="comment">// 我们混入特质 User 因为 Tweeter 需要</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">username</span> </span>= <span class="string">s"real <span class="subst">$username_</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> realBeyoncé = <span class="keyword">new</span> <span class="type">VerifiedTweeter</span>(<span class="string">"Beyoncé"</span>)</span><br><span class="line">realBeyoncé.tweet(<span class="string">"Just spilled my glass of lemonade"</span>)  <span class="comment">// 打印出 "real Beyoncé: Just spilled my glass of lemonade"</span></span><br></pre></td></tr></table></figure></p>
<p>因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。<br><strong>自类型别名</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  self =&gt;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(num1: <span class="type">Int</span>, num2: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    num1 + num2</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(self.sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h1><p><strong>定义</strong>：指的是那种以implicit关键字声明的带有单个参数的函数。<br>通过隐式转换，程序员可以在编写Scala程序时故意漏掉一些信息，让编译器去尝试在编译期间自动推导出这些信息来，这种特性可以极大的减少代码量，忽略那些冗长，过于细节的代码。<br>1.将方法或变量标记为implicit<br>2.将方法的参数列表标记为implicit<br>3.将类标记为implicit   </p>
<p>Scala支持两种形式的隐式转换：<br>隐式值：用于给方法提供参数<br>隐式视图：用于类型间转换或使针对某类型的方法能调用成功   </p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="给File类增加read方法"><a href="#给File类增加read方法" class="headerlink" title="给File类增加read方法"></a>给File类增加read方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">val f: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>() = <span class="type">Source</span>.fromFile(f).mkString</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFilePredef</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">fileToRichFile</span></span>(f: <span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile</span>(f)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>使用</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"><span class="keyword">import</span> com.tm.scala.implic.<span class="type">RichFilePredef</span>._</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFile</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">     <span class="keyword">val</span> f = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"/home/hadoop/sample_movielens_data.txt"</span>)</span><br><span class="line">    <span class="comment">// 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object</span></span><br><span class="line">    <span class="keyword">val</span> contents = f.read()</span><br><span class="line">    println(contents)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="排序中的使用案例"><a href="#排序中的使用案例" class="headerlink" title="排序中的使用案例"></a>排序中的使用案例</h3><h4 id="Ordered方式"><a href="#Ordered方式" class="headerlink" title="Ordered方式"></a>Ordered方式</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Girl</span>(<span class="params">val name: <span class="type">String</span>, var faceValue: <span class="type">Int</span>, var age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>定义比较规则</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 视图定界</span></span><br><span class="line"><span class="comment">  * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered</span></span><br><span class="line"><span class="comment">  * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。</span></span><br><span class="line"><span class="comment">  * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。</span></span><br><span class="line"><span class="comment">  * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderedChooser</span>[<span class="type">T</span> &lt;% <span class="type">Ordered</span>[<span class="type">T</span>]] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (first &gt; second) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderedPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 方式一</span></span><br><span class="line"> <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">gilrToOrdered</span></span>(girl: <span class="type">Girl</span>):<span class="type">Ordered</span>[<span class="type">Girl</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">            girl.age - that.age</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            girl.faceValue - that.faceValue</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdered = (girl: <span class="type">Girl</span>) =&gt; <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">        girl.age - that.age</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        girl.faceValue - that.faceValue</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdered</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> girl1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"spark"</span>, <span class="number">100</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> girl2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"mxnet"</span>, <span class="number">90</span>,<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderedPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> chooser = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = chooser.choose(girl1, girl2)</span><br><span class="line">    println(g.faceValue)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="Ordering方式"><a href="#Ordering方式" class="headerlink" title="Ordering方式"></a>Ordering方式</h4><p><strong>定义比较规则</strong>  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 文本定界</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderingChoose</span>[<span class="type">T</span>: <span class="type">Ordering</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ord = implicitly[<span class="type">Ordering</span>[<span class="type">T</span>]]</span><br><span class="line">    <span class="keyword">if</span> (ord.gt(first, second)) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderingPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">//  方式一</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdering = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//  方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">GilrToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式三</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 参考：Ordering中的下面方法</span></span><br><span class="line"><span class="comment">    * trait IntOrdering extends Ordering[Int] &#123;</span></span><br><span class="line"><span class="comment">    * def compare(x: Int, y: Int) =</span></span><br><span class="line"><span class="comment">    * if (x &lt; y) -1</span></span><br><span class="line"><span class="comment">    * else if (x == y) 0</span></span><br><span class="line"><span class="comment">    * else 1</span></span><br><span class="line"><span class="comment">    * &#125;</span></span><br><span class="line"><span class="comment">    * implicit object Int extends IntOrdering</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">trait</span> <span class="title">GirlToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">Girl</span> <span class="keyword">extends</span> <span class="title">GirlToOrdering</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdering</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> g1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"zhangsan"</span>, <span class="number">50</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> g2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"lisi"</span>, <span class="number">500</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderingPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> choose = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = choose.choose(g1, g2)</span><br><span class="line">    println(g.name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="scala导入类并取别名"><a href="#scala导入类并取别名" class="headerlink" title="scala导入类并取别名"></a>scala导入类并取别名</h1><p>导入Map,并取别名<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Map</span> =&gt; <span class="type">JMap</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="scala-Try的使用"><a href="#scala-Try的使用" class="headerlink" title="scala Try的使用"></a>scala Try的使用</h1><p>spark Utils中的使用案例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classIsLoadable</span></span>(clazz: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   <span class="comment">// scalastyle:off classforname</span></span><br><span class="line">   <span class="type">Try</span> &#123;</span><br><span class="line">     <span class="type">Class</span>.forName(clazz, <span class="literal">false</span>, getContextOrSparkClassLoader)</span><br><span class="line">   &#125;.isSuccess</span><br><span class="line">   <span class="comment">// scalastyle:on classforname</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="quasiquotes-q字符串-用于代码生成"><a href="#quasiquotes-q字符串-用于代码生成" class="headerlink" title="quasiquotes(q字符串)用于代码生成"></a>quasiquotes(q字符串)用于代码生成</h1><p>官方文档： <a href="https://docs.scala-lang.org/overviews/quasiquotes/intro.html" target="_blank" rel="noopener">https://docs.scala-lang.org/overviews/quasiquotes/intro.html</a><br>参考文档：<a href="https://www.cnblogs.com/shishanyuan/p/8455786.html" target="_blank" rel="noopener">https://www.cnblogs.com/shishanyuan/p/8455786.html</a><br><strong>作用</strong>： Quasiquotes允许在Scala语言中对抽象语法树（AST）进行编程式构建，然后在运行时将其提供给Scala编译器以生成字节码。<br>以q开头的字符串是quasiquotes，虽然它们看起来像字符串，但它们在编译时由Scala编译器解析，并代表其代码的AST。   </p>
<pre><code class="scala"><span class="keyword">val</span> tree = <span class="string">q"i am { a quasiquote }"</span>  
tree: universe.<span class="type">Tree</span> = i.am(a.quasiquote)   
</code></pre>
<h1 id="符号详解"><a href="#符号详解" class="headerlink" title="符号详解"></a>符号详解</h1><h2 id="和-的区别"><a href="#和-的区别" class="headerlink" title="== 和===的区别"></a>== 和===的区别</h2><p>参考：<a href="http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie" target="_blank" rel="noopener">http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie</a>      </p>
<blockquote>
<p>== 返回一个布尔值<br>=== 返回一列（包含两列元素比较的结果）</p>
</blockquote>
<h1 id="call-by-value-and-call-by-name"><a href="#call-by-value-and-call-by-name" class="headerlink" title="call-by-value and call-by-name"></a>call-by-value and call-by-name</h1><p>传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；<br>传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部   </p>
<h1 id="半生对象"><a href="#半生对象" class="headerlink" title="半生对象"></a>半生对象</h1><p>object中的构造器在第一次调用执行一次，以后调用的话不会多次执行。<br>object会有自己的构造方法，默认是没有参数的构造方法。</p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/01/09/scala枚举/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/09/scala枚举/" itemprop="url">scala枚举</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-09T23:34:37+08:00">
                2019-01-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/09/scala枚举/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/01/09/scala枚举/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  175
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="定义枚举对象"><a href="#定义枚举对象" class="headerlink" title="定义枚举对象"></a>定义枚举对象</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 定义一个枚举类 */</span></span><br><span class="line"><span class="keyword">private</span>[deploy] <span class="class"><span class="keyword">object</span> <span class="title">SparkSubmitAction</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 声明枚举对外暴露的变量类型</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">SparkSubmitAction</span> </span>= <span class="type">Value</span></span><br><span class="line">  <span class="comment">// 枚举的定义</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">SUBMIT</span>, <span class="type">KILL</span>, <span class="type">REQUEST_STATUS</span> = <span class="type">Value</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="使用枚举"><a href="#使用枚举" class="headerlink" title="使用枚举"></a>使用枚举</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 定义一个枚举类 */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSubmitAction</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 声明枚举对外暴露的变量类型</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">SparkSubmitAction</span> </span>= <span class="type">Value</span></span><br><span class="line">  <span class="comment">// 枚举的定义</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">SUBMIT</span>, <span class="type">KILL</span>, <span class="type">REQUEST_STATUS</span> = <span class="type">Value</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAction</span></span>(action: <span class="type">SparkSubmitAction</span>)&#123;</span><br><span class="line">  action <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SUBMIT</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">KILL</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">REQUEST_STATUS</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println (<span class="string">"Unknown type"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EnumerationTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> action = <span class="type">SparkSubmitAction</span>.apply(<span class="number">1</span>)</span><br><span class="line">    <span class="type">SparkSubmitAction</span>.getAction(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> action1 = <span class="type">SparkSubmitAction</span>.withName(<span class="string">"quit"</span>)</span><br><span class="line">    <span class="type">SparkSubmitAction</span>.getAction(action1)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/01/04/scala特殊符号使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/04/scala特殊符号使用/" itemprop="url">scala特殊符号使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-04T19:53:18+08:00">
                2019-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/04/scala特殊符号使用/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/01/04/scala特殊符号使用/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  248
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="符号"><a href="#符号" class="headerlink" title="_*符号"></a>_*符号</h1><p>作用：它告诉编译器将序列类型的单个参数视为可变参数序列。<br>举例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">ProcessBuilder</span>(<span class="type">String</span>... command) &#123;</span><br><span class="line">       <span class="keyword">this</span>.command = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(command.length);</span><br><span class="line">       <span class="keyword">for</span> (<span class="type">String</span> arg : command)</span><br><span class="line">           <span class="keyword">this</span>.command.add(arg);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> commandSeq = <span class="type">Seq</span>(command.mainClass) ++ command.arguments</span><br><span class="line"><span class="keyword">val</span> builder = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(commandSeq: _*)</span><br></pre></td></tr></table></figure>
<h1 id=""><a href="#" class="headerlink" title="::"></a>::</h1><p>该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    args.toList <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> workerUrl :: userJar :: mainClass :: extraArgs =&gt;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">"Driver"</span>,</span><br><span class="line">          <span class="type">Utils</span>.localHostName(), <span class="number">0</span>, conf, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf))</span><br><span class="line">        rpcEnv.setupEndpoint(<span class="string">"workerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(rpcEnv, workerUrl))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> currentLoader = <span class="type">Thread</span>.currentThread.getContextClassLoader</span><br><span class="line">        <span class="keyword">val</span> userJarUrl = <span class="keyword">new</span> <span class="type">File</span>(userJar).toURI().toURL()</span><br><span class="line">        <span class="keyword">val</span> loader =</span><br><span class="line">          <span class="keyword">if</span> (sys.props.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delegate to supplied main class</span></span><br><span class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(mainClass)</span><br><span class="line">        <span class="keyword">val</span> mainMethod = clazz.getMethod(<span class="string">"main"</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line">        mainMethod.invoke(<span class="literal">null</span>, extraArgs.toArray[<span class="type">String</span>])</span><br><span class="line"></span><br><span class="line">        rpcEnv.shutdown()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="comment">// scalastyle:off println</span></span><br><span class="line">        <span class="type">System</span>.err.println(<span class="string">"Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]"</span>)</span><br><span class="line">        <span class="comment">// scalastyle:on println</span></span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/01/04/scala自定义注解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/04/scala自定义注解/" itemprop="url">scala自定注解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-04T19:53:18+08:00">
                2019-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/04/scala自定义注解/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/01/04/scala自定义注解/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  279
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么要注解"><a href="#为什么要注解" class="headerlink" title="为什么要注解"></a>为什么要注解</h1><p>除了编译和允许之外，还可以对程序做：<br>1、使用Scaladoc 自动产生文档;<br>2、漂亮的打印印出符合你偏爱分割的代码;<br>3、代码常见错误检查，如：打开了文件却没(在全部逻辑分支中)关闭。<br>4、实验类型检查，例如副作用管理或所有权属性确认。<br>这类工具（程序）被称为元编程工具，<code>它们把其它程序当做输入程序</code>。  </p>
<h1 id="scala注解所在包和基本语法格式"><a href="#scala注解所在包和基本语法格式" class="headerlink" title="scala注解所在包和基本语法格式"></a>scala注解所在包和基本语法格式</h1><p>注解所在包： 标准库定义的注解相关内容在包scala.annotation中。<br>基本语法： @注解名称(注解参数)  </p>
<h1 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h1><p>自定义注解需要从注解特质继承，scala提供两种注解：<br> 1、基本语法： @注解名称(注解参数)<br> scala中的自定义注解不是接口/特质，而是类。<br>自定义注解需要从注解特质中继承，Scala中提供了两类注解特质：</p>
<p>scala.annotation.ClassfileAnnotation 由Java编译器生成注解<br>scala.annotation.StaticAnnotation 由Scala编译器生成注解</p>

          
        
      
    </div>
    
    
    
     <div>
      
     </div>
     <div>
     
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="tang" />
            
              <p class="site-author-name" itemprop="name">tang</p>
              <p class="site-description motion-element" itemprop="description">火星度假村追梦程序员。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/tgluon" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tang</span>

  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
<span id="busuanzi_container_site_pv">
    本站总访问量:<span id="busuanzi_value_site_pv"></span>次
</span>
</div>
  
<!--<div class="powered-by">{
  }由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动{}</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">{
  }主题 &mdash; {
  }<a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">{
    }NexT.Pisces{
  }</a> v5.1.4{
}</div>
-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共52.4k字</span>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>


  
  <script type="text/javascript"
color="0,0,255" opacity='0.7' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    

  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
<script type="text/javascript" src="/js/src/love.js"></script>
</html>
