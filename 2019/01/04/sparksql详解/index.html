<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script>
    (function(){
        if('jeffdean$limu'){
            if (prompt('请输入文章密码') !== 'jeffdean$limu'){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="spark," />





  <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml" />






<meta name="description" content="Spark SQL的Dataset基本操作https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解    基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Datas">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="sparksql详解">
<meta property="og:url" content="https://tgluon.github.io/2019/01/04/sparksql详解/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="Spark SQL的Dataset基本操作https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解    基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Datas">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult1.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult2.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult3.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult4.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult5.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/dataset分区.jpg">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/ExternalClusterManager.png">
<meta property="og:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/UDWFResult.jpg">
<meta property="og:updated_time" content="2019-02-26T07:20:26.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="sparksql详解">
<meta name="twitter:description" content="Spark SQL的Dataset基本操作https://www.toutiao.com/i6631318012546793992/版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解    基本简介和准备工作Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Datas">
<meta name="twitter:image" content="https://tgluon.github.io/2019/01/04/sparksql详解/images/PivotReSult1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://tgluon.github.io/2019/01/04/sparksql详解/"/>





  <title>sparksql详解 | blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/tgluon"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_orange_ff7600.png" alt="Fork me on GitHub"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">追梦青年</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://tgluon.github.io/2019/01/04/sparksql详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">sparksql详解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-04T19:53:18+08:00">
                2019-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sparksql/" itemprop="url" rel="index">
                    <span itemprop="name">sparksql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/04/sparksql详解/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/01/04/sparksql详解/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10,476
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  48
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Spark-SQL的Dataset基本操作"><a href="#Spark-SQL的Dataset基本操作" class="headerlink" title="Spark SQL的Dataset基本操作"></a>Spark SQL的Dataset基本操作</h1><p><a href="https://www.toutiao.com/i6631318012546793992/" target="_blank" rel="noopener">https://www.toutiao.com/i6631318012546793992/</a><br>版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解   </p>
<h2 id="基本简介和准备工作"><a href="#基本简介和准备工作" class="headerlink" title="基本简介和准备工作"></a>基本简介和准备工作</h2><p>Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。</p>
<p>1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">DataFrame</span> </span>= <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure></p>
<p>结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。<br>下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。</p>
<p>首先要创建一个SparkSession:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(<span class="keyword">this</span>.getClass.getName)</span><br><span class="line">.setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">.set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"localhost"</span>) </span><br><span class="line"><span class="comment">// executor的实例数</span></span><br><span class="line">.set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>) </span><br><span class="line">.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"4"</span>) </span><br><span class="line"><span class="comment">// sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。</span></span><br><span class="line">.set(<span class="string">"spark.sql.shuffle.partitions"</span>,<span class="string">"4"</span>)</span><br><span class="line">.setJars(<span class="type">List</span>(<span class="string">"/Users/meitu/Desktop/sparkjar/bigdata.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span> </span><br><span class="line"> ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span> </span><br><span class="line">.builder() </span><br><span class="line">.config(sparkConf) </span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure></p>
<p>创建dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(</span><br><span class="line"><span class="type">Seq</span>( (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">150</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p>
<p>使用函数的时候要导入包：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col,expr&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="select"><a href="#select" class="headerlink" title="select"></a>select</h2><p>列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。<br>使用col函数，可以直接对列进行一些逻辑操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.select(<span class="string">"city"</span>,<span class="string">"year"</span>,<span class="string">"amount"</span>).show(<span class="number">1</span>)</span><br><span class="line">sales.select(col(<span class="string">"city"</span>),col(<span class="string">"amount"</span>)+<span class="number">1</span>).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h2><p>参数是字符串，且直接可以使用表达式。<br>也可以使用select+expr函数来替代。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.selectExpr(<span class="string">"city"</span>,<span class="string">"year as date"</span>,<span class="string">"amount+1"</span>).show(<span class="number">10</span>)</span><br><span class="line">sales.select(expr(<span class="string">"city"</span>),expr(<span class="string">"year as date"</span>),expr(<span class="string">"amount+1"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.filter(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.filter(row=&gt;&#123; row.getInt(<span class="number">2</span>)&gt;<span class="number">150</span>&#125;).show(<span class="number">10</span>)</span><br><span class="line">sales.filter(<span class="string">"amount &gt; 150 "</span>).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="where"><a href="#where" class="headerlink" title="where"></a>where</h2><p>类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.where(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.where(<span class="string">"amount &gt; 150 "</span>).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h2><p>主要是以count和agg聚合函数为例讲解groupby函数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.groupBy(<span class="string">"city"</span>).count().show(<span class="number">10</span>)</span><br><span class="line">sales.groupBy(col(<span class="string">"city"</span>)).agg(sum(<span class="string">"amount"</span>).as(<span class="string">"total"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.union(sales).groupBy(<span class="string">"city"</span>).count().show()</span><br></pre></td></tr></table></figure></p>
<h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>join操作相对比较复杂，具体如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 相同的列进行join</span></span><br><span class="line"> sales.join(sales,<span class="string">"city"</span>).show(<span class="number">10</span>)</span><br><span class="line"> <span class="comment">// 多列join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>)).show() </span><br><span class="line"><span class="comment">/* 指定join类型， join 类型可以选择: </span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, </span></span><br><span class="line"><span class="comment">`left`, `left_outer`, `right`, `right_outer`, </span></span><br><span class="line"><span class="comment">`left_semi`, `left_anti`. </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="comment">// 内部join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>),<span class="string">"inner"</span>).show() </span><br><span class="line"><span class="comment">/* join条件 ：</span></span><br><span class="line"><span class="comment">可以在join方法里放入join条件，</span></span><br><span class="line"><span class="comment">也可以使用where,这两种情况都要求字段名称不一样。</span></span><br><span class="line"><span class="comment">*/</span> </span><br><span class="line">sales.join(sales, col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() sales.join(sales).where(col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() </span><br><span class="line"> <span class="comment">/* </span></span><br><span class="line"><span class="comment">dataset的self join 此处使用where作为条件，</span></span><br><span class="line"><span class="comment">需要增加配置.set("spark.sql.crossJoin.enabled","true") </span></span><br><span class="line"><span class="comment">也可以加第三个参数，join类型，可以选择如下： </span></span><br><span class="line"><span class="comment">`inner`, `cross`, `outer`, `full`, `full_outer`, `left`,</span></span><br><span class="line"><span class="comment"> `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.join(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show() sales.join(sales).where(sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show()</span><br><span class="line"><span class="comment">/* joinwith,可以指定第三个参数，join类型，</span></span><br><span class="line"><span class="comment">类型可以选择如下：</span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, </span></span><br><span class="line"><span class="comment"> `right`, `right_outer`。 </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.joinWith(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>),<span class="string">"inner"</span>).show()</span><br></pre></td></tr></table></figure></p>
<p>输出结果： </p>
<h2 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h2><p>orderby 全局有序，其实用的还是sort<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.orderBy(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p>
<h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><p>全局排序，直接替换掉8小结的orderby即可。</p>
<h2 id="sortwithinpartition"><a href="#sortwithinpartition" class="headerlink" title="sortwithinpartition"></a>sortwithinpartition</h2><p>在分区内部进行排序，局部排序。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.sortWithinPartitions(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.sortWithinPartitions(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p>
<p>可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。</p>
<h2 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* withColumn </span></span><br><span class="line"><span class="comment">假如列，存在就替换，不存在新增 </span></span><br><span class="line"><span class="comment">withColumnRenamed </span></span><br><span class="line"><span class="comment">对已有的列进行重命名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//相当于给原来amount列，+1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 对amount列+1，然后将值增加到一个新列 amount1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount1"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 将amount列名，修改为amount1</span></span><br><span class="line">sales.withColumnRenamed(<span class="string">"amount"</span>,<span class="string">"amount1"</span>).show()</span><br></pre></td></tr></table></figure>
<h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><p>这个跟rdd的foreach一样，元素类型是row。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.foreach(row=&gt;&#123; </span><br><span class="line">println(row.getString(<span class="number">0</span>))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p>
<h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><p>跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sales.foreachPartition(partition=&gt;&#123; </span><br><span class="line"> <span class="comment">//打开数据库链接等 </span></span><br><span class="line"> partition.foreach(each=&gt;&#123; </span><br><span class="line"> println(each.getString(<span class="number">0</span>))</span><br><span class="line"> <span class="comment">//插入数据库 </span></span><br><span class="line"> &#125;)</span><br><span class="line"> <span class="comment">//关闭数据库链接 </span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p>
<h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><p>针对dataset的行去重，返回的是所有行都不重复的dataset。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.distinct().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="dropDuplicates"><a href="#dropDuplicates" class="headerlink" title="dropDuplicates"></a>dropDuplicates</h2><p>这个适用于dataset有唯一的主键，然后对主键进行去重。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> before = sales.count()</span><br><span class="line"><span class="keyword">val</span> after = sales.dropDuplicates(<span class="string">"city"</span>).count()</span><br><span class="line">println(<span class="string">"before ====&gt; "</span> +before)</span><br><span class="line">println(<span class="string">"after ====&gt; "</span>+after)</span><br></pre></td></tr></table></figure></p>
<h2 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h2><p>删除一列，或者多列，这是一个变参数算子。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sales.drop(<span class="string">"city"</span>).show()</span><br><span class="line">打印出来schema信息如下：</span><br><span class="line">root</span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h2><p>输出dataset的schema信息<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sales.printSchema()</span><br><span class="line"></span><br><span class="line">输出结果如下：</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"></span><br><span class="line">|-- city: string (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="explain"><a href="#explain" class="headerlink" title="explain()"></a>explain()</h2><p>打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).explain()</span><br><span class="line">执行计划输出如下：</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">1</span>) <span class="type">Sort</span> [year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>], <span class="literal">true</span>, <span class="number">0</span></span><br><span class="line">+- <span class="type">Exchange</span> rangepartitioning(year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>, <span class="number">3</span>)</span><br><span class="line">+- <span class="type">LocalTableScan</span> [city#<span class="number">6</span>, year#<span class="number">7</span>, amount#<span class="number">8</span>]</span><br></pre></td></tr></table></figure></p>
<h1 id="Spark-SQL入门到精通之第二篇Dataset的复杂操作"><a href="#Spark-SQL入门到精通之第二篇Dataset的复杂操作" class="headerlink" title="Spark SQL入门到精通之第二篇Dataset的复杂操作"></a>Spark SQL入门到精通之第二篇Dataset的复杂操作</h1><p>本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：<br>cube，rollup，pivot操作。</p>
<h2 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h2><p>简单的理解就是维度及度量组成的数据体。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sales.cube(<span class="string">"city"</span>,<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>)) </span><br><span class="line">.sort(col(<span class="string">"city"</span>).desc_nulls_first,col(<span class="string">"year"</span>).desc_nulls_first) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p>
<p>举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。<br>实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> city_year = sales</span><br><span class="line">.groupBy(<span class="string">"city"</span>,<span class="string">"year"</span>).</span><br><span class="line">agg(sum(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> city = sales.groupBy(<span class="string">"city"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"city"</span>), lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> year = sales.groupBy(<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select( lit(<span class="literal">null</span>) as <span class="string">"city"</span>,col(<span class="string">"year"</span>), col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = sales .groupBy()</span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"city"</span>, lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>)) </span><br><span class="line">city_year.union(city).union(year).union(none) </span><br><span class="line">.sort(desc_nulls_first(<span class="string">"city"</span>), desc_nulls_first(<span class="string">"year"</span>)) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h2><p>这里也是以案例开始，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> expenses = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line">((<span class="number">2012</span>, <span class="type">Month</span>.<span class="type">DECEMBER</span>, <span class="number">12</span>), <span class="number">5</span>), </span><br><span class="line"> ((<span class="number">2016</span>, <span class="type">Month</span>.<span class="type">AUGUST</span>, <span class="number">13</span>), <span class="number">10</span>), </span><br><span class="line"> ((<span class="number">2017</span>, <span class="type">Month</span>.<span class="type">MAY</span>, <span class="number">27</span>), <span class="number">15</span>)) </span><br><span class="line">.map &#123; <span class="keyword">case</span> ((yy, mm, dd), a) =&gt; (<span class="type">LocalDate</span>.of(yy, mm, dd), a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (d.toString, a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (<span class="type">Date</span>.valueOf(d), a) &#125;).toDF(<span class="string">"date"</span>, <span class="string">"amount"</span>)</span><br><span class="line"><span class="comment">// rollup time!</span></span><br><span class="line"><span class="keyword">val</span> res = expenses </span><br><span class="line">.rollup(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p>
<p>这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> year_month = expenses</span><br><span class="line">.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>)</span><br><span class="line"><span class="keyword">val</span> yearOnly = expenses.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"year"</span>), lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = expenses.groupBy() </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"year"</span>, lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line">year_month.union(yearOnly).union(none) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="pivot"><a href="#pivot" class="headerlink" title="pivot"></a>pivot</h2><p>旋转操作<br><a href="https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw</a><br>Pivot 算子是 spark 1.6 版本开始引入的，在 spark2.4版本中功能做了增强，还是比较强大的，做过数据清洗ETL工作的都知道，行列转换是一个常见的数据整理需求。spark 中的Pivot 可以根据枢轴点(Pivot Point) 把多行的值归并到一行数据的不同列，这个估计不太好理解，我们下面使用例子说明，看看pivot 这个算子在处理复杂数据时候的威力。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line">(<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">150</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Toronto"</span>, <span class="number">2017</span>, <span class="number">50</span>,<span class="string">"Toronto"</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>,<span class="string">"test"</span>)</span><br><span class="line">sales.groupBy(<span class="string">"year"</span>)</span><br><span class="line">.pivot(<span class="string">"city"</span>,<span class="type">Seq</span>(<span class="string">"Warsaw"</span>,<span class="string">"Boston"</span>,<span class="string">"Toronto"</span>)) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p>
<p>思路就是首先对year分组，然后旋转city字段，只取:<br>“Warsaw”,”Boston”,”Toronto”<br>然后对amount进行聚合操作<br><strong>案例：</strong><br>使用Pivot 来统计天气走势。<br>下面是西雅图的天气数据表，每行代表一天的天气最高值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  <span class="type">Date</span>         <span class="type">Temp</span> (°<span class="type">F</span>)   </span><br><span class="line"><span class="number">07</span><span class="number">-22</span><span class="number">-2018</span>	<span class="number">86</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-23</span><span class="number">-2018</span>	<span class="number">90</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-24</span><span class="number">-2018</span>	<span class="number">91</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-25</span><span class="number">-2018</span>	<span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-26</span><span class="number">-2018</span>	<span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-27</span><span class="number">-2018</span>	<span class="number">88</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-28</span><span class="number">-2018</span>	<span class="number">85</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-29</span><span class="number">-2018</span>	<span class="number">94</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-30</span><span class="number">-2018</span>	<span class="number">89</span></span><br></pre></td></tr></table></figure></p>
<p>如果我们想看下最近几年的天气走势，如果这样一天一行数据，是很难看出趋势来的，最直观的方式是 按照年来分行，然后每一列代表一个月的平均天气，这样一行数据，就可以看到这一年12个月的一个天气走势，下面我们使用 pivot 来构造这样一个查询结果：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (</span><br><span class="line">      <span class="number">1</span> JAN, <span class="number">2</span> FEB, <span class="number">3</span> MAR, <span class="number">4</span> APR, <span class="number">5</span> MAY, <span class="number">6</span> JUN, <span class="number">7</span> JUL, </span><br><span class="line">      <span class="number">8</span> AUG, <span class="number">9</span> SEP, <span class="number">10</span> <span class="keyword">OCT</span>, <span class="number">11</span> NOV, <span class="number">12</span> <span class="built_in">DEC</span></span><br><span class="line">    )</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p>
<p>结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult1.png" alt=""><br>是不是很直观，第一行就代表 2018 年，从1月到12月的平均天气，能看出一年的天气走势。<br>我们来看下这个 sql 是怎么玩的，首先是一个 子查询语句，我们最终关心的是 年份，月份，和最高天气值，所以先使用子查询对原始数据进行处理，从日期里面抽取出来年份和月份。</p>
<p>下面在子查询的结果上使用 pivot 语句，pivot 第一个参数是一个聚合语句，这个代表聚合出来一个月30天的一个平均气温，第二个参数是 FOR month，这个是指定以哪个列为枢轴列，第三个 In 子语句指定我们需要进行行列转换的具体的 枢轴点(Pivot Point)的值，上面的例子中 1到12月份都包含了，而且给了一个别名，如果只指定 1到6月份，结果就如下了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult2.png" alt=""><br>上面sql语句里面有个特别的点需要注意， 就是聚合的时候有个隐含的维度字段，就是 年份，按理来讲，我们没有写  group-by year， 为啥结果表里面不同年份区分在了不同的行，原因是，FORM 子查询出来的每行有 3个列， year，month，tmp，如果一个列既不出现在进行聚合计算的列中（temp 是聚合计算的列）， 也不作为枢轴列 ， 就会作为聚合的时候一个隐含的维度。我们的例子中算平均值聚合操作的维度是 (year, month)，一个是隐含维度，一个是 枢轴列维度， 这一点一定要注意，如果不需要按照 year 来区分，FORM 查询的时候就不要加上这个列。<br><strong><em>指定多个聚合语句</em></strong><br>上文中只有一个聚合语句，就是计算平均天气，其实是可以加多个聚合语句的，比如我们需要看到 7，8，9 月份每个月的最大气温和平均气温，就可以用以下SQL语句。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">avg</span>, </span><br><span class="line">    <span class="keyword">max</span>(temp) <span class="keyword">max</span> <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN, <span class="number">7</span> JUL, <span class="number">8</span> AUG, <span class="number">9</span> SEP)</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p>
<p>上文中指定了两个聚合语句，查询后， 枢轴点(Pivot Point)  和 聚合语句 的笛卡尔积作为结果的不同列，也就是  <value>_<aggexpr>， 看下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult3.png" alt=""><br><strong><em>聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处</em></strong><br>现在假如我们有西雅图每天的最低温数据，我们需要把最高温和最低温放在同一张表里面看对比着看.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Date</span>	   <span class="type">Temp</span> (°<span class="type">F</span>)</span><br><span class="line">…	        …</span><br><span class="line"><span class="number">08</span><span class="number">-01</span><span class="number">-2018</span>	<span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-02</span><span class="number">-2018</span>	<span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-03</span><span class="number">-2018</span>	<span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-04</span><span class="number">-2018</span>	<span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-05</span><span class="number">-2018</span>	<span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-06</span><span class="number">-2018</span>	<span class="number">59</span></span><br><span class="line">…	         …</span><br></pre></td></tr></table></figure></aggexpr></value></p>
<p>我们使用  UNION ALL 把两张表做一个合并：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'H'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  high_temps </span><br><span class="line"><span class="keyword">UNION</span> ALL </span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'L'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  low_temps;</span><br></pre></td></tr></table></figure></p>
<p>现在使用 pivot 来进行处理：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN,<span class="number">7</span> JUL,<span class="number">8</span> AUG , <span class="number">9</span> SEP)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span> ,<span class="string">`H/L`</span> <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure></p>
<p>我们统计了 4年中  7，8，9 月份最低温和最高温的平均值，这里要注意的是，我们把 year 和 一个最低最高的标记（H/L）都作为隐含维度，不然算出来的就是最低最高温度在一起的平均值了。结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult4.png" alt=""><br>上面的查询中，我们把最低最高的标记（H/L）也作为了一个隐含维度，group-by 的维度就变成了 （year， H/L, month）, 但是year 和 H/L 体现在了不同行上面，month 体现在了不同的列上面，这就是  聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处。</p>
<p>再接再厉，我们把 H/L 作为  枢轴列（Pivot Columns） 进行查询：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> (<span class="keyword">month</span>,flag) <span class="keyword">in</span> (</span><br><span class="line">(<span class="number">6</span>,<span class="string">'H'</span>) JUN_hi,(<span class="number">6</span>,<span class="string">'L'</span>) JUN_lo,</span><br><span class="line">(<span class="number">7</span>,<span class="string">'H'</span>) JUl_hi,(<span class="number">7</span>,<span class="string">'L'</span>) JUl_lo,</span><br><span class="line">(<span class="number">8</span>,<span class="string">'H'</span>) AUG_hi,(<span class="number">8</span>,<span class="string">'L'</span>) AUG_lo,</span><br><span class="line">(<span class="number">9</span>,<span class="string">'H'</span>) SEP_hi,(<span class="number">9</span>,<span class="string">'L'</span>) SEP_lo</span><br><span class="line">)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p>
<p>结果的展现方式就和上面的不同了，虽然每个单元格值都是相同的，但是把  H/L 和 month 笛卡尔积作为列，H/L 维度体现在了列上面了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult5.png" alt="">  </p>
<h1 id="源码-Spark-SQL-分区特性第一弹"><a href="#源码-Spark-SQL-分区特性第一弹" class="headerlink" title="源码:Spark SQL 分区特性第一弹"></a>源码:Spark SQL 分区特性第一弹</h1><h2 id="常见RDD分区"><a href="#常见RDD分区" class="headerlink" title="常见RDD分区"></a>常见RDD分区</h2><p>Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。<br>举几个浪尖在星球里分享比较多的例子，比如：<br>Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。<br>Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。<br>普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。<br>这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。  </p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>首先是由Seq数据集合生成一个Dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">80</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">130</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">160</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">170</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">180</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">20</span>)</span><br><span class="line">   )).toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p>
<p>将Dataset存处为partquet格式的hive表，分两种情况：<br>用city和year字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>,<span class="string">"year"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCityAndYear"</span>)</span><br></pre></td></tr></table></figure></p>
<p>用city字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCity"</span>)</span><br></pre></td></tr></table></figure></p>
<p>读取数据采用的是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> res = spark.read.parquet(<span class="string">"/user/hive/warehouse/parquettestcity"</span>)</span><br></pre></td></tr></table></figure></p>
<p>直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。   </p>
<h3 id="1-spark-default-parallelism-40"><a href="#1-spark-default-parallelism-40" class="headerlink" title="1. spark.default.parallelism =40"></a>1. spark.default.parallelism =40</h3><p>Dataset的分区数是由参数：<br>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行<br>这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。   </p>
<h3 id="2-spark-default-parallelism-4"><a href="#2-spark-default-parallelism-4" class="headerlink" title="2. spark.default.parallelism =4"></a>2. spark.default.parallelism =4</h3><p>Dataset的分区数是由参数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">"partition size = "</span>+res.rdd.partitions.length)</span><br></pre></td></tr></table></figure></p>
<p>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。<br><img src="/2019/01/04/sparksql详解/images/dataset分区.jpg" alt=""><br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。</p>
<p>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createNonBucketedReadRDD</span></span>(</span><br><span class="line">                                       readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">                                       selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">                                       fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">     selectedPartitions 的大小代表目录数目</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(<span class="string">"selectedPartitions.size : "</span>+ selectedPartitions.size)</span><br><span class="line">   <span class="keyword">val</span> defaultMaxSplitBytes =</span><br><span class="line">     fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br><span class="line">   <span class="keyword">val</span> openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br><span class="line"></span><br><span class="line">   <span class="comment">// spark.default.parallelism</span></span><br><span class="line">   <span class="keyword">val</span> defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 计算文件总大小，单位字节数</span></span><br><span class="line">   <span class="keyword">val</span> totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br><span class="line"></span><br><span class="line">   <span class="comment">//计算平均每个并行度读取数据大小</span></span><br><span class="line">   <span class="keyword">val</span> bytesPerCore = totalBytes / defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值</span></span><br><span class="line">   <span class="comment">// 然后，比较spark.sql.files.maxPartitionBytes 取小者</span></span><br><span class="line">   <span class="keyword">val</span> maxSplitBytes = <span class="type">Math</span>.min(defaultMaxSplitBytes, <span class="type">Math</span>.max(openCostInBytes, bytesPerCore))</span><br><span class="line">   logInfo(<span class="string">s"Planning scan with bin packing, max size: <span class="subst">$maxSplitBytes</span> bytes, "</span> +</span><br><span class="line">     <span class="string">s"open cost is considered as scanning <span class="subst">$openCostInBytes</span> bytes."</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 这对目录遍历</span></span><br><span class="line">   <span class="keyword">val</span> splitFiles = selectedPartitions.flatMap &#123; partition =&gt;</span><br><span class="line">     partition.files.flatMap &#123; file =&gt;</span><br><span class="line">       <span class="keyword">val</span> blockLocations = getBlockLocations(file)</span><br><span class="line"></span><br><span class="line">       <span class="comment">//判断文件类型是否支持分割，以parquet为例，是支持分割的</span></span><br><span class="line">       <span class="keyword">if</span> (fsRelation.fileFormat.isSplitable(</span><br><span class="line">         fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// eg. 0 until 2不包括 2。相当于</span></span><br><span class="line">    <span class="comment">// println(0 until(10) by 3) 输出 Range(0, 3, 6, 9)</span></span><br><span class="line">         (<span class="number">0</span>L until file.getLen by maxSplitBytes).map &#123; offset =&gt;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 计算文件剩余的量</span></span><br><span class="line">           <span class="keyword">val</span> remaining = file.getLen - offset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区</span></span><br><span class="line">           <span class="keyword">val</span> size = <span class="keyword">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="keyword">else</span> remaining</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 位置信息</span></span><br><span class="line">           <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, offset, size)</span><br><span class="line">           <span class="type">PartitionedFile</span>(</span><br><span class="line">             partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// 不可分割的话，那即是一个文件一个分区</span></span><br><span class="line">         <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, <span class="number">0</span>, file.getLen)</span><br><span class="line">         <span class="type">Seq</span>(<span class="type">PartitionedFile</span>(</span><br><span class="line">           partition.values, file.getPath.toUri.toString, <span class="number">0</span>, file.getLen, hosts))</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;.toArray.sortBy(_.length)(implicitly[<span class="type">Ordering</span>[<span class="type">Long</span>]].reverse)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> partitions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">FilePartition</span>]</span><br><span class="line">   <span class="keyword">val</span> currentFiles = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">PartitionedFile</span>]</span><br><span class="line">   <span class="keyword">var</span> currentSize = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Close the current partition and move to the next. */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">closePartition</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">if</span> (currentFiles.nonEmpty) &#123;</span><br><span class="line">       <span class="keyword">val</span> newPartition =</span><br><span class="line">         <span class="type">FilePartition</span>(</span><br><span class="line">           partitions.size,</span><br><span class="line">           currentFiles.toArray.toSeq) <span class="comment">// Copy to a new Array.</span></span><br><span class="line">       partitions += newPartition</span><br><span class="line">     &#125;</span><br><span class="line">     currentFiles.clear()</span><br><span class="line">     currentSize = <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Assign files to partitions using "Next Fit Decreasing"</span></span><br><span class="line">   splitFiles.foreach &#123; file =&gt;</span><br><span class="line">     <span class="keyword">if</span> (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class="line">       closePartition()</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Add the given file to the current partition.</span></span><br><span class="line">     currentSize += file.length + openCostInBytes</span><br><span class="line">     currentFiles += file</span><br><span class="line">   &#125;</span><br><span class="line">   closePartition()</span><br><span class="line"></span><br><span class="line">   println(<span class="string">"FileScanRDD partitions size : "</span>+partitions.size)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, partitions)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="找到一列中的中位数"><a href="#找到一列中的中位数" class="headerlink" title="找到一列中的中位数"></a>找到一列中的中位数</h1><p><a href="https://spark.apache.org/docs/latest/api/sql/index.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/api/sql/index.html</a><br>df函数： approxQuantile<br>sql函数： percentile_approx  </p>
<h1 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h1><p>ServiceLoader<br><a href="https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w</a><br>ServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。<br>要配置在相关项目的固定目录下：<br>resources/META-INF/services/接口全称。<br>这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：<br>SparkContext类<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getClusterManager</span></span>(url: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">ExternalClusterManager</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="keyword">val</span> serviceLoaders =</span><br><span class="line">      <span class="type">ServiceLoader</span>.load(classOf[<span class="type">ExternalClusterManager</span>], loader).asScala.filter(_.canCreate(url))</span><br><span class="line">    <span class="keyword">if</span> (serviceLoaders.size &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">        <span class="string">s"Multiple external cluster managers registered for the url <span class="subst">$url</span>: <span class="subst">$serviceLoaders</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    serviceLoaders.headOption</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>配置是在<br><img src="/2019/01/04/sparksql详解/images/ExternalClusterManager.png" alt=""><br>spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。<br>简单测试<br>首先实现一个接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="comment">//可以制定实现类名加载</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后将接口配置在resources/META-INF/services/<br>bigdata.spark.services.DoSomething文件<br>内容：<br>实现该接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SayHello</span> <span class="keyword">implements</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="string">"SayHello"</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(<span class="string">"hello !!!"</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>测试</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceLoader;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line">   <span class="keyword">static</span> ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class);</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">       <span class="keyword">for</span>(DoSomething sayhello : loader)&#123;</span><br><span class="line">        <span class="comment">//要加载的类名称我们可以制定</span></span><br><span class="line">           <span class="keyword">if</span>(sayhello.shortName().equalsIgnoreCase(<span class="string">"SayHello"</span>))&#123;</span><br><span class="line">               sayhello.doSomeThing();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个主要是为讲自定义数据源作准备。    </p>
<p><a href="https://articles.zsxq.com/id_702s32f46zet.html" target="_blank" rel="noopener">https://articles.zsxq.com/id_702s32f46zet.html</a><br>首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Given a provider name, look up the data source class definition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lookupDataSource</span></span>(provider: <span class="type">String</span>): <span class="type">Class</span>[_] = &#123;</span><br><span class="line">    <span class="keyword">val</span> provider1 = backwardCompatibilityMap.getOrElse(provider, provider)</span><br><span class="line">    <span class="comment">// 指定路径加载，默认加载类名是DefaultSource</span></span><br><span class="line">    <span class="keyword">val</span> provider2 = <span class="string">s"<span class="subst">$provider1</span>.DefaultSource"</span></span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="comment">// ServiceLoader加载</span></span><br><span class="line">    <span class="keyword">val</span> serviceLoader = <span class="type">ServiceLoader</span>.load(classOf[<span class="type">DataSourceRegister</span>], loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// the provider format did not match any given registered aliases</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Try</span>(loader.loadClass(provider1)).orElse(<span class="type">Try</span>(loader.loadClass(provider2))) <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Success</span>(dataSource) =&gt;</span><br><span class="line">                <span class="comment">// Found the data source using fully qualified path</span></span><br><span class="line">                dataSource</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Failure</span>(error) =&gt;</span><br><span class="line">                <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"orc"</span> ||</span><br><span class="line">                  provider1.startsWith(<span class="string">"org.apache.spark.sql.hive.orc"</span>)) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">"The ORC data source must be used with Hive support enabled"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"avro"</span> ||</span><br><span class="line">                  provider1 == <span class="string">"com.databricks.spark.avro"</span>) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$&#123;provider1.toLowerCase&#125;</span>. Please find an Avro "</span> +</span><br><span class="line">                      <span class="string">"package at http://spark.apache.org/third-party-projects.html"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$provider1</span>. Please find packages at "</span> +</span><br><span class="line">                      <span class="string">"http://spark.apache.org/third-party-projects.html"</span>,</span><br><span class="line">                    error)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt; <span class="comment">// This one won't be caught by Scala NonFatal</span></span><br><span class="line">              <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">              <span class="keyword">val</span> className = e.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">              <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"<span class="subst">$className</span> was removed in Spark 2.0. "</span> +</span><br><span class="line">                  <span class="string">"Please check if your library is compatible with Spark 2.0"</span>, e)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> e</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> head :: <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="comment">// there is exactly one registered alias</span></span><br><span class="line">          head.getClass</span><br><span class="line">        <span class="keyword">case</span> sources =&gt;</span><br><span class="line">          <span class="comment">// There are multiple registered aliases for the input</span></span><br><span class="line">          sys.error(<span class="string">s"Multiple sources found for <span class="subst">$provider1</span> "</span> +</span><br><span class="line">            <span class="string">s"(<span class="subst">$&#123;sources.map(_.getClass.getName).mkString(", ")&#125;</span>), "</span> +</span><br><span class="line">            <span class="string">"please specify the fully qualified class name."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ServiceConfigurationError</span> <span class="keyword">if</span> e.getCause.isInstanceOf[<span class="type">NoClassDefFoundError</span>] =&gt;</span><br><span class="line">        <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">        <span class="keyword">val</span> className = e.getCause.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">        <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"Detected an incompatible DataSourceRegister. "</span> +</span><br><span class="line">            <span class="string">"Please remove the incompatible library from classpath or upgrade it. "</span> +</span><br><span class="line">            <span class="string">s"Error: <span class="subst">$&#123;e.getMessage&#125;</span>"</span>, e)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">throw</span> e</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。</p>
<p>主要思路是：</p>
<ol>
<li><p>实现DefaultSource。</p>
</li>
<li><p>实现工厂类。</p>
</li>
<li><p>实现具体的数据加载类。</p>
</li>
</ol>
<p>首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.&#123;<span class="type">DataSourceOptions</span>, <span class="type">DataSourceV2</span>, <span class="type">ReadSupport</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span>  <span class="keyword">extends</span> <span class="title">DataSourceV2</span> <span class="keyword">with</span> <span class="title">ReadSupport</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(options: <span class="type">DataSourceOptions</span>) = <span class="keyword">new</span> <span class="type">SimpleDataSourceReader</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后是，要实现DataSourceReader，负责创建阅读器工厂：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReaderFactory</span>, <span class="type">DataSourceReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span></span>() = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> factoryList = <span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">DataReaderFactory</span>[<span class="type">Row</span>]]</span><br><span class="line">    factoryList.add(<span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>())</span><br><span class="line">    factoryList</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据源的具体实现类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReader</span>, <span class="type">DataReaderFactory</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReaderFactory</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">  <span class="title">DataReaderFactory</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReader</span> </span>= <span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>()</span><br><span class="line">  <span class="keyword">val</span> values = <span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span> </span>= index &lt; values.length</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> row = <span class="type">Row</span>(values(index))</span><br><span class="line">    index = index + <span class="number">1</span></span><br><span class="line">    row</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() = <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用我们默认的数据源：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getName).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">      .set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"mt-mdh.local"</span>)</span><br><span class="line">      .set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>)</span><br><span class="line">      .setJars(<span class="type">List</span>(<span class="string">"/opt/sparkjar/bigdata.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span>))</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> simpleDf = spark.read</span><br><span class="line">      .format(<span class="string">"bigdata.spark.SparkSQL.DataSources"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    simpleDf.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。</p>
<h1 id="删除多个字段"><a href="#删除多个字段" class="headerlink" title="删除多个字段"></a>删除多个字段</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropColumns</span></span>(columns: <span class="type">Seq</span>[<span class="type">String</span>]):<span class="type">DataFAME</span>=&#123;</span><br><span class="line">  columns.foldLeft(dataFame)((df,column)=&gt; df.drop(column))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="spark-sql-窗口函数"><a href="#spark-sql-窗口函数" class="headerlink" title="spark sql 窗口函数"></a>spark sql 窗口函数</h1><p><a href="https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw</a><br>最近理了下spark sql 中窗口函数的知识，打算开几篇文章讲讲，窗口函数有很多应用场景，比如说炒股的时候有个5日移动均线，或者让你对一个公司所有销售所有部门按照销售业绩进行排名等等，都是要用到窗口函数，在spark sql中，窗口函数和聚合函数的区别，之前文章中也提到过，就是聚合函数是按照你聚合的维度，每个分组中算出来一个聚合值，而窗口函数是对每一行，都根据当前窗口（5日均线就是今日往前的5天组成的窗口），都聚合出一个值。</p>
<p>1  开胃菜，spark sql 窗口函数的基本概念和使用姿势<br>2  spark sql 中窗口函数深入理解<br>3  不是UDF，也不是UDAF，教你自定义一个窗口函数（UDWF）<br>4  从 spark sql 源码层面理解窗口函数   </p>
<h2 id="什么是简单移动平均值"><a href="#什么是简单移动平均值" class="headerlink" title="什么是简单移动平均值"></a>什么是简单移动平均值</h2><p>简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local"</span>)</span><br><span class="line">    .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">    .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 这个 window spec 中，数据根据用户(customer)来分去。</span></span><br><span class="line"><span class="comment">    * 每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行)</span></span><br><span class="line"><span class="comment">    * ，每一个滑动的窗口总用有3行</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  df.withColumn(<span class="string">"movinAvg"</span>, avg(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line">  spark.stop()</span><br></pre></td></tr></table></figure></p>
<p><strong>结果：</strong><br>2~3~2<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------------------+</span><br><span class="line">|site|     date|user_cnt|          movinAvg|</span><br><span class="line">+----+---------+--------+------------------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|              <span class="number">70.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|              <span class="number">80.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|              <span class="number">95.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|              <span class="number">65.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|<span class="number">56.666666666666664</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|              <span class="number">50.0</span>|</span><br><span class="line">+----+---------+--------+------------------+</span><br></pre></td></tr></table></figure></p>
<h2 id="窗口函数和窗口特征定义"><a href="#窗口函数和窗口特征定义" class="headerlink" title="窗口函数和窗口特征定义"></a>窗口函数和窗口特征定义</h2><p>正如上述例子中，窗口函数主要包含两个部分：</p>
<p> 指定窗口特征（wSpec）：</p>
<p>“partitionyBY” 定义数据如何分组；在上面的例子中，是用户 site</p>
<p>“orderBy” 定义分组中的排序</p>
<p>“rowsBetween” 定义窗口的大小</p>
<p>指定窗口函数函数：</p>
<p>指定窗口函数函数，你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数.</p>
<h2 id="累计汇总"><a href="#累计汇总" class="headerlink" title="累计汇总"></a>累计汇总</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .master(<span class="string">"local"</span>)</span><br><span class="line">  .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">  .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line"><span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">"cumsum"</span>, sum(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------+</span><br><span class="line">|site|     date|user_cnt|cumsum|</span><br><span class="line">+----+---------+--------+------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">140</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">130</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">170</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">170</span>|</span><br><span class="line">+----+---------+--------+------+</span><br></pre></td></tr></table></figure></p>
<h2 id="前一行数据"><a href="#前一行数据" class="headerlink" title="前一行数据"></a>前一行数据</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"preUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>),<span class="number">1</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure>
<p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----------+</span><br><span class="line">|site|     date|user_cnt|preUserCnt|</span><br><span class="line">+----+---------+--------+----------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|        <span class="number">50</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|        <span class="number">90</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|        <span class="number">70</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|        <span class="number">60</span>|</span><br><span class="line">+----+---------+--------+----------+</span><br></pre></td></tr></table></figure></p>
<p>如果计算环比的时候，是不是特别有用啊？！</p>
<p>在介绍几个常用的行数：</p>
<p>first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到</p>
<p>lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件</p>
<h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"rank"</span>, rank().over(wSpce)).show()</span><br></pre></td></tr></table></figure>
<p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----+</span><br><span class="line">|site|     date|user_cnt|rank|</span><br><span class="line">+----+---------+--------+----+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">3</span>|</span><br><span class="line">+----+---------+--------+----+</span><br></pre></td></tr></table></figure></p>
<p>这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。</p>
<h2 id="自定义窗口函数"><a href="#自定义窗口函数" class="headerlink" title="自定义窗口函数"></a>自定义窗口函数</h2><p><a href="https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ</a><br><a href="https://github.com/zheniantoushipashi/spark-udwf-session" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session</a>  </p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在使用 spark sql 的时候，有时候默认提供的sql 函数可能满足不了需求，这时候可以自定义一些函数，可以自定义 UDF 或者UDAF。</p>
<p>UDF 在sql中只是简单的处理转换一些字段，类似默认的trim 函数把一个字符串类型的列的头尾空格去掉， UDAF函数不同于UDF，是在sql聚合语句中使用的函数，必须配合 GROUP BY 一同使用，类似默认提供的count，sum函数，但是还有一种自定义函数叫做 UDWF， 这种一般人就不知道了，这种叫做窗口自定义函数，不了解窗口函数的，可以参考 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势 ，或者官方的介绍 <a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a>    </p>
<p>窗口函数是 SQL 中一类特别的函数。和聚合函数相似，<strong>窗口函数的输入也是多行记录</strong>。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口</p>
<p>这里怎么理解一个窗口呢，spark君在这里得好好的解释解释，一个窗口是怎么定义的，</p>
<p>窗口语句中，partition by用来指定分区的列，在同一个分区的行属于同一个窗口</p>
<p>order by用来指定窗口内的多行，如何排序</p>
<p>windowing_clause 用来指定开窗方式，在spark sql 中开窗方式有那么几种    </p>
<ul>
<li><p>一个分区中的所有行作为一个窗口:UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING（上下都没有边界)，这种情况下，spark sql 会把所有行作为一个输入，进行一次求值</p>
</li>
<li><p>Growing frame:UNBOUNDED PRECEDING AND ….（上无边界）, 这种就是不断的把当前行加入的窗口中，而不删除， 例子：.rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行，在数据迭代过程中，不断的把当前行加入的窗口中。</p>
</li>
<li><p>Shrinking frame：… AND UNBOUNDED FOLLOWING（下无边界）和Growing frame 相反，窗口不断的把迭代到的当前行从窗口中删除掉。</p>
</li>
<li><p>Moving frame：滑动的窗口，举例：.rowsBetween(-1, 1) 就是指窗口定义从 -1(当前行前一行)到 1(当前行后一行) ，每一个滑动的窗口总用有3行</p>
</li>
<li><p>Offset frame： 窗口中只有一条数据，就是偏移当前行一定距离的那一行，举例：lag(field, n): 就是取从当前行往前的n个行的字段值</p>
</li>
</ul>
<p>这里就针对窗口函数就介绍这么多，如果不懂请参考相关文档，加强理解，我们在平时使用 spark sql 的过程中，会发现有很多教你自定义 UDF 和 UDAF 的教程，却没有针对UDWF的教程，这是为啥呢，这是因为 UDF 和UDAF 都作为上层API暴露给用户了，使用scala很简单就可以写一个函数出来，但是UDWF没有对上层用户暴露，只能使用 Catalyst expressions. 也就是Catalyst框架底层的表达式语句才可以定义，如果没有对源码有很深入的研究，根本就搞不出来。spark 君在工作中写了一些UDWF的函数，但是都比较复杂，不太好单独抽出来作为一个简明的例子给大家讲解，挑一个网上的例子来进行说明，这个例子 spark君亲测可用。</p>
<h3 id="窗口函数的使用场景"><a href="#窗口函数的使用场景" class="headerlink" title="窗口函数的使用场景"></a>窗口函数的使用场景</h3><p>我们来举个实际例子来说明 窗口函数的使用场景，在网站的统计指标中，有一个概念叫做用户会话，什么叫做用户会话呢，我来说明一下，我们在网站服务端使用用户session来管理用户状态，过程如下</p>
<p>1) 服务端session是用户第一次访问应用时，服务器就会创建的对象，代表用户的一次会话过程，可以用来存放数据。服务器为每一个session都分配一个唯一的sessionid，以保证每个用户都有一个不同的session对象。</p>
<p>2）服务器在创建完session后，会把sessionid通过cookie返回给用户所在的浏览器，这样当用户第二次及以后向服务器发送请求的时候，就会通过cookie把sessionid传回给服务器，以便服务器能够根据sessionid找到与该用户对应的session对象。</p>
<p>3）session通常有失效时间的设定，比如1个小时。当失效时间到，服务器会销毁之前的session，并创建新的session返回给用户。但是只要用户在失效时间内，有发送新的请求给服务器，通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长1个小时。</p>
<p>也就是说如果用户在1个超过一个小时不产生用户事件，当前会话就结束了，如果后续再产生用户事件，就当做新的用户会话，我们现在就使用spark sql 来统计用户的会话数，首先我们先加一个列，作为当前列的session，然后再 distinct 这个列就得到用户的会话数了，所以关键是怎么加上这个newSession 这个列，这种场景就很适合使用窗口函数来做统计，因为判断当前是否是一个新会话的依据，需要依赖当前行的前一行的时间戳和当前行的时间戳的间隔来判断，下面的表格可以帮助你理解这个概念，例子中有3列数据，用户，event字段代表用户访问了一个页面产生了一个用户事件，time字段代表访问页面的时间戳：  </p>
<table>
<thead>
<tr>
<th>user</th>
<th>event</th>
<th>time</th>
<th>session </th>
</tr>
</thead>
<tbody>
<tr>
<td>user1</td>
<td>page1</td>
<td>10:12</td>
<td>session1(new session)</td>
<td></td>
</tr>
<tr>
<td>user1</td>
<td>page2</td>
<td>10:20</td>
<td>session1(same session,8 minutes from last event)</td>
<td></td>
</tr>
<tr>
<td>user1</td>
<td>page1</td>
<td>11:13</td>
<td>session1(same session,53 minutes from last event)</td>
<td></td>
</tr>
<tr>
<td>user1</td>
<td>page3</td>
<td>14:12</td>
<td>session1(new session,3 minutes from last event)</td>
<td></td>
</tr>
</tbody>
</table>
<p>上面只有一个用户，如果多个用户，可以使用 partition by 来进行分区。</p>
<h3 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h3><p><strong>构造数据</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActivityData</span>(<span class="params">user:<span class="type">String</span>, ts:<span class="type">Long</span>, session:<span class="type">String</span></span>)   </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">st</span> </span>= <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">  <span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="string">"a CustomWindowFunction"</span> should <span class="string">"correctly create a session "</span> in &#123;</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.createDataFrame(sc.parallelize(d))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">    <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br></pre></td></tr></table></figure></p>
<p>怎么使用 spark sql 来统计会话数目呢，因为不同用户产生的是不同的会话，首先使用user字段进行分区，然后按照时间戳进行排序，然后我们需要一个自定义函数来加一个列，这个列的值的逻辑如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IF (no previous event) <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> (<span class="keyword">if</span> cerrent <span class="keyword">event</span> was past <span class="keyword">session</span> window)</span><br><span class="line"><span class="keyword">THEN</span> <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> <span class="keyword">use</span> <span class="keyword">current</span> <span class="keyword">session</span></span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><img src="/2019/01/04/sparksql详解/./images/UDWFResult.jpg" alt=""><br>我们使用 UUID 来作为会话id， 当后一行的时间戳和前一行的时间戳间隔大于1小时的时候，就创建一个新的会话id作为列值，否则使用老的会话id作为列值。</p>
<p>这种就涉及到状态，我们在内部需要维护的状态数据</p>
<ul>
<li><p>当前的session ID</p>
</li>
<li><p>当前session的最后活动事件的时间戳</p>
</li>
</ul>
<p>下面我们就看看这个怎样自定义 caculateSession 这个窗口函数，先自定义一个静态对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Column</span>, <span class="type">Row</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">Add</span>, <span class="type">AggregateWindowFunction</span>, <span class="type">AttributeReference</span>, <span class="type">Expression</span>, <span class="type">If</span>, <span class="type">IsNotNull</span>, <span class="type">LessThanOrEqual</span>, <span class="type">Literal</span>, <span class="type">ScalaUDF</span>, <span class="type">Subtract</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.unsafe.types.<span class="type">UTF8String</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyUDWF</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> defaultMaxSessionLengthms = <span class="number">3600</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SessionUDWF</span>(<span class="params">timestamp:<span class="type">Expression</span>, session:<span class="type">Expression</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                         sessionWindow:<span class="type">Expression</span> = <span class="type">Literal</span>(defaultMaxSessionLengthms</span>)) <span class="keyword">extends</span> <span class="title">AggregateWindowFunction</span> </span>&#123;</span><br><span class="line">    self: <span class="type">Product</span> =&gt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">children</span></span>: <span class="type">Seq</span>[<span class="type">Expression</span>] = <span class="type">Seq</span>(timestamp, session)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> zero = <span class="type">Literal</span>( <span class="number">0</span>L )</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> nullString = <span class="type">Literal</span>(<span class="literal">null</span>:<span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> curentSession = <span class="type">AttributeReference</span>(<span class="string">"currentSession"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>)()</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> previousTs =    <span class="type">AttributeReference</span>(<span class="string">"lastTs"</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>)()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> aggBufferAttributes: <span class="type">Seq</span>[<span class="type">AttributeReference</span>] =  curentSession  :: previousTs :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> assignSession =  <span class="type">If</span>(<span class="type">LessThanOrEqual</span>(<span class="type">Subtract</span>(timestamp, aggBufferAttributes(<span class="number">1</span>)), sessionWindow),</span><br><span class="line">      aggBufferAttributes(<span class="number">0</span>), <span class="comment">// if</span></span><br><span class="line">      <span class="type">ScalaUDF</span>( createNewSession, <span class="type">StringType</span>, children = <span class="type">Nil</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> initialValues: <span class="type">Seq</span>[<span class="type">Expression</span>] =  nullString :: zero :: <span class="type">Nil</span></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> updateExpressions: <span class="type">Seq</span>[<span class="type">Expression</span>] =</span><br><span class="line">      <span class="type">If</span>(<span class="type">IsNotNull</span>(session), session, assignSession) ::</span><br><span class="line">        timestamp ::</span><br><span class="line">        <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> evaluateExpression: <span class="type">Expression</span> = aggBufferAttributes(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">prettyName</span></span>: <span class="type">String</span> = <span class="string">"makeSession"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">val</span>  createNewSession = () =&gt; org.apache.spark.unsafe.types.<span class="type">UTF8String</span>.fromString(<span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, <span class="type">Literal</span>(defaultMaxSessionLengthms)) &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>, sessionWindow:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, sessionWindow.expr) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withExpr</span></span>(expr: <span class="type">Expression</span>): <span class="type">Column</span> = <span class="keyword">new</span> <span class="type">Column</span>(expr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>代码说明：</p>
<ul>
<li><p>状态保存在 Seq[AttributeReference]中</p>
</li>
<li><p>重写 initialValues方法进行初始化</p>
</li>
<li><p>updateExpressions 函数针对每一行数据都会调用</p>
</li>
</ul>
<blockquote>
<p>spark sql 在迭代处理每一行数据的时候，都会调用 updateExpressions 函数来处理，根据当后一行的时间戳和前一行的时间戳间隔大于1小时来进行不同的逻辑处理，如果不大于，就使用 aggBufferAttributes(0) 中保存的老的sessionid，如果大于，就把 createNewSession 包装为一个scalaUDF作为一个子表达式来创建一个新的sessionID，并且每次都把当前行的时间戳作为用户活动的最后时间戳。</p>
</blockquote>
<p>最后包装为静态对象的方法，就可以在spark sql中使用这个自定义窗口函数了，下面是两个重载的方法，一个最大间隔时间使用默认值，一个可以运行用户自定义，perfect</p>
<p>现在，我们就可以拿来用在我们的main函数中了。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> st = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = spark.createDataFrame(sc.parallelize(d))</span><br><span class="line">  <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">  <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br><span class="line">  df.show(<span class="number">20</span>)</span><br><span class="line">  res.show(<span class="number">20</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果我们学会了自定义 spark 窗口函数，原理是就可以处理一切这种对前后数据依赖的统计需求，不过这种自定义毕竟需要对 spark 源码有很深入的研究才可以，这就需要功力了，希望 spark君的读者可以跟着spark君日益精进。文章中的代码可能不太清楚，完整demo参考  <a href="https://github.com/zheniantoushipashi/spark-udwf-session。" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session。</a></p>

      
    </div>
    
    
    
     <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">
  <p><span>本文标题:</span><a href="/2019/01/04/sparksql详解/">sparksql详解</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 tang 的个人博客">tang</a></p>
  <p><span>发布时间:</span>2019年01月04日 - 19:01</p>
  <p><span>最后更新:</span>2019年02月26日 - 15:02</p>
  <p><span>原始链接:</span><a href="/2019/01/04/sparksql详解/" title="sparksql详解">https://tgluon.github.io/2019/01/04/sparksql详解/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://tgluon.github.io/2019/01/04/sparksql详解/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>


      
     </div>
     <div>
     
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>


     
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/27/注解详解/" rel="next" title="java自定义注解">
                <i class="fa fa-chevron-left"></i> java自定义注解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/01/04/scala通过反射获取当前类类名/" rel="prev" title="scala通过反射获取当前类类名">
                scala通过反射获取当前类类名 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="tang" />
            
              <p class="site-author-name" itemprop="name">tang</p>
              <p class="site-description motion-element" itemprop="description">火星度假村追梦程序员。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/tgluon" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL的Dataset基本操作"><span class="nav-number">1.</span> <span class="nav-text">Spark SQL的Dataset基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本简介和准备工作"><span class="nav-number">1.1.</span> <span class="nav-text">基本简介和准备工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#select"><span class="nav-number">1.2.</span> <span class="nav-text">select</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#selectExpr"><span class="nav-number">1.3.</span> <span class="nav-text">selectExpr</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#filter"><span class="nav-number">1.4.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#where"><span class="nav-number">1.5.</span> <span class="nav-text">where</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#group-by"><span class="nav-number">1.6.</span> <span class="nav-text">group by</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#union"><span class="nav-number">1.7.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#join"><span class="nav-number">1.8.</span> <span class="nav-text">join</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#order-by"><span class="nav-number">1.9.</span> <span class="nav-text">order by</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sort"><span class="nav-number">1.10.</span> <span class="nav-text">sort</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sortwithinpartition"><span class="nav-number">1.11.</span> <span class="nav-text">sortwithinpartition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#withColumn"><span class="nav-number">1.12.</span> <span class="nav-text">withColumn</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreach"><span class="nav-number">1.13.</span> <span class="nav-text">foreach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreachPartition"><span class="nav-number">1.14.</span> <span class="nav-text">foreachPartition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#distinct"><span class="nav-number">1.15.</span> <span class="nav-text">distinct</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropDuplicates"><span class="nav-number">1.16.</span> <span class="nav-text">dropDuplicates</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#drop"><span class="nav-number">1.17.</span> <span class="nav-text">drop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#printSchema"><span class="nav-number">1.18.</span> <span class="nav-text">printSchema</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#explain"><span class="nav-number">1.19.</span> <span class="nav-text">explain()</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL入门到精通之第二篇Dataset的复杂操作"><span class="nav-number">2.</span> <span class="nav-text">Spark SQL入门到精通之第二篇Dataset的复杂操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cube"><span class="nav-number">2.1.</span> <span class="nav-text">cube</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rollup"><span class="nav-number">2.2.</span> <span class="nav-text">rollup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pivot"><span class="nav-number">2.3.</span> <span class="nav-text">pivot</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#源码-Spark-SQL-分区特性第一弹"><span class="nav-number">3.</span> <span class="nav-text">源码:Spark SQL 分区特性第一弹</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#常见RDD分区"><span class="nav-number">3.1.</span> <span class="nav-text">常见RDD分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#准备数据"><span class="nav-number">3.2.</span> <span class="nav-text">准备数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-spark-default-parallelism-40"><span class="nav-number">3.2.1.</span> <span class="nav-text">1. spark.default.parallelism =40</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-spark-default-parallelism-4"><span class="nav-number">3.2.2.</span> <span class="nav-text">2. spark.default.parallelism =4</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#找到一列中的中位数"><span class="nav-number">4.</span> <span class="nav-text">找到一列中的中位数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自定义数据源"><span class="nav-number">5.</span> <span class="nav-text">自定义数据源</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#删除多个字段"><span class="nav-number">6.</span> <span class="nav-text">删除多个字段</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-sql-窗口函数"><span class="nav-number">7.</span> <span class="nav-text">spark sql 窗口函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是简单移动平均值"><span class="nav-number">7.1.</span> <span class="nav-text">什么是简单移动平均值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#窗口函数和窗口特征定义"><span class="nav-number">7.2.</span> <span class="nav-text">窗口函数和窗口特征定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#累计汇总"><span class="nav-number">7.3.</span> <span class="nav-text">累计汇总</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前一行数据"><span class="nav-number">7.4.</span> <span class="nav-text">前一行数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#排名"><span class="nav-number">7.5.</span> <span class="nav-text">排名</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自定义窗口函数"><span class="nav-number">7.6.</span> <span class="nav-text">自定义窗口函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#背景"><span class="nav-number">7.6.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#窗口函数的使用场景"><span class="nav-number">7.6.2.</span> <span class="nav-text">窗口函数的使用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深入研究"><span class="nav-number">7.6.3.</span> <span class="nav-text">深入研究</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tang</span>

  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
<span id="busuanzi_container_site_pv">
    本站总访问量:<span id="busuanzi_value_site_pv"></span>次
</span>
</div>
  
<!--<div class="powered-by">{
  }由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动{}</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">{
  }主题 &mdash; {
  }<a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">{
    }NexT.Pisces{
  }</a> v5.1.4{
}</div>
-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共20.7k字</span>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>


  
  <script type="text/javascript"
color="0,0,255" opacity='0.7' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://tgluon.github.io/2019/01/04/sparksql详解/';
          this.page.identifier = '2019/01/04/sparksql详解/';
          this.page.title = 'sparksql详解';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
<script type="text/javascript" src="/js/src/love.js"></script>
</html>
