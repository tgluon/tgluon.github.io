<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog</title>
  
  <subtitle>追梦青年</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tgluon.github.io/"/>
  <updated>2019-07-01T22:04:18.278Z</updated>
  <id>https://tgluon.github.io/</id>
  
  <author>
    <name>tang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)</title>
    <link href="https://tgluon.github.io/2019/07/02/idea%E4%B8%ADSpark%20on%20yarn%20%E8%BF%9C%E7%A8%8BDebug%20Spark%20%E6%BA%90%E7%A0%81(Yarn%20Client+attach%20%E6%A8%A1%E5%BC%8F)/"/>
    <id>https://tgluon.github.io/2019/07/02/idea中Spark on yarn 远程Debug Spark 源码(Yarn Client+attach 模式)/</id>
    <published>2019-07-01T22:04:18.278Z</published>
    <updated>2019-07-01T22:04:18.278Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>idea 中连接yarn集群debug spark代码,有利于定位线上问题。</p><h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>默认你hadoop环境已经搞定情况下的说明！！！<br><strong>scala版本：</strong> 2.11.8<br><strong>jdk版本</strong>：JDK1.8<br><strong>hadoop版本：</strong> 2.7.3<br><strong>spark版本：</strong> 2.4.1<br><strong>zookeeper：</strong> 3.4.6<br><strong>高能预警：</strong> 一定要保持yarn集群机器scala版本和项目pom文件中scala版本一致。     </p><table><thead><tr><th>linux版本</th><th>IP</th><th>hostname</th><th>进程 </th></tr></thead><tbody><tr><td>centos7</td><td>192.168.8.81</td><td>hadoop01</td><td>NameNode、DFSZKFailoverController</td></tr><tr><td>centos7</td><td>192.168.8.82</td><td>hadoop02</td><td>NameNode、DFSZKFailoverController、ResourceManager、JournalNode、NodeManager、DataNode、QuorumPeerMain</td></tr><tr><td>centos7</td><td>192.168.8.83</td><td>hadoop03</td><td>JournalNode、NodeManager、DataNode、QuorumPeerMain</td></tr><tr><td>centos7</td><td>192.168.8.84</td><td>hadoop04</td><td>JournalNode、NodeManager、DataNode、QuorumPeerMain</td></tr></tbody></table><p><strong>idea所在机器：</strong>   </p><table><thead><tr><th>linux版本</th><th>IP</th><th>hostname</th></tr></thead><tbody><tr><td>ubuntu18.04</td><td>192.168.8.85</td><td>hadoop05</td><td></td></tr></tbody></table><h1 id="maven项目中resources目录中配置文件"><a href="#maven项目中resources目录中配置文件" class="headerlink" title="maven项目中resources目录中配置文件"></a>maven项目中resources目录中配置文件</h1><p><strong>高能预警：</strong> 配置文件需要和hadoop集群一致</p><p><strong>具体配置文件如下：</strong><br>core-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop临时目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>hdfs-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--修改block块的大小,默认为128M--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.block.seze<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>yarn-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.embedded<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zk集群地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.client.failover-proxy-provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 两个可选值：org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore 以及 默认值org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.8.82:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打开日志聚合功能，这样才能从web界面查看日志 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 聚合日志最长保留时间 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 中间结果存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/data/localdir1,/home/hadoop/hadoop/data/localdir2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 日志存放位置。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop/data/hdfs/logdir1,/home/hadoop/hadoop/data/hdfs/logdir2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p><strong>spark-defaults.conf配置文件：</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the "License"); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line">spark.driver.extraClassPath         /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*</span><br><span class="line"></span><br><span class="line">spark.executor.extraClassPath     /home/hadoop/spark/jars/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*:/home/hadoop/hadoop/share/hadoop/yarn/lib/*</span><br><span class="line">spark.driver.extraJavaOptions     -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005</span><br><span class="line">spark.executor.extraJavaOptions   -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005</span><br><span class="line">spark.driver.memory                512m</span><br><span class="line">spark.executor.memory              512m</span><br><span class="line"># Example:</span><br><span class="line"># spark.master                     spark://master:7077</span><br><span class="line"># spark.eventLog.enabled           true</span><br><span class="line"># spark.eventLog.dir               hdfs://namenode:8021/directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</span><br></pre></td></tr></table></figure></p><h1 id="maven-pom配置文件"><a href="#maven-pom配置文件" class="headerlink" title="maven pom配置文件"></a>maven pom配置文件</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.xh.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>sparklearning<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mysql.version</span>&gt;</span>5.1.35<span class="tag">&lt;/<span class="name">mysql.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava.version</span>&gt;</span>26.0-jre<span class="tag">&lt;/<span class="name">guava.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fastjson.version</span>&gt;</span>1.2.40<span class="tag">&lt;/<span class="name">fastjson.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">zookeeper.version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">zookeeper.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--scala--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;zookeeper.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--spark--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-yarn_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;fastjson.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--hive--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--hadoop--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--mysql--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--guava--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin compiles Scala files --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-compile-first<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>process-resources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>add-source<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-test-compile<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>process-test-resources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin compiles Java files --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--跳过test begin--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">skipTests</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skipTests</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- This plugin adds all dependencies to JAR file during 'package' command.</span></span><br><span class="line"><span class="comment">            Pay EXTRA attention to the 'mainClass' tag.</span></span><br><span class="line"><span class="comment">            You have to set name of class with entry point to program ('main' method) --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifestFile</span>&gt;</span><span class="tag">&lt;/<span class="name">manifestFile</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.defonds.RsaEncryptor<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>META-INF/spring.handlers<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>META-INF/spring.schemas<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="开发程序"><a href="#开发程序" class="headerlink" title="开发程序"></a>开发程序</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xh.spark.sql.function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">          .set(<span class="string">"spark.master"</span>, <span class="string">"yarn"</span>)</span><br><span class="line">          .set(<span class="string">"spark.submit.deployMode"</span>, <span class="string">"client"</span>) <span class="comment">// 部署模式为client</span></span><br><span class="line">          .set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"hadoop02"</span>) <span class="comment">// resourcemanager主机名</span></span><br><span class="line">          .set(<span class="string">"spark.executor.instances"</span>, <span class="string">"2"</span>) <span class="comment">// Executor实例的数量</span></span><br><span class="line">          .set(<span class="string">"spark.dynamicAllocation.enabled"</span>, <span class="string">"false"</span>)</span><br><span class="line">          .setJars(<span class="type">List</span>(<span class="string">"/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar"</span>,</span><br><span class="line">            <span class="string">"/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar"</span></span><br><span class="line">          ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-01"</span>, <span class="number">50</span>),</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-02"</span>, <span class="number">45</span>),</span><br><span class="line">      (<span class="string">"站点1"</span>, <span class="string">"2018-01-03"</span>, <span class="number">55</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-01"</span>, <span class="number">25</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-02"</span>, <span class="number">29</span>),</span><br><span class="line">      (<span class="string">"站点2"</span>, <span class="string">"2018-01-03"</span>, <span class="number">27</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 简单移动平均值</span></span><br><span class="line">    <span class="comment">// API方式</span></span><br><span class="line">    <span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)，每一个滑动的窗口总用有3行</span></span><br><span class="line">    <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sql方式</span></span><br><span class="line">     df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">        spark.sql(</span><br><span class="line">          <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">            |select site,</span></span><br><span class="line"><span class="string">            |       date,</span></span><br><span class="line"><span class="string">            |       user_cnt,</span></span><br><span class="line"><span class="string">            |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">            |from   site_info</span></span><br><span class="line"><span class="string">          "</span><span class="string">""</span>.stripMargin).show()</span><br><span class="line">    </span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上确认无误之后，直接在idea里面run该程序。   </p><h1 id="执行结果如下"><a href="#执行结果如下" class="headerlink" title="执行结果如下"></a>执行结果如下</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/jvm/java-1.8.0-openjdk/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:45315,suspend=y,server=n -javaagent:/home/hadoop/idea/lib/rt/debugger-agent.jar -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-1.8.0-openjdk/jre/lib/charsets.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jce.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/jsse.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/management-agent.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/resources.jar:/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/rt.jar:/home/hadoop/worker/sparklearning/target/classes:/home/hadoop/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar:/home/hadoop/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar:/home/hadoop/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/hadoop/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/hadoop/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar:/home/hadoop/repository/org/apache/spark/spark-yarn_2.11/2.4.1/spark-yarn_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/hadoop/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/hadoop/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/hadoop/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/hadoop/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.5/hadoop-yarn-server-web-proxy-2.6.5.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/hadoop/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/hadoop/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-core_2.11/2.4.1/spark-core_2.11-2.4.1.jar:/home/hadoop/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/hadoop/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/hadoop/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/hadoop/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/hadoop/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/hadoop/repository/com/twitter/chill_2.11/0.9.3/chill_2.11-0.9.3.jar:/home/hadoop/repository/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/hadoop/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/hadoop/repository/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/hadoop/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/hadoop/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/hadoop/repository/org/apache/spark/spark-launcher_2.11/2.4.1/spark-launcher_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-kvstore_2.11/2.4.1/spark-kvstore_2.11-2.4.1.jar:/home/hadoop/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/hadoop/repository/org/apache/spark/spark-network-common_2.11/2.4.1/spark-network-common_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-network-shuffle_2.11/2.4.1/spark-network-shuffle_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-unsafe_2.11/2.4.1/spark-unsafe_2.11-2.4.1.jar:/home/hadoop/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/hadoop/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/hadoop/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/hadoop/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/hadoop/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/hadoop/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/hadoop/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/hadoop/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/hadoop/repository/org/xerial/snappy/snappy-java/1.1.7.1/snappy-java-1.1.7.1.jar:/home/hadoop/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/hadoop/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/hadoop/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/hadoop/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/repository/org/json4s/json4s-jackson_2.11/3.5.3/json4s-jackson_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-core_2.11/3.5.3/json4s-core_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-ast_2.11/3.5.3/json4s-ast_2.11-3.5.3.jar:/home/hadoop/repository/org/json4s/json4s-scalap_2.11/3.5.3/json4s-scalap_2.11-3.5.3.jar:/home/hadoop/repository/org/scala-lang/modules/scala-xml_2.11/1.0.6/scala-xml_2.11-1.0.6.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/hadoop/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/hadoop/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/hadoop/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/hadoop/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/hadoop/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/hadoop/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/hadoop/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/hadoop/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/hadoop/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/hadoop/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/hadoop/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/hadoop/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/hadoop/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/hadoop/repository/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar:/home/hadoop/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/hadoop/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/hadoop/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/hadoop/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/hadoop/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/hadoop/repository/org/apache/spark/spark-tags_2.11/2.4.1/spark-tags_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/hadoop/repository/org/apache/spark/spark-sql_2.11/2.4.1/spark-sql_2.11-2.4.1.jar:/home/hadoop/repository/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar:/home/hadoop/repository/org/apache/spark/spark-sketch_2.11/2.4.1/spark-sketch_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-catalyst_2.11/2.4.1/spark-catalyst_2.11-2.4.1.jar:/home/hadoop/repository/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar:/home/hadoop/repository/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar:/home/hadoop/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar:/home/hadoop/repository/io/airlift/aircompressor/0.10/aircompressor-0.10.jar:/home/hadoop/repository/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar:/home/hadoop/repository/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar:/home/hadoop/repository/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar:/home/hadoop/repository/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar:/home/hadoop/repository/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar:/home/hadoop/repository/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar:/home/hadoop/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/hadoop/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/hadoop/repository/org/apache/spark/spark-mllib_2.11/2.4.1/spark-mllib_2.11-2.4.1.jar:/home/hadoop/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar:/home/hadoop/repository/org/apache/spark/spark-graphx_2.11/2.4.1/spark-graphx_2.11-2.4.1.jar:/home/hadoop/repository/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar:/home/hadoop/repository/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1.jar:/home/hadoop/repository/org/apache/spark/spark-mllib-local_2.11/2.4.1/spark-mllib-local_2.11-2.4.1.jar:/home/hadoop/repository/org/scalanlp/breeze_2.11/0.13.2/breeze_2.11-0.13.2.jar:/home/hadoop/repository/org/scalanlp/breeze-macros_2.11/0.13.2/breeze-macros_2.11-0.13.2.jar:/home/hadoop/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/hadoop/repository/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar:/home/hadoop/repository/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar:/home/hadoop/repository/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar:/home/hadoop/repository/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar:/home/hadoop/repository/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar:/home/hadoop/repository/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-hive_2.11/2.4.1/spark-hive_2.11-2.4.1.jar:/home/hadoop/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/home/hadoop/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/home/hadoop/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/hadoop/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/home/hadoop/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/hadoop/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/home/hadoop/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/home/hadoop/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/hadoop/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/home/hadoop/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/home/hadoop/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/home/hadoop/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/hadoop/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/hadoop/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/home/hadoop/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/home/hadoop/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/home/hadoop/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/hadoop/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/hadoop/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/hadoop/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/hadoop/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/home/hadoop/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/home/hadoop/repository/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar:/home/hadoop/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/hadoop/repository/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming_2.11/2.4.1/spark-streaming_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.4.1/spark-streaming-kafka-0-10_2.11-2.4.1.jar:/home/hadoop/repository/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar:/home/hadoop/repository/com/alibaba/fastjson/1.2.40/fastjson-1.2.40.jar:/home/hadoop/repository/org/apache/hive/hive-jdbc/2.3.3/hive-jdbc-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-common/2.3.3/hive-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-storage-api/2.4.0/hive-storage-api-2.4.0.jar:/home/hadoop/repository/org/apache/orc/orc-core/1.3.3/orc-core-1.3.3.jar:/home/hadoop/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/home/hadoop/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/home/hadoop/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/home/hadoop/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/home/hadoop/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/home/hadoop/repository/asm/asm/3.1/asm-3.1.jar:/home/hadoop/repository/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-web/2.6.2/log4j-web-2.6.2.jar:/home/hadoop/repository/com/tdunning/json/1.8/json-1.8.jar:/home/hadoop/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/hadoop/repository/org/apache/hive/hive-service/2.3.3/hive-service-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-server/2.3.3/hive-llap-server-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3.jar:/home/hadoop/repository/org/apache/slider/slider-core/0.90.2-incubating/slider-core-0.90.2-incubating.jar:/home/hadoop/repository/com/beust/jcommander/1.30/jcommander-1.30.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-registry/2.7.1/hadoop-yarn-registry-2.7.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/hadoop/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/hadoop/repository/org/apache/hive/hive-llap-common/2.3.3/hive-llap-common-2.3.3-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop2-compat/1.1.1/hbase-hadoop2-compat-1.1.1.jar:/home/hadoop/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/hadoop/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-server/1.1.1/hbase-server-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-procedure/1.1.1/hbase-procedure-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1-tests.jar:/home/hadoop/repository/org/apache/hbase/hbase-prefix-tree/1.1.1/hbase-prefix-tree-1.1.1.jar:/home/hadoop/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/home/hadoop/repository/org/apache/hbase/hbase-common/1.1.1/hbase-common-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-hadoop-compat/1.1.1/hbase-hadoop-compat-1.1.1.jar:/home/hadoop/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/hadoop/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/repository/javax/servlet/jsp-api/2.0/jsp-api-2.0.jar:/home/hadoop/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/hadoop/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/repository/javax/servlet/servlet-api/2.4/servlet-api-2.4.jar:/home/hadoop/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/repository/org/apache/hive/hive-serde/2.3.3/hive-serde-2.3.3.jar:/home/hadoop/repository/org/apache/parquet/parquet-hadoop-bundle/1.8.1/parquet-hadoop-bundle-1.8.1.jar:/home/hadoop/repository/org/apache/hive/hive-metastore/2.3.3/hive-metastore-2.3.3.jar:/home/hadoop/repository/org/apache/hbase/hbase-client/1.1.1/hbase-client-1.1.1.jar:/home/hadoop/repository/org/apache/hbase/hbase-annotations/1.1.1/hbase-annotations-1.1.1.jar:/usr/lib/jvm/java-1.8.0-openjdk/lib/tools.jar:/home/hadoop/repository/org/apache/hbase/hbase-protocol/1.1.1/hbase-protocol-1.1.1.jar:/home/hadoop/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/hadoop/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/hadoop/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/repository/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/home/hadoop/repository/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/home/hadoop/repository/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/home/hadoop/repository/co/cask/tephra/tephra-api/0.6.0/tephra-api-0.6.0.jar:/home/hadoop/repository/co/cask/tephra/tephra-core/0.6.0/tephra-core-0.6.0.jar:/home/hadoop/repository/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/hadoop/repository/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/hadoop/repository/org/apache/twill/twill-common/0.6.0-incubating/twill-common-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-core/0.6.0-incubating/twill-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-api/0.6.0-incubating/twill-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-api/0.6.0-incubating/twill-discovery-api-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-discovery-core/0.6.0-incubating/twill-discovery-core-0.6.0-incubating.jar:/home/hadoop/repository/org/apache/twill/twill-zookeeper/0.6.0-incubating/twill-zookeeper-0.6.0-incubating.jar:/home/hadoop/repository/co/cask/tephra/tephra-hbase-compat-1.0/0.6.0/tephra-hbase-compat-1.0-0.6.0.jar:/home/hadoop/repository/org/apache/hive/hive-shims/2.3.3/hive-shims-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-common/2.3.3/hive-shims-common-2.3.3.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-0.23/2.3.3/hive-shims-0.23-2.3.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.7.2/hadoop-yarn-server-resourcemanager-2.7.2.jar:/home/hadoop/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.7.2/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/home/hadoop/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6-tests.jar:/home/hadoop/repository/org/apache/hive/shims/hive-shims-scheduler/2.3.3/hive-shims-scheduler-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-service-rpc/2.3.3/hive-service-rpc-2.3.3.jar:/home/hadoop/repository/org/apache/httpcomponents/httpcore/4.4/httpcore-4.4.jar:/home/hadoop/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/hadoop/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/hadoop/repository/org/apache/hive/hive-exec/2.3.3/hive-exec-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-vector-code-gen/2.3.3/hive-vector-code-gen-2.3.3.jar:/home/hadoop/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/hadoop/repository/org/apache/hive/hive-llap-tez/2.3.3/hive-llap-tez-2.3.3.jar:/home/hadoop/repository/org/apache/hive/hive-llap-client/2.3.3/hive-llap-client-2.3.3.jar:/home/hadoop/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-1.2-api/2.6.2/log4j-1.2-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar:/home/hadoop/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/hadoop/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/hadoop/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/hadoop/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/hadoop/repository/org/apache/commons/commons-compress/1.9/commons-compress-1.9.jar:/home/hadoop/repository/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar:/home/hadoop/repository/org/apache/calcite/calcite-druid/1.10.0/calcite-druid-1.10.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica/1.8.0/avatica-1.8.0.jar:/home/hadoop/repository/org/apache/calcite/avatica/avatica-metrics/1.8.0/avatica-metrics-1.8.0.jar:/home/hadoop/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/hadoop/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-client/2.7.3/hadoop-client-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-common/2.7.3/hadoop-common-2.7.3.jar:/home/hadoop/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/hadoop/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/hadoop/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-auth/2.7.3/hadoop-auth-2.7.3.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/hadoop/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/hadoop/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-hdfs/2.7.3/hadoop-hdfs-2.7.3.jar:/home/hadoop/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/hadoop/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.3/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.3/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.3/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.3/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.3/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/repository/org/apache/hadoop/hadoop-annotations/2.7.3/hadoop-annotations-2.7.3.jar:/home/hadoop/repository/mysql/mysql-connector-java/5.1.35/mysql-connector-java-5.1.35.jar:/home/hadoop/repository/com/google/guava/guava/26.0-jre/guava-26.0-jre.jar:/home/hadoop/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/home/hadoop/repository/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar:/home/hadoop/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/hadoop/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/hadoop/scala/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-reflect.jar:/home/hadoop/scala/lib/scala-actors-migration_2.11-1.1.0.jar:/home/hadoop/scala/lib/scala-xml_2.11-1.0.4.jar:/home/hadoop/scala/lib/scala-library.jar:/home/hadoop/scala/lib/scala-swing_2.11-1.0.2.jar:/home/hadoop/scala/lib/scala-actors-2.11.0.jar:/home/hadoop/idea/lib/idea_rt.jar com.xh.spark.sql.function.WindowFunctionTest</span><br><span class="line">Connected to the target VM, address: '127.0.0.1:45315', transport: 'socket'</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">19/04/12 16:47:02 INFO SparkContext: Running Spark version 2.4.1</span><br><span class="line">19/04/12 16:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/04/12 16:47:04 INFO SparkContext: Submitted application: WindowFunctionTest</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing view acls to: hadoop</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing modify acls to: hadoop</span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/04/12 16:47:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">19/04/12 16:47:07 INFO Utils: Successfully started service 'sparkDriver' on port 43568.</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/04/12 16:47:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/04/12 16:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07913749-f493-4bae-95aa-f9ea019dde51</span><br><span class="line">19/04/12 16:47:07 INFO MemoryStore: MemoryStore started with capacity 447.3 MB</span><br><span class="line">19/04/12 16:47:07 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/04/12 16:47:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.</span><br><span class="line">19/04/12 16:47:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop04:4040</span><br><span class="line">19/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT.jar with timestamp 1555058828687</span><br><span class="line">19/04/12 16:47:08 INFO SparkContext: Added JAR /home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://hadoop04:43568/jars/sparklearning-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1555058828703</span><br><span class="line">19/04/12 16:47:13 INFO Client: Requesting a new application from cluster with 3 NodeManagers</span><br><span class="line">19/04/12 16:47:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">19/04/12 16:47:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead</span><br><span class="line">19/04/12 16:47:13 INFO Client: Setting up container launch context for our AM</span><br><span class="line">19/04/12 16:47:13 INFO Client: Setting up the launch environment for our AM container</span><br><span class="line">19/04/12 16:47:14 INFO Client: Preparing resources for our AM container</span><br><span class="line">19/04/12 16:47:14 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">19/04/12 16:47:23 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_libs__8176900855339303501.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_libs__8176900855339303501.zip</span><br><span class="line">19/04/12 16:47:50 INFO Client: Uploading resource file:/tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33/__spark_conf__4747135480716905311.zip -&gt; hdfs://ns1/user/hadoop/.sparkStaging/application_1555049463744_0005/__spark_conf__.zip</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing view acls to: hadoop</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing modify acls to: hadoop</span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/04/12 16:47:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">19/04/12 16:47:55 INFO Client: Submitting application application_1555049463744_0005 to ResourceManager</span><br><span class="line">19/04/12 16:47:55 INFO YarnClientImpl: Submitted application application_1555049463744_0005</span><br><span class="line">19/04/12 16:47:55 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1555049463744_0005 and attemptId None</span><br><span class="line">19/04/12 16:47:57 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:47:57 INFO Client: </span><br><span class="line"> client token: N/A</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: N/A</span><br><span class="line"> ApplicationMaster RPC port: -1</span><br><span class="line"> queue: root.hadoop</span><br><span class="line"> start time: 1555058875812</span><br><span class="line"> final status: UNDEFINED</span><br><span class="line"> tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/</span><br><span class="line"> user: hadoop</span><br><span class="line">19/04/12 16:47:58 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:47:59 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:00 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:01 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:02 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:03 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:04 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:05 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:06 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:07 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:08 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:09 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:10 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:11 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:12 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:13 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:14 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:15 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:16 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:17 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:18 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:19 INFO Client: Application report for application_1555049463744_0005 (state: ACCEPTED)</span><br><span class="line">19/04/12 16:48:20 INFO Client: Application report for application_1555049463744_0005 (state: RUNNING)</span><br><span class="line">19/04/12 16:48:20 INFO Client: </span><br><span class="line"> client token: N/A</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: 192.168.8.82</span><br><span class="line"> ApplicationMaster RPC port: -1</span><br><span class="line"> queue: root.hadoop</span><br><span class="line"> start time: 1555058875812</span><br><span class="line"> final status: UNDEFINED</span><br><span class="line"> tracking URL: http://192.168.8.82:8089/proxy/application_1555049463744_0005/</span><br><span class="line"> user: hadoop</span><br><span class="line">19/04/12 16:48:20 INFO YarnClientSchedulerBackend: Application application_1555049463744_0005 has started running.</span><br><span class="line">19/04/12 16:48:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42715.</span><br><span class="line">19/04/12 16:48:21 INFO NettyBlockTransferService: Server created on hadoop04:42715</span><br><span class="line">19/04/12 16:48:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:42715 with 447.3 MB RAM, BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop04, 42715, None)</span><br><span class="line">19/04/12 16:48:23 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)</span><br><span class="line">19/04/12 16:48:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; 192.168.8.82, PROXY_URI_BASES -&gt; http://192.168.8.82:8089/proxy/application_1555049463744_0005, RM_HA_URLS -&gt; hadoop02:8088,hadoop03:8088), /proxy/application_1555049463744_0005</span><br><span class="line">19/04/12 16:48:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill, /metrics/json.</span><br><span class="line">19/04/12 16:48:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)</span><br><span class="line">19/04/12 16:48:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/worker/sparklearning/spark-warehouse').</span><br><span class="line">19/04/12 16:48:32 INFO SharedState: Warehouse path is 'file:/home/hadoop/worker/sparklearning/spark-warehouse'.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.</span><br><span class="line">19/04/12 16:48:32 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.</span><br><span class="line">19/04/12 16:48:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">19/04/12 16:48:55 INFO CodeGenerator: Code generated in 2751.822246 ms</span><br><span class="line">19/04/12 16:52:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.83:53042) with ID 1</span><br><span class="line">19/04/12 16:52:30 INFO BlockManagerMasterEndpoint: Registering block manager hadoop03:45104 with 366.3 MB RAM, BlockManagerId(1, hadoop03, 45104, None)</span><br><span class="line">19/04/12 16:52:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.8.84:37146) with ID 2</span><br><span class="line">19/04/12 16:52:39 INFO BlockManagerMasterEndpoint: Registering block manager hadoop04:38267 with 366.3 MB RAM, BlockManagerId(2, hadoop04, 38267, None)</span><br><span class="line">19/04/12 16:53:38 INFO CodeGenerator: Code generated in 668.087005 ms</span><br><span class="line">19/04/12 16:53:45 INFO CodeGenerator: Code generated in 732.99189 ms</span><br><span class="line">19/04/12 16:53:47 INFO CodeGenerator: Code generated in 1265.744172 ms</span><br><span class="line">19/04/12 16:53:47 INFO CodeGenerator: Code generated in 271.771791 ms</span><br><span class="line">19/04/12 16:53:51 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Registering RDD 2 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Got job 0 (show at WindowFunctionTest.scala:45) with 1 output partitions</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Final stage: ResultStage 1 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)</span><br><span class="line">19/04/12 16:53:51 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:53:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.1 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:42715 (size: 3.1 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:53:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:53:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0, 1))</span><br><span class="line">19/04/12 16:53:55 INFO YarnScheduler: Adding task set 0.0 with 2 tasks</span><br><span class="line">19/04/12 16:53:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 8230 bytes)</span><br><span class="line">19/04/12 16:53:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 8230 bytes)</span><br><span class="line">19/04/12 16:55:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop03:45104 (size: 3.1 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:55:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop04:38267 (size: 3.1 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 137230 ms on hadoop03 (executor 1) (1/2)</span><br><span class="line">19/04/12 16:56:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 147247 ms on hadoop04 (executor 2) (2/2)</span><br><span class="line">19/04/12 16:56:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: ShuffleMapStage 0 (show at WindowFunctionTest.scala:45) finished in 151.743 s</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: looking for newly runnable stages</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: running: Set()</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: waiting: Set(ResultStage 1)</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: failed: Set()</span><br><span class="line">19/04/12 16:56:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.4 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(0))</span><br><span class="line">19/04/12 16:56:25 INFO YarnScheduler: Adding task set 1.0 with 1 tasks</span><br><span class="line">19/04/12 16:56:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hadoop03, executor 1, partition 0, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.83:53042</span><br><span class="line">19/04/12 16:56:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2345 ms on hadoop03 (executor 1) (1/1)</span><br><span class="line">19/04/12 16:56:27 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:27 INFO DAGScheduler: ResultStage 1 (show at WindowFunctionTest.scala:45) finished in 3.127 s</span><br><span class="line">19/04/12 16:56:27 INFO DAGScheduler: Job 0 finished: show at WindowFunctionTest.scala:45, took 156.360095 s</span><br><span class="line">19/04/12 16:56:28 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Got job 1 (show at WindowFunctionTest.scala:45) with 4 output partitions</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Final stage: ResultStage 3 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(1, 2, 3, 4))</span><br><span class="line">19/04/12 16:56:28 INFO YarnScheduler: Adding task set 3.0 with 4 tasks</span><br><span class="line">19/04/12 16:56:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, hadoop04, executor 2, partition 1, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:28 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, hadoop03, executor 1, partition 2, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5, hadoop03, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 447 ms on hadoop03 (executor 1) (1/4)</span><br><span class="line">19/04/12 16:56:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6, hadoop03, executor 1, partition 4, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 290 ms on hadoop03 (executor 1) (2/4)</span><br><span class="line">19/04/12 16:56:29 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 334 ms on hadoop03 (executor 1) (3/4)</span><br><span class="line">19/04/12 16:56:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.8.84:37146</span><br><span class="line">19/04/12 16:56:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3241 ms on hadoop04 (executor 2) (4/4)</span><br><span class="line">19/04/12 16:56:31 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:31 INFO DAGScheduler: ResultStage 3 (show at WindowFunctionTest.scala:45) finished in 3.803 s</span><br><span class="line">19/04/12 16:56:31 INFO DAGScheduler: Job 1 finished: show at WindowFunctionTest.scala:45, took 3.848347 s</span><br><span class="line">19/04/12 16:56:32 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Got job 2 (show at WindowFunctionTest.scala:45) with 20 output partitions</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Final stage: ResultStage 5 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:32 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))</span><br><span class="line">19/04/12 16:56:32 INFO YarnScheduler: Adding task set 5.0 with 20 tasks</span><br><span class="line">19/04/12 16:56:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7, hadoop03, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:32 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, hadoop04, executor 2, partition 6, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 9, hadoop04, executor 2, partition 7, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 726 ms on hadoop04 (executor 2) (1/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 10, hadoop03, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 811 ms on hadoop03 (executor 1) (2/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 11, hadoop03, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 10) in 393 ms on hadoop03 (executor 1) (3/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 12, hadoop04, executor 2, partition 10, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 9) in 523 ms on hadoop04 (executor 2) (4/20)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 13, hadoop03, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:33 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 11) in 317 ms on hadoop03 (executor 1) (5/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 14, hadoop03, executor 1, partition 12, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 15, hadoop03, executor 1, partition 13, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 13) in 851 ms on hadoop03 (executor 1) (6/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 14) in 541 ms on hadoop03 (executor 1) (7/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 16, hadoop04, executor 2, partition 14, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 12) in 1219 ms on hadoop04 (executor 2) (8/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 17, hadoop04, executor 2, partition 15, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 18, hadoop03, executor 1, partition 16, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 16) in 313 ms on hadoop04 (executor 2) (9/20)</span><br><span class="line">19/04/12 16:56:34 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 15) in 596 ms on hadoop03 (executor 1) (10/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 19, hadoop04, executor 2, partition 17, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 20, hadoop03, executor 1, partition 18, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 18) in 158 ms on hadoop03 (executor 1) (11/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 17) in 167 ms on hadoop04 (executor 2) (12/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 21, hadoop03, executor 1, partition 19, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 22, hadoop04, executor 2, partition 20, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 20) in 250 ms on hadoop03 (executor 1) (13/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 19) in 343 ms on hadoop04 (executor 2) (14/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 23, hadoop04, executor 2, partition 21, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 22) in 247 ms on hadoop04 (executor 2) (15/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 24, hadoop03, executor 1, partition 22, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 21) in 481 ms on hadoop03 (executor 1) (16/20)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 25, hadoop04, executor 2, partition 23, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:35 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 23) in 393 ms on hadoop04 (executor 2) (17/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 26, hadoop04, executor 2, partition 24, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 25) in 473 ms on hadoop04 (executor 2) (18/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 24) in 898 ms on hadoop03 (executor 1) (19/20)</span><br><span class="line">19/04/12 16:56:36 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 26) in 524 ms on hadoop04 (executor 2) (20/20)</span><br><span class="line">19/04/12 16:56:36 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:56:36 INFO DAGScheduler: ResultStage 5 (show at WindowFunctionTest.scala:45) finished in 4.493 s</span><br><span class="line">19/04/12 16:56:36 INFO DAGScheduler: Job 2 finished: show at WindowFunctionTest.scala:45, took 4.566402 s</span><br><span class="line">19/04/12 16:56:37 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Got job 3 (show at WindowFunctionTest.scala:45) with 100 output partitions</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Final stage: ResultStage 7 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:56:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:56:37 INFO DAGScheduler: Submitting 100 missing tasks from ResultStage 7 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39))</span><br><span class="line">19/04/12 16:56:37 INFO YarnScheduler: Adding task set 7.0 with 100 tasks</span><br><span class="line">19/04/12 16:56:37 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 27, hadoop04, executor 2, partition 110, NODE_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:37 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 28, hadoop03, executor 1, partition 102, NODE_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 29, hadoop04, executor 2, partition 25, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 27) in 1607 ms on hadoop04 (executor 2) (1/100)</span><br><span class="line">19/04/12 16:56:39 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 30, hadoop04, executor 2, partition 26, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 31, hadoop04, executor 2, partition 27, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 29) in 1083 ms on hadoop04 (executor 2) (2/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 30) in 659 ms on hadoop04 (executor 2) (3/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 32, hadoop04, executor 2, partition 28, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 31) in 473 ms on hadoop04 (executor 2) (4/100)</span><br><span class="line">19/04/12 16:56:40 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 33, hadoop04, executor 2, partition 29, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 34, hadoop03, executor 1, partition 30, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 28) in 3578 ms on hadoop03 (executor 1) (5/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 32) in 505 ms on hadoop04 (executor 2) (6/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 35, hadoop04, executor 2, partition 31, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 33) in 438 ms on hadoop04 (executor 2) (7/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 36, hadoop03, executor 1, partition 32, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 37, hadoop04, executor 2, partition 33, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 35) in 353 ms on hadoop04 (executor 2) (8/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 34) in 583 ms on hadoop03 (executor 1) (9/100)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 38, hadoop03, executor 1, partition 34, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:41 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 36) in 301 ms on hadoop03 (executor 1) (10/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 39, hadoop03, executor 1, partition 35, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 40, hadoop04, executor 2, partition 36, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 38) in 453 ms on hadoop03 (executor 1) (11/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 41, hadoop04, executor 2, partition 37, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 37) in 1017 ms on hadoop04 (executor 2) (12/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 42, hadoop03, executor 1, partition 38, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 39) in 407 ms on hadoop03 (executor 1) (13/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 40) in 486 ms on hadoop04 (executor 2) (14/100)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 43, hadoop04, executor 2, partition 39, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:42 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 41) in 259 ms on hadoop04 (executor 2) (15/100)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 44, hadoop04, executor 2, partition 40, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 43) in 472 ms on hadoop04 (executor 2) (16/100)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 45, hadoop03, executor 1, partition 41, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:43 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 42) in 964 ms on hadoop03 (executor 1) (17/100)</span><br><span class="line">19/04/12 16:56:45 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 46, hadoop03, executor 1, partition 42, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 47, hadoop04, executor 2, partition 43, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 44) in 2989 ms on hadoop04 (executor 2) (18/100)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 45) in 3210 ms on hadoop03 (executor 1) (19/100)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 48, hadoop03, executor 1, partition 44, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:46 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 46) in 1507 ms on hadoop03 (executor 1) (20/100)</span><br><span class="line">19/04/12 16:56:47 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 49, hadoop04, executor 2, partition 45, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:47 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 47) in 1533 ms on hadoop04 (executor 2) (21/100)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 50, hadoop04, executor 2, partition 46, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 49) in 715 ms on hadoop04 (executor 2) (22/100)</span><br><span class="line">19/04/12 16:56:48 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 51, hadoop03, executor 1, partition 47, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 48) in 2132 ms on hadoop03 (executor 1) (23/100)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 52, hadoop04, executor 2, partition 48, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 50) in 1332 ms on hadoop04 (executor 2) (24/100)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 53, hadoop03, executor 1, partition 49, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:49 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 51) in 776 ms on hadoop03 (executor 1) (25/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 54, hadoop04, executor 2, partition 50, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 52) in 1066 ms on hadoop04 (executor 2) (26/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 55, hadoop03, executor 1, partition 51, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 53) in 698 ms on hadoop03 (executor 1) (27/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 56, hadoop04, executor 2, partition 52, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 54) in 679 ms on hadoop04 (executor 2) (28/100)</span><br><span class="line">19/04/12 16:56:50 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 57, hadoop03, executor 1, partition 53, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 55) in 1082 ms on hadoop03 (executor 1) (29/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 58, hadoop04, executor 2, partition 54, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 56) in 731 ms on hadoop04 (executor 2) (30/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 59, hadoop03, executor 1, partition 55, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 57) in 592 ms on hadoop03 (executor 1) (31/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 60, hadoop04, executor 2, partition 56, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 58) in 557 ms on hadoop04 (executor 2) (32/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 61, hadoop03, executor 1, partition 57, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 59) in 468 ms on hadoop03 (executor 1) (33/100)</span><br><span class="line">19/04/12 16:56:51 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 62, hadoop04, executor 2, partition 58, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 60) in 241 ms on hadoop04 (executor 2) (34/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 63, hadoop04, executor 2, partition 59, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 62) in 728 ms on hadoop04 (executor 2) (35/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 64, hadoop03, executor 1, partition 60, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 61) in 804 ms on hadoop03 (executor 1) (36/100)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 65, hadoop04, executor 2, partition 61, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:52 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 63) in 392 ms on hadoop04 (executor 2) (37/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 66, hadoop03, executor 1, partition 62, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 64) in 320 ms on hadoop03 (executor 1) (38/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 67, hadoop04, executor 2, partition 63, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 65) in 154 ms on hadoop04 (executor 2) (39/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 68, hadoop03, executor 1, partition 64, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 66) in 297 ms on hadoop03 (executor 1) (40/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 69, hadoop03, executor 1, partition 65, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 68) in 463 ms on hadoop03 (executor 1) (41/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 70, hadoop04, executor 2, partition 66, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 67) in 672 ms on hadoop04 (executor 2) (42/100)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 71, hadoop03, executor 1, partition 67, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:53 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 69) in 165 ms on hadoop03 (executor 1) (43/100)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 72, hadoop03, executor 1, partition 68, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 71) in 1656 ms on hadoop03 (executor 1) (44/100)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 73, hadoop03, executor 1, partition 69, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:55 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 72) in 534 ms on hadoop03 (executor 1) (45/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 74, hadoop03, executor 1, partition 70, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 73) in 376 ms on hadoop03 (executor 1) (46/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 75, hadoop04, executor 2, partition 71, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 70) in 2844 ms on hadoop04 (executor 2) (47/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 76, hadoop03, executor 1, partition 72, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 74) in 444 ms on hadoop03 (executor 1) (48/100)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 77, hadoop03, executor 1, partition 73, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:56 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 76) in 341 ms on hadoop03 (executor 1) (49/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 78, hadoop03, executor 1, partition 74, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 77) in 169 ms on hadoop03 (executor 1) (50/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 79, hadoop03, executor 1, partition 75, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 78) in 193 ms on hadoop03 (executor 1) (51/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 80, hadoop04, executor 2, partition 76, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 75) in 1174 ms on hadoop04 (executor 2) (52/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 81, hadoop03, executor 1, partition 77, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 82, hadoop04, executor 2, partition 78, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 79) in 337 ms on hadoop03 (executor 1) (53/100)</span><br><span class="line">19/04/12 16:56:57 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 80) in 310 ms on hadoop04 (executor 2) (54/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 83, hadoop03, executor 1, partition 79, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 81) in 761 ms on hadoop03 (executor 1) (55/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 84, hadoop04, executor 2, partition 80, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 82) in 696 ms on hadoop04 (executor 2) (56/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 85, hadoop03, executor 1, partition 81, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 83) in 691 ms on hadoop03 (executor 1) (57/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 86, hadoop03, executor 1, partition 82, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 85) in 118 ms on hadoop03 (executor 1) (58/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 87, hadoop04, executor 2, partition 83, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 84) in 447 ms on hadoop04 (executor 2) (59/100)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 88, hadoop03, executor 1, partition 84, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:58 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 86) in 240 ms on hadoop03 (executor 1) (60/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 89, hadoop04, executor 2, partition 85, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 87) in 325 ms on hadoop04 (executor 2) (61/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 90, hadoop03, executor 1, partition 86, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 88) in 255 ms on hadoop03 (executor 1) (62/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 91, hadoop04, executor 2, partition 87, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 89) in 147 ms on hadoop04 (executor 2) (63/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 92, hadoop03, executor 1, partition 88, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 90) in 130 ms on hadoop03 (executor 1) (64/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 93, hadoop03, executor 1, partition 89, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 92) in 110 ms on hadoop03 (executor 1) (65/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 94, hadoop04, executor 2, partition 90, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 91) in 264 ms on hadoop04 (executor 2) (66/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 95, hadoop03, executor 1, partition 91, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 93) in 165 ms on hadoop03 (executor 1) (67/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 96, hadoop04, executor 2, partition 92, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 94) in 267 ms on hadoop04 (executor 2) (68/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 97, hadoop03, executor 1, partition 93, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 95) in 222 ms on hadoop03 (executor 1) (69/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 98, hadoop04, executor 2, partition 94, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 96) in 191 ms on hadoop04 (executor 2) (70/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 99, hadoop03, executor 1, partition 95, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 97) in 173 ms on hadoop03 (executor 1) (71/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 100, hadoop04, executor 2, partition 96, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 98) in 165 ms on hadoop04 (executor 2) (72/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 101, hadoop03, executor 1, partition 97, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 99) in 181 ms on hadoop03 (executor 1) (73/100)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 102, hadoop04, executor 2, partition 98, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:56:59 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 100) in 176 ms on hadoop04 (executor 2) (74/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 103, hadoop04, executor 2, partition 99, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 102) in 103 ms on hadoop04 (executor 2) (75/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 104, hadoop03, executor 1, partition 100, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 101) in 202 ms on hadoop03 (executor 1) (76/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 105, hadoop04, executor 2, partition 101, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 103) in 159 ms on hadoop04 (executor 2) (77/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 106, hadoop04, executor 2, partition 103, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 105) in 378 ms on hadoop04 (executor 2) (78/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 107, hadoop03, executor 1, partition 104, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 104) in 551 ms on hadoop03 (executor 1) (79/100)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 108, hadoop04, executor 2, partition 105, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:00 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 106) in 437 ms on hadoop04 (executor 2) (80/100)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 109, hadoop03, executor 1, partition 106, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 107) in 621 ms on hadoop03 (executor 1) (81/100)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 110, hadoop04, executor 2, partition 107, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:01 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 108) in 1254 ms on hadoop04 (executor 2) (82/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 111, hadoop03, executor 1, partition 108, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 109) in 1316 ms on hadoop03 (executor 1) (83/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 112, hadoop04, executor 2, partition 109, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 110) in 793 ms on hadoop04 (executor 2) (84/100)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 113, hadoop03, executor 1, partition 111, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:02 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 111) in 780 ms on hadoop03 (executor 1) (85/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 114, hadoop04, executor 2, partition 112, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 112) in 567 ms on hadoop04 (executor 2) (86/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 115, hadoop03, executor 1, partition 113, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 113) in 689 ms on hadoop03 (executor 1) (87/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 116, hadoop04, executor 2, partition 114, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 114) in 513 ms on hadoop04 (executor 2) (88/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 117, hadoop03, executor 1, partition 115, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 115) in 432 ms on hadoop03 (executor 1) (89/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 118, hadoop04, executor 2, partition 116, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 116) in 271 ms on hadoop04 (executor 2) (90/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 119, hadoop03, executor 1, partition 117, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 120, hadoop04, executor 2, partition 118, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 118) in 129 ms on hadoop04 (executor 2) (91/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 117) in 173 ms on hadoop03 (executor 1) (92/100)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 121, hadoop04, executor 2, partition 119, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:03 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 120) in 139 ms on hadoop04 (executor 2) (93/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 122, hadoop03, executor 1, partition 120, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 119) in 232 ms on hadoop03 (executor 1) (94/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 123, hadoop04, executor 2, partition 121, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 121) in 110 ms on hadoop04 (executor 2) (95/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 124, hadoop03, executor 1, partition 122, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 122) in 115 ms on hadoop03 (executor 1) (96/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 125, hadoop04, executor 2, partition 123, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 123) in 136 ms on hadoop04 (executor 2) (97/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 126, hadoop03, executor 1, partition 124, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 124) in 124 ms on hadoop03 (executor 1) (98/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 125) in 140 ms on hadoop04 (executor 2) (99/100)</span><br><span class="line">19/04/12 16:57:04 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 126) in 145 ms on hadoop03 (executor 1) (100/100)</span><br><span class="line">19/04/12 16:57:04 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:57:04 INFO DAGScheduler: ResultStage 7 (show at WindowFunctionTest.scala:45) finished in 26.819 s</span><br><span class="line">19/04/12 16:57:04 INFO DAGScheduler: Job 3 finished: show at WindowFunctionTest.scala:45, took 27.209729 s</span><br><span class="line">19/04/12 16:57:05 INFO SparkContext: Starting job: show at WindowFunctionTest.scala:45</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Got job 4 (show at WindowFunctionTest.scala:45) with 75 output partitions</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Final stage: ResultStage 9 (show at WindowFunctionTest.scala:45)</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Missing parents: List()</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45), which has no missing parents</span><br><span class="line">19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.4 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:57:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.8 KB, free 447.2 MB)</span><br><span class="line">19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:42715 (size: 8.8 KB, free: 447.3 MB)</span><br><span class="line">19/04/12 16:57:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161</span><br><span class="line">19/04/12 16:57:05 INFO DAGScheduler: Submitting 75 missing tasks from ResultStage 9 (MapPartitionsRDD[8] at show at WindowFunctionTest.scala:45) (first 15 tasks are for partitions Vector(125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139))</span><br><span class="line">19/04/12 16:57:05 INFO YarnScheduler: Adding task set 9.0 with 75 tasks</span><br><span class="line">19/04/12 16:57:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 127, hadoop04, executor 2, partition 125, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:05 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 128, hadoop03, executor 1, partition 126, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop04:38267 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 129, hadoop04, executor 2, partition 127, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 127) in 398 ms on hadoop04 (executor 2) (1/75)</span><br><span class="line">19/04/12 16:57:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop03:45104 (size: 8.8 KB, free: 366.3 MB)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 130, hadoop04, executor 2, partition 128, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 129) in 259 ms on hadoop04 (executor 2) (2/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 131, hadoop03, executor 1, partition 129, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 128) in 796 ms on hadoop03 (executor 1) (3/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 132, hadoop04, executor 2, partition 130, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 130) in 277 ms on hadoop04 (executor 2) (4/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 133, hadoop03, executor 1, partition 131, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 131) in 188 ms on hadoop03 (executor 1) (5/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 134, hadoop03, executor 1, partition 132, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 133) in 173 ms on hadoop03 (executor 1) (6/75)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 135, hadoop04, executor 2, partition 133, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:06 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 132) in 289 ms on hadoop04 (executor 2) (7/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 136, hadoop03, executor 1, partition 134, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 134) in 206 ms on hadoop03 (executor 1) (8/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 137, hadoop04, executor 2, partition 135, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 135) in 187 ms on hadoop04 (executor 2) (9/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 138, hadoop03, executor 1, partition 136, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 139, hadoop04, executor 2, partition 137, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 136) in 218 ms on hadoop03 (executor 1) (10/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 140, hadoop03, executor 1, partition 138, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 138) in 102 ms on hadoop03 (executor 1) (11/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 137) in 232 ms on hadoop04 (executor 2) (12/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 141, hadoop03, executor 1, partition 139, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 140) in 96 ms on hadoop03 (executor 1) (13/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 142, hadoop04, executor 2, partition 140, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 139) in 200 ms on hadoop04 (executor 2) (14/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 143, hadoop03, executor 1, partition 141, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 141) in 156 ms on hadoop03 (executor 1) (15/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 144, hadoop03, executor 1, partition 142, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 143) in 234 ms on hadoop03 (executor 1) (16/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 145, hadoop04, executor 2, partition 143, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 142) in 453 ms on hadoop04 (executor 2) (17/75)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 146, hadoop03, executor 1, partition 144, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:07 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 144) in 273 ms on hadoop03 (executor 1) (18/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 147, hadoop04, executor 2, partition 145, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 145) in 220 ms on hadoop04 (executor 2) (19/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 148, hadoop03, executor 1, partition 146, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 146) in 178 ms on hadoop03 (executor 1) (20/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 149, hadoop04, executor 2, partition 147, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 147) in 313 ms on hadoop04 (executor 2) (21/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 150, hadoop03, executor 1, partition 148, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 148) in 279 ms on hadoop03 (executor 1) (22/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 151, hadoop04, executor 2, partition 149, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 149) in 371 ms on hadoop04 (executor 2) (23/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 152, hadoop03, executor 1, partition 150, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 150) in 291 ms on hadoop03 (executor 1) (24/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 153, hadoop04, executor 2, partition 151, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 151) in 321 ms on hadoop04 (executor 2) (25/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 154, hadoop03, executor 1, partition 152, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 152) in 385 ms on hadoop03 (executor 1) (26/75)</span><br><span class="line">19/04/12 16:57:08 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 155, hadoop03, executor 1, partition 153, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 156, hadoop04, executor 2, partition 154, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 153) in 386 ms on hadoop04 (executor 2) (27/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 154) in 332 ms on hadoop03 (executor 1) (28/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 157, hadoop03, executor 1, partition 155, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 155) in 199 ms on hadoop03 (executor 1) (29/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 158, hadoop04, executor 2, partition 156, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 156) in 303 ms on hadoop04 (executor 2) (30/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 159, hadoop04, executor 2, partition 157, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 158) in 94 ms on hadoop04 (executor 2) (31/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 160, hadoop03, executor 1, partition 158, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 157) in 276 ms on hadoop03 (executor 1) (32/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 161, hadoop04, executor 2, partition 159, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 159) in 171 ms on hadoop04 (executor 2) (33/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 162, hadoop03, executor 1, partition 160, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 160) in 182 ms on hadoop03 (executor 1) (34/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 163, hadoop03, executor 1, partition 161, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 164, hadoop04, executor 2, partition 162, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 165, hadoop04, executor 2, partition 163, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 164) in 52 ms on hadoop04 (executor 2) (35/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 161) in 191 ms on hadoop04 (executor 2) (36/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 162) in 206 ms on hadoop03 (executor 1) (37/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 166, hadoop03, executor 1, partition 164, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 167, hadoop04, executor 2, partition 165, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 165) in 108 ms on hadoop04 (executor 2) (38/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 168, hadoop04, executor 2, partition 166, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 163) in 218 ms on hadoop03 (executor 1) (39/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 167) in 88 ms on hadoop04 (executor 2) (40/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 169, hadoop03, executor 1, partition 167, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 166) in 112 ms on hadoop03 (executor 1) (41/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 170, hadoop03, executor 1, partition 168, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 169) in 75 ms on hadoop03 (executor 1) (42/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 171, hadoop04, executor 2, partition 169, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 168) in 162 ms on hadoop04 (executor 2) (43/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 172, hadoop03, executor 1, partition 170, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 173, hadoop04, executor 2, partition 171, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 170) in 96 ms on hadoop03 (executor 1) (44/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 174, hadoop03, executor 1, partition 172, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 171) in 75 ms on hadoop04 (executor 2) (45/75)</span><br><span class="line">19/04/12 16:57:09 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 172) in 49 ms on hadoop03 (executor 1) (46/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 175, hadoop03, executor 1, partition 173, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 174) in 85 ms on hadoop03 (executor 1) (47/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 176, hadoop04, executor 2, partition 174, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 177, hadoop03, executor 1, partition 175, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 178, hadoop03, executor 1, partition 176, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 177) in 38 ms on hadoop03 (executor 1) (48/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 173) in 141 ms on hadoop04 (executor 2) (49/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 179, hadoop03, executor 1, partition 177, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 175) in 123 ms on hadoop03 (executor 1) (50/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 178) in 57 ms on hadoop03 (executor 1) (51/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 180, hadoop04, executor 2, partition 178, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 176) in 114 ms on hadoop04 (executor 2) (52/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 181, hadoop03, executor 1, partition 179, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 182, hadoop03, executor 1, partition 180, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 183, hadoop04, executor 2, partition 181, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 179) in 159 ms on hadoop03 (executor 1) (53/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 184, hadoop03, executor 1, partition 182, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 181) in 110 ms on hadoop03 (executor 1) (54/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 185, hadoop04, executor 2, partition 183, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 180) in 165 ms on hadoop04 (executor 2) (55/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 182) in 262 ms on hadoop03 (executor 1) (56/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 186, hadoop03, executor 1, partition 184, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 187, hadoop04, executor 2, partition 185, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 183) in 331 ms on hadoop04 (executor 2) (57/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 184) in 301 ms on hadoop03 (executor 1) (58/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 185) in 289 ms on hadoop04 (executor 2) (59/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 188, hadoop03, executor 1, partition 186, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 186) in 151 ms on hadoop03 (executor 1) (60/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 189, hadoop04, executor 2, partition 187, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 187) in 197 ms on hadoop04 (executor 2) (61/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 190, hadoop03, executor 1, partition 188, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 188) in 114 ms on hadoop03 (executor 1) (62/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 191, hadoop04, executor 2, partition 189, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 189) in 148 ms on hadoop04 (executor 2) (63/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 192, hadoop03, executor 1, partition 190, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 190) in 113 ms on hadoop03 (executor 1) (64/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 193, hadoop03, executor 1, partition 191, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 194, hadoop04, executor 2, partition 192, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 191) in 129 ms on hadoop04 (executor 2) (65/75)</span><br><span class="line">19/04/12 16:57:10 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 192) in 105 ms on hadoop03 (executor 1) (66/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 195, hadoop03, executor 1, partition 193, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 193) in 126 ms on hadoop03 (executor 1) (67/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 196, hadoop04, executor 2, partition 194, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 194) in 131 ms on hadoop04 (executor 2) (68/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 197, hadoop04, executor 2, partition 195, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 196) in 217 ms on hadoop04 (executor 2) (69/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 198, hadoop03, executor 1, partition 196, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 195) in 301 ms on hadoop03 (executor 1) (70/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 199, hadoop04, executor 2, partition 197, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 197) in 270 ms on hadoop04 (executor 2) (71/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 200, hadoop03, executor 1, partition 198, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 198) in 256 ms on hadoop03 (executor 1) (72/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 201, hadoop04, executor 2, partition 199, PROCESS_LOCAL, 7778 bytes)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 199) in 282 ms on hadoop04 (executor 2) (73/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 200) in 214 ms on hadoop03 (executor 1) (74/75)</span><br><span class="line">19/04/12 16:57:11 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 201) in 104 ms on hadoop04 (executor 2) (75/75)</span><br><span class="line">19/04/12 16:57:11 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool </span><br><span class="line">19/04/12 16:57:11 INFO DAGScheduler: ResultStage 9 (show at WindowFunctionTest.scala:45) finished in 6.626 s</span><br><span class="line">19/04/12 16:57:11 INFO DAGScheduler: Job 4 finished: show at WindowFunctionTest.scala:45, took 6.700226 s</span><br><span class="line">19/04/12 16:57:14 INFO SparkUI: Stopped Spark web UI at http://hadoop04:4040</span><br><span class="line">19/04/12 16:57:14 INFO YarnClientSchedulerBackend: Interrupting monitor thread</span><br><span class="line">19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Shutting down all executors</span><br><span class="line">19/04/12 16:57:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down</span><br><span class="line">19/04/12 16:57:15 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices</span><br><span class="line">(serviceOption=None,</span><br><span class="line"> services=List(),</span><br><span class="line"> started=false)</span><br><span class="line">19/04/12 16:57:15 INFO YarnClientSchedulerBackend: Stopped</span><br><span class="line">19/04/12 16:57:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/04/12 16:57:17 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/04/12 16:57:17 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/04/12 16:57:17 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/04/12 16:57:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/04/12 16:57:18 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/04/12 16:57:19 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/04/12 16:57:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3533ae6-b45b-4a1f-8119-0c12775c5a33</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line">| site|      date|user_cnt|MovingAvg|</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line">|站点1|2018-01-01|      50|     47.5|</span><br><span class="line">|站点1|2018-01-02|      45|     50.0|</span><br><span class="line">|站点1|2018-01-03|      55|     50.0|</span><br><span class="line">|站点2|2018-01-01|      25|     27.0|</span><br><span class="line">|站点2|2018-01-02|      29|     27.0|</span><br><span class="line">|站点2|2018-01-03|      27|     28.0|</span><br><span class="line">+-----+----------+--------+---------+</span><br><span class="line"></span><br><span class="line">Disconnected from the target VM, address: '127.0.0.1:45315', transport: 'socket'</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure><p>至此,IDEA已经能连接上yarn集群。</p><h1 id="开启远程yarn-client-debug调试"><a href="#开启远程yarn-client-debug调试" class="headerlink" title="开启远程yarn client debug调试"></a>开启远程yarn client debug调试</h1><h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><p>将我们的应用程序打包(在hadoop05机器)，发送到hadoop01机器，并在hadoop01上启动我们的应用程序,具体如下：  </p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/hadoop/worker/sparklearning/target/   </span><br><span class="line"></span><br><span class="line">scp -r sparklearning-1.0-SNAPSHOT.jar  hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/</span><br><span class="line"></span><br><span class="line">scp -r sparklearning-1.0-SNAPSHOT.jar  hadoop@hadoop01:/home/hadoop/worker/sparklearning/target/</span><br></pre></td></tr></table></figure><p><strong>在hadoop01提交spark应用</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class com.xh.spark.sql.practice.JDSaleQuotaRevert \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--executor-cores 6 \</span><br><span class="line">--driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005" \</span><br><span class="line">--conf "spark.executor.extraJavaOptions=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005" \</span><br><span class="line">/home/hadoop/worker/sparklearning/target/sparklearning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></p><h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><p>在idea中Edit Configurations ——&gt; 点击+号 ——&gt;Remote —— Configuration ——&gt; Debugger Mode (用Attach to remote JVM)——&gt;host(hadoop01)<br>——&gt; Port(默认5005，也可以选择使用未使用的端口，比如5656) ——&gt; Command line arguments for remote JVN (-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005)</p><p>然后点击小虫子图标就可以run程序了！！！！</p><h1 id="问题解决分享"><a href="#问题解决分享" class="headerlink" title="问题解决分享"></a>问题解决分享</h1><h2 id="Could-not-parse-Master-URL"><a href="#Could-not-parse-Master-URL" class="headerlink" title="Could not parse Master URL"></a>Could not parse Master URL</h2><p><strong>解决：</strong><br> 在pom.xml中添加如下依赖<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-yarn_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="Name-node-is-in-safe-mode"><a href="#Name-node-is-in-safe-mode" class="headerlink" title="Name node is in safe mode"></a>Name node is in safe mode</h2><p><strong>解决：</strong><br>在hadoop所在linux环境输入如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure></p><h2 id="Exception-in-thread-“main”-org-apache-hadoop-security-AccessControlException"><a href="#Exception-in-thread-“main”-org-apache-hadoop-security-AccessControlException" class="headerlink" title="Exception in thread “main” org.apache.hadoop.security.AccessControlException:"></a>Exception in thread “main” org.apache.hadoop.security.AccessControlException:</h2><blockquote><p>Exception in thread “main” org.apache.hadoop.security.AccessControlException: Permission denied: user=deeplearning, access=WRITE, inode=”/user/deeplearning/.sparkStaging/application_1554947367832_0002”:hadoop:supergroup:drwxr-xr-x</p></blockquote><p><strong>解决：</strong><br>在hdfs-site.xml添加如下配置<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="Diagnostics-Container-pid-3293-containerID-container-e12-1555047553207-0002-02-000001-is-running-beyond-virtual-memory-limits-Current-usage-116-1-MB-of-1-GB-physical-memory-used-2-3-GB-of-2-1-GB-virtual-memory-used-Killing-container"><a href="#Diagnostics-Container-pid-3293-containerID-container-e12-1555047553207-0002-02-000001-is-running-beyond-virtual-memory-limits-Current-usage-116-1-MB-of-1-GB-physical-memory-used-2-3-GB-of-2-1-GB-virtual-memory-used-Killing-container" class="headerlink" title="Diagnostics: Container [pid=3293,containerID=container_e12_1555047553207_0002_02_000001] is running beyond virtual memory limits. Current usage: 116.1 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container."></a>Diagnostics: Container [pid=3293,containerID=container_e12_1555047553207_0002_02_000001] is running beyond virtual memory limits. Current usage: 116.1 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container.</h2><p><strong>解决：</strong> 在yarn-site.xml文件中添加如下配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><h2 id="问题解决参考文章"><a href="#问题解决参考文章" class="headerlink" title="问题解决参考文章"></a>问题解决参考文章</h2><p><a href="https://stackoverflow.com/questions/41054700/could-not-parse-master-url" target="_blank" rel="noopener">https://stackoverflow.com/questions/41054700/could-not-parse-master-url</a></p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p><a href="http://www.cnblogs.com/lisi2016/p/6863923.html" target="_blank" rel="noopener">http://www.cnblogs.com/lisi2016/p/6863923.html</a>  </p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢!"></a>致谢!</h1><p>如有遇到问题，将问题发送至本人邮箱(<a href="mailto:t_spider@aliyun.com" target="_blank" rel="noopener">t_spider@aliyun.com</a>)。欢迎大家一起讨论问题！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;idea 中连接yarn集群debug spark代码,有利于定位线上问题。&lt;/p&gt;
&lt;h1 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; c
      
    
    </summary>
    
      <category term="spark" scheme="https://tgluon.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>scala正则表达式实战</title>
    <link href="https://tgluon.github.io/2019/05/22/scala%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AE%9E%E6%88%98/"/>
    <id>https://tgluon.github.io/2019/05/22/scala正则表达式实战/</id>
    <published>2019-05-22T06:47:29.253Z</published>
    <updated>2019-05-22T06:47:29.253Z</updated>
    
    <content type="html"><![CDATA[<h1 id="案例一"><a href="#案例一" class="headerlink" title="案例一"></a>案例一</h1><p><strong>说明</strong>：源码来自spark ConfigReader类<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">ConfigReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">REF_RE</span> = <span class="string">"\\$\\&#123;(?:(\\w+?):)?(\\S+?)\\&#125;"</span>.r</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">substitute</span></span>(input: <span class="type">String</span>, usedRefs: <span class="type">Set</span>[<span class="type">String</span>]): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (input != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="type">ConfigReader</span>.<span class="type">REF_RE</span>.replaceAllIn(input, &#123; m =&gt;</span><br><span class="line">        <span class="keyword">val</span> prefix = m.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> name = m.group(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">val</span> ref = <span class="keyword">if</span> (prefix == <span class="literal">null</span>) name <span class="keyword">else</span> <span class="string">s"<span class="subst">$prefix</span>:<span class="subst">$name</span>"</span></span><br><span class="line">        require(!usedRefs.contains(ref), <span class="string">s"Circular reference in <span class="subst">$input</span>: <span class="subst">$ref</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> replacement = bindings.get(prefix)</span><br><span class="line">          .flatMap(getOrDefault(_, name))</span><br><span class="line">          .map &#123; v =&gt; substitute(v, usedRefs + ref) &#125;</span><br><span class="line">          .getOrElse(m.matched)</span><br><span class="line">        <span class="type">Regex</span>.quoteReplacement(replacement)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      input</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;案例一&quot;&gt;&lt;a href=&quot;#案例一&quot; class=&quot;headerlink&quot; title=&quot;案例一&quot;&gt;&lt;/a&gt;案例一&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：源码来自spark ConfigReader类&lt;br&gt;&lt;figure class=&quot;high
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch学习第四弹：聚合案例</title>
    <link href="https://tgluon.github.io/2019/04/25/elasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E5%BC%B9%EF%BC%9A%E8%81%9A%E5%90%88%E6%A1%88%E4%BE%8B/"/>
    <id>https://tgluon.github.io/2019/04/25/elasticsearch学习第四弹：聚合案例/</id>
    <published>2019-04-25T10:57:58.199Z</published>
    <updated>2019-04-25T10:57:58.199Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算每个tag下的商品数量"><a href="#计算每个tag下的商品数量" class="headerlink" title="计算每个tag下的商品数量"></a>计算每个tag下的商品数量</h1><p>将文本field的fielddata属性设置为true<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /ecommerce/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"properties"</span>: &#123;</span><br><span class="line">    <span class="string">"tags"</span>: &#123;</span><br><span class="line">      <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">      <span class="string">"fielddata"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"aggs"</span>: &#123;</span><br><span class="line">    <span class="string">"group_by_tags"</span>: &#123;</span><br><span class="line">      <span class="string">"terms"</span>: &#123; <span class="string">"field"</span>: <span class="string">"tags"</span> &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="对名称中包含yagao的商品，计算每个tag下的商品数量"><a href="#对名称中包含yagao的商品，计算每个tag下的商品数量" class="headerlink" title="对名称中包含yagao的商品，计算每个tag下的商品数量"></a>对名称中包含yagao的商品，计算每个tag下的商品数量</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"size"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="string">"query"</span>: &#123;</span><br><span class="line">    <span class="string">"match"</span>: &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"yagao"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"aggs"</span>: &#123;</span><br><span class="line">    <span class="string">"all_tags"</span>: &#123;</span><br><span class="line">      <span class="string">"terms"</span>: &#123;</span><br><span class="line">        <span class="string">"field"</span>: <span class="string">"tags"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="先分组，再算每组的平均值，计算每个tag下的商品的平均价格"><a href="#先分组，再算每组的平均值，计算每个tag下的商品的平均价格" class="headerlink" title="先分组，再算每组的平均值，计算每个tag下的商品的平均价格"></a>先分组，再算每组的平均值，计算每个tag下的商品的平均价格</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"size"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"aggs"</span> : &#123;</span><br><span class="line">        <span class="string">"group_by_tags"</span> : &#123;</span><br><span class="line">            <span class="string">"terms"</span> : &#123; <span class="string">"field"</span> : <span class="string">"tags"</span> &#125;,</span><br><span class="line">            <span class="string">"aggs"</span> : &#123;</span><br><span class="line">                <span class="string">"avg_price"</span> : &#123;</span><br><span class="line">                    <span class="string">"avg"</span> : &#123; <span class="string">"field"</span> : <span class="string">"price"</span> &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="计算每个tag下的商品的平均价格，并且按照平均价格降序排序"><a href="#计算每个tag下的商品的平均价格，并且按照平均价格降序排序" class="headerlink" title="计算每个tag下的商品的平均价格，并且按照平均价格降序排序"></a>计算每个tag下的商品的平均价格，并且按照平均价格降序排序</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"size"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"aggs"</span> : &#123;</span><br><span class="line">        <span class="string">"all_tags"</span> : &#123;</span><br><span class="line">            <span class="string">"terms"</span> : &#123; <span class="string">"field"</span> : <span class="string">"tags"</span>, <span class="string">"order"</span>: &#123; <span class="string">"avg_price"</span>: <span class="string">"desc"</span> &#125; &#125;,</span><br><span class="line">            <span class="string">"aggs"</span> : &#123;</span><br><span class="line">                <span class="string">"avg_price"</span> : &#123;</span><br><span class="line">                    <span class="string">"avg"</span> : &#123; <span class="string">"field"</span> : <span class="string">"price"</span> &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格"><a href="#按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格" class="headerlink" title="按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格"></a>按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"size"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="string">"aggs"</span>: &#123;</span><br><span class="line">    <span class="string">"group_by_price"</span>: &#123;</span><br><span class="line">      <span class="string">"range"</span>: &#123;</span><br><span class="line">        <span class="string">"field"</span>: <span class="string">"price"</span>,</span><br><span class="line">        <span class="string">"ranges"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"from"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"to"</span>: <span class="number">20</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"from"</span>: <span class="number">20</span>,</span><br><span class="line">            <span class="string">"to"</span>: <span class="number">40</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"from"</span>: <span class="number">40</span>,</span><br><span class="line">            <span class="string">"to"</span>: <span class="number">50</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"aggs"</span>: &#123;</span><br><span class="line">        <span class="string">"group_by_tags"</span>: &#123;</span><br><span class="line">          <span class="string">"terms"</span>: &#123;</span><br><span class="line">            <span class="string">"field"</span>: <span class="string">"tags"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="string">"aggs"</span>: &#123;</span><br><span class="line">            <span class="string">"average_price"</span>: &#123;</span><br><span class="line">              <span class="string">"avg"</span>: &#123;</span><br><span class="line">                <span class="string">"field"</span>: <span class="string">"price"</span></span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;计算每个tag下的商品数量&quot;&gt;&lt;a href=&quot;#计算每个tag下的商品数量&quot; class=&quot;headerlink&quot; title=&quot;计算每个tag下的商品数量&quot;&gt;&lt;/a&gt;计算每个tag下的商品数量&lt;/h1&gt;&lt;p&gt;将文本field的fielddata属性设置为tru
      
    
    </summary>
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch学习第二弹：CRUD操作</title>
    <link href="https://tgluon.github.io/2019/04/25/elasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%BC%B9%EF%BC%9ACRUD%E6%93%8D%E4%BD%9C/"/>
    <id>https://tgluon.github.io/2019/04/25/elasticsearch学习第二弹：CRUD操作/</id>
    <published>2019-04-25T10:57:58.195Z</published>
    <updated>2019-04-25T10:57:58.195Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简单的索引操作"><a href="#简单的索引操作" class="headerlink" title="简单的索引操作"></a>简单的索引操作</h1><h2 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /test_index?pretty</span><br></pre></td></tr></table></figure><h2 id="查看索引"><a href="#查看索引" class="headerlink" title="查看索引"></a>查看索引</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> _cat/indices?v</span><br></pre></td></tr></table></figure><h2 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DELETE</span> /test_index?pretty</span><br></pre></td></tr></table></figure><h1 id="CRUD文档"><a href="#CRUD文档" class="headerlink" title="CRUD文档"></a>CRUD文档</h1><p>es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索。</p><h2 id="新增文档"><a href="#新增文档" class="headerlink" title="新增文档"></a>新增文档</h2><p><strong>格式</strong>：<br><strong>注意</strong>：elasticsearch7.0已经没有type。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /index/_doc/id</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"json数据"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /ecommerce/_doc/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"name"</span> : <span class="string">"jiaqiangban gaolujie yagao"</span>,</span><br><span class="line">    <span class="string">"desc"</span> :  <span class="string">"gaoxiao meibai"</span>,</span><br><span class="line">    <span class="string">"price"</span> :  <span class="number">30</span>,</span><br><span class="line">    <span class="string">"producer"</span> :      <span class="string">"gaolujie producer"</span>,</span><br><span class="line">    <span class="string">"tags"</span>: [ <span class="string">"meibai"</span>, <span class="string">"fangzhu"</span> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="查询文档"><a href="#查询文档" class="headerlink" title="查询文档"></a>查询文档</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_doc/<span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>sql方式</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">POST</span> /_xpack/sql?format=txt</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span>: <span class="string">"select name,price from ecommerce where price &gt; 20"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="修改文档"><a href="#修改文档" class="headerlink" title="修改文档"></a>修改文档</h2><p><strong>两种方式</strong>：一种替换、一种更新的方式。   </p><h3 id="替换方式"><a href="#替换方式" class="headerlink" title="替换方式"></a>替换方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /ecommerce/_doc/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"name"</span> : <span class="string">"jiaqiangban gaolujie yagao"</span>,</span><br><span class="line">    <span class="string">"desc"</span> :  <span class="string">"gaoxiao meibai"</span>,</span><br><span class="line">    <span class="string">"price"</span> :  <span class="number">40</span>,</span><br><span class="line">    <span class="string">"producer"</span> :      <span class="string">"gaolujie producer"</span>,</span><br><span class="line">    <span class="string">"tags"</span>: [ <span class="string">"meibai"</span>, <span class="string">"fangzhu"</span> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>缺点</strong>：替换方式有一个不好，必须带上所有的field，才能去进行信息的修改。</p><h3 id="更新方式"><a href="#更新方式" class="headerlink" title="更新方式"></a>更新方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">POST</span> /ecommerce/_update/<span class="number">1</span>/</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"doc"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"jiaqiangban gaolujie yagao"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DELETE</span> /ecommerce/_doc/<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简单的索引操作&quot;&gt;&lt;a href=&quot;#简单的索引操作&quot; class=&quot;headerlink&quot; title=&quot;简单的索引操作&quot;&gt;&lt;/a&gt;简单的索引操作&lt;/h1&gt;&lt;h2 id=&quot;创建索引&quot;&gt;&lt;a href=&quot;#创建索引&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch学习第三弹：查询的几种方式</title>
    <link href="https://tgluon.github.io/2019/04/25/elasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%BC%B9%EF%BC%9A%E6%9F%A5%E8%AF%A2%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>https://tgluon.github.io/2019/04/25/elasticsearch学习第三弹：查询的几种方式/</id>
    <published>2019-04-25T10:57:58.191Z</published>
    <updated>2019-04-25T10:57:58.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PUT</span> /ecommerce/_doc/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"jiajieshi yagao"</span>,</span><br><span class="line">  <span class="string">"desc"</span>: <span class="string">"youxiao fangzhu"</span>,</span><br><span class="line">  <span class="string">"price"</span>: <span class="number">25</span>,</span><br><span class="line">  <span class="string">"producer"</span>: <span class="string">"jiajieshi producer"</span>,</span><br><span class="line">  <span class="string">"tags"</span>: [</span><br><span class="line">    <span class="string">"fangzhu"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">PUT</span> /ecommerce/_doc/<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"jiajieshi yagao"</span>,</span><br><span class="line">  <span class="string">"desc"</span>: <span class="string">"youxiao fangzhu"</span>,</span><br><span class="line">  <span class="string">"price"</span>: <span class="number">25</span>,</span><br><span class="line">  <span class="string">"producer"</span>: <span class="string">"jiajieshi producer"</span>,</span><br><span class="line">  <span class="string">"tags"</span>: [</span><br><span class="line">    <span class="string">"fangzhu"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">PUT</span> /ecommerce/_doc/<span class="number">3</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"zhonghua yagao"</span>,</span><br><span class="line">  <span class="string">"desc"</span>: <span class="string">"caoben zhiwu"</span>,</span><br><span class="line">  <span class="string">"price"</span>: <span class="number">40</span>,</span><br><span class="line">  <span class="string">"producer"</span>: <span class="string">"zhonghua producer"</span>,</span><br><span class="line">  <span class="string">"tags"</span>: [</span><br><span class="line">    <span class="string">"qingxin"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="query-string-search"><a href="#query-string-search" class="headerlink" title="query string search"></a>query string search</h1><p>搜索全部:GET /ecommerce/_search/<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search/</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong>：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"took"</span> : <span class="number">48</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"_shards"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"successful"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"skipped"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"failed"</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"hits"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : &#123;</span><br><span class="line">      <span class="string">"value"</span> : <span class="number">2</span>,</span><br><span class="line">      <span class="string">"relation"</span> : <span class="string">"eq"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"max_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"hits"</span> : [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"_index"</span> : <span class="string">"ecommerce"</span>,</span><br><span class="line">        <span class="string">"_type"</span> : <span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"_id"</span> : <span class="string">"PBRZU2oB2AOCCrYzyACo"</span>,</span><br><span class="line">        <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"_source"</span> : &#123;</span><br><span class="line">          <span class="string">"name"</span> : <span class="string">"gaolujie yagao"</span>,</span><br><span class="line">          <span class="string">"desc"</span> : <span class="string">"gaoxiao meibai"</span>,</span><br><span class="line">          <span class="string">"price"</span> : <span class="number">30</span>,</span><br><span class="line">          <span class="string">"producer"</span> : <span class="string">"gaolujie producer"</span>,</span><br><span class="line">          <span class="string">"tags"</span> : [</span><br><span class="line">            <span class="string">"meibai"</span>,</span><br><span class="line">            <span class="string">"fangzhu"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"_index"</span> : <span class="string">"ecommerce"</span>,</span><br><span class="line">        <span class="string">"_type"</span> : <span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"_source"</span> : &#123;</span><br><span class="line">          <span class="string">"name"</span> : <span class="string">"jiaqiangban gaolujie yagao"</span>,</span><br><span class="line">          <span class="string">"desc"</span> : <span class="string">"gaoxiao meibai"</span>,</span><br><span class="line">          <span class="string">"price"</span> : <span class="number">30</span>,</span><br><span class="line">          <span class="string">"producer"</span> : <span class="string">"gaolujie producer"</span>,</span><br><span class="line">          <span class="string">"tags"</span> : [</span><br><span class="line">            <span class="string">"meibai"</span>,</span><br><span class="line">            <span class="string">"fangzhu"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>took</strong>：耗费了几毫秒。<br><strong>timed_out</strong>：是否超时，这里是没有。<br>_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）。<br><strong>hits.total</strong>：查询结果的数量，3个document。<br><strong>hits.max_score</strong>：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高。<br><strong>hits.hits</strong>：包含了匹配搜索的document的详细数据 。</p><h1 id="query-DSL"><a href="#query-DSL" class="headerlink" title="query DSL"></a>query DSL</h1><p>DSL：Domain Specified Language，特定领域的语言<br>http request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了。</p><p>查询所有的商品<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"query"</span>: &#123; <span class="string">"match_all"</span>: &#123;&#125; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>查询名称包含yagao的商品，同时按照价格降序排序<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"query"</span>: &#123;</span><br><span class="line">   <span class="string">"match"</span>: &#123;</span><br><span class="line">     <span class="string">"name"</span>: <span class="string">"yagao"</span></span><br><span class="line">   &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"sort"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"price"</span>: &#123;</span><br><span class="line">        <span class="string">"order"</span>: <span class="string">"desc"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>分页查询商品，总共2条商品，假设每页就显示1条商品，现在显示第2页，所以就查出来第2个商品.</p><p>指定要查询出来商品的名称和价格就可以<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"query"</span>: &#123;</span><br><span class="line">    <span class="string">"match_all"</span>: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"_source"</span>:[<span class="string">"name"</span>,<span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="query-filter"><a href="#query-filter" class="headerlink" title="query filter"></a>query filter</h1><p>搜索商品名称包含yagao，而且售价大于25元的商品<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"bool"</span> : &#123;</span><br><span class="line">            <span class="string">"must"</span> : &#123;</span><br><span class="line">                <span class="string">"match"</span> : &#123;</span><br><span class="line">                    <span class="string">"name"</span> : <span class="string">"yagao"</span> </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"filter"</span> : &#123;</span><br><span class="line">                <span class="string">"range"</span> : &#123;</span><br><span class="line">                    <span class="string">"price"</span> : &#123; <span class="string">"gt"</span> : <span class="number">30</span> &#125; </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="full-text-search（全文检索）"><a href="#full-text-search（全文检索）" class="headerlink" title="full-text search（全文检索）"></a>full-text search（全文检索）</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"match"</span> : &#123;</span><br><span class="line">            <span class="string">"producer"</span> : <span class="string">"yagao producer"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="phrase-search（短语搜索）"><a href="#phrase-search（短语搜索）" class="headerlink" title="phrase search（短语搜索）"></a>phrase search（短语搜索）</h1><p>跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回<br>phrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"match_phrase"</span> : &#123;</span><br><span class="line">            <span class="string">"producer"</span> : <span class="string">"yagao producer"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="highlight-search（高亮搜索结果）"><a href="#highlight-search（高亮搜索结果）" class="headerlink" title="highlight search（高亮搜索结果）"></a>highlight search（高亮搜索结果）</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"match"</span> : &#123;</span><br><span class="line">            <span class="string">"producer"</span> : <span class="string">"producer"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"highlight"</span>: &#123;</span><br><span class="line">        <span class="string">"fields"</span> : &#123;</span><br><span class="line">            <span class="string">"producer"</span> : &#123;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;添加数据&quot;&gt;&lt;a href=&quot;#添加数据&quot; class=&quot;headerlink&quot; title=&quot;添加数据&quot;&gt;&lt;/a&gt;添加数据&lt;/h1&gt;&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr
      
    
    </summary>
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch学习第一弹：集群搭建</title>
    <link href="https://tgluon.github.io/2019/04/25/elasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E5%BC%B9%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>https://tgluon.github.io/2019/04/25/elasticsearch学习第一弹：集群搭建/</id>
    <published>2019-04-25T10:57:58.178Z</published>
    <updated>2019-04-25T10:57:58.178Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h1><p><strong>版本</strong>： elasticsearch7.0</p><h2 id="下载资源包"><a href="#下载资源包" class="headerlink" title="下载资源包"></a>下载资源包</h2><p>到官方网站 <a href="https://www.elastic.co/downloads/" target="_blank" rel="noopener">https://www.elastic.co/downloads/</a> 分别下载需要安装的组件 </p><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th>主机名</th><th>ip</th><th>组件</th><th>节点类型 </th></tr></thead><tbody><tr><td>hadoop01</td><td>192.168.8.81</td><td>elasticsearch、kibana</td><td>主节点、非数据节点</td></tr><tr><td>hadoop02</td><td>192.168.8.82</td><td>elasticsearch</td><td>主节点、非数据节点</td></tr><tr><td>hadoop03</td><td>192.168.8.83</td><td>elasticsearch</td><td>非主节点、数据节点</td></tr></tbody></table><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>elasticsearch.yml  </p><h3 id="hadoop01"><a href="#hadoop01" class="headerlink" title="hadoop01"></a>hadoop01</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"># ======================== <span class="type">Elasticsearch</span> <span class="type">Configuration</span> =========================</span><br><span class="line">#</span><br><span class="line"># <span class="type">NOTE</span>: <span class="type">Elasticsearch</span> comes <span class="keyword">with</span> reasonable defaults <span class="keyword">for</span> most settings.</span><br><span class="line">#       <span class="type">Before</span> you set out to tweak and tune the configuration, make sure you</span><br><span class="line">#       understand what are you trying to accomplish and the consequences.</span><br><span class="line">#</span><br><span class="line"># <span class="type">The</span> primary way of configuring a node is via <span class="keyword">this</span> file. <span class="type">This</span> template lists</span><br><span class="line"># the most important settings you may want to configure <span class="keyword">for</span> a production cluster.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Please</span> consult the documentation <span class="keyword">for</span> further information on configuration options:</span><br><span class="line"># https:<span class="comment">//www.elastic.co/guide/en/elasticsearch/reference/index.html</span></span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Cluster</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> your cluster:</span><br><span class="line">#</span><br><span class="line">#cluster.name: my-application</span><br><span class="line">cluster.name: cluster</span><br><span class="line">#</span><br><span class="line"># ------------------------------------ <span class="type">Node</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> the node:</span><br><span class="line">#</span><br><span class="line">#node.name: node<span class="number">-1</span></span><br><span class="line">node.name: node<span class="number">-1</span></span><br><span class="line">node.master: <span class="literal">true</span></span><br><span class="line">node.data: <span class="literal">false</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Add</span> custom attributes to the node:</span><br><span class="line">#</span><br><span class="line">#node.attr.rack: r1</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Paths</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to directory where to store the data (separate multiple locations by comma):</span><br><span class="line">#</span><br><span class="line">#path.data: /path/to/data</span><br><span class="line">path.data: /home/hadoop/elasticsearch/data</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to log files:</span><br><span class="line">#</span><br><span class="line">#path.logs: /path/to/logs</span><br><span class="line">path.logs: /home/hadoop/elasticsearch/logs</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Memory</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Lock</span> the memory on startup:</span><br><span class="line">#</span><br><span class="line">#bootstrap.memory_lock: <span class="literal">true</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Make</span> sure that the heap size is set to about half the memory available</span><br><span class="line"># on the system and that the owner of the process is allowed to use <span class="keyword">this</span></span><br><span class="line"># limit.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Elasticsearch</span> performs poorly when the system is swapping the memory.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Network</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> the bind address to a specific <span class="type">IP</span> (<span class="type">IPv4</span> or <span class="type">IPv6</span>):</span><br><span class="line">#</span><br><span class="line">#network.host: <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">network.host: <span class="number">192.168</span><span class="number">.8</span><span class="number">.81</span></span><br><span class="line">network.bind_host: <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> a custom port <span class="keyword">for</span> <span class="type">HTTP</span>:</span><br><span class="line">#</span><br><span class="line">#http.port: <span class="number">9200</span></span><br><span class="line">http.port: <span class="number">9200</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the network module documentation.</span><br><span class="line">#</span><br><span class="line"># --------------------------------- <span class="type">Discovery</span> ----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Pass</span> an initial list of hosts to perform discovery when <span class="keyword">this</span> node is started:</span><br><span class="line"># <span class="type">The</span> <span class="keyword">default</span> list of hosts is [<span class="string">"127.0.0.1"</span>, <span class="string">"[::1]"</span>]</span><br><span class="line">#</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"host1"</span>, <span class="string">"host2"</span>]</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"192.168.8.81"</span>,<span class="string">"192.168.8.82"</span>,<span class="string">"192.168.8.83"</span>]</span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"192.168.8.81"</span>, <span class="string">"192.168.8.82"</span>, <span class="string">"192.168.8.83"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">Bootstrap</span> the cluster using an initial set of master-eligible nodes:</span><br><span class="line">#</span><br><span class="line">#cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#可能成为主的节点</span><br><span class="line">cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the discovery and cluster formation module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Gateway</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Block</span> initial recovery after a full cluster restart until <span class="type">N</span> nodes are started:</span><br><span class="line">#</span><br><span class="line">#gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#当集群内达到<span class="number">3</span>个时开始恢复数据（防止集群启动时部分节点自动恢复数据）</span><br><span class="line">gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the gateway module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Various</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Require</span> explicit names when deleting indices:</span><br><span class="line">#</span><br><span class="line">#action.destructive_requires_name: <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="hadoop04"><a href="#hadoop04" class="headerlink" title="hadoop04"></a>hadoop04</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"># ======================== <span class="type">Elasticsearch</span> <span class="type">Configuration</span> =========================</span><br><span class="line">#</span><br><span class="line"># <span class="type">NOTE</span>: <span class="type">Elasticsearch</span> comes <span class="keyword">with</span> reasonable defaults <span class="keyword">for</span> most settings.</span><br><span class="line">#       <span class="type">Before</span> you set out to tweak and tune the configuration, make sure you</span><br><span class="line">#       understand what are you trying to accomplish and the consequences.</span><br><span class="line">#</span><br><span class="line"># <span class="type">The</span> primary way of configuring a node is via <span class="keyword">this</span> file. <span class="type">This</span> template lists</span><br><span class="line"># the most important settings you may want to configure <span class="keyword">for</span> a production cluster.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Please</span> consult the documentation <span class="keyword">for</span> further information on configuration options:</span><br><span class="line"># https:<span class="comment">//www.elastic.co/guide/en/elasticsearch/reference/index.html</span></span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Cluster</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> your cluster:</span><br><span class="line">#</span><br><span class="line">#cluster.name: my-application</span><br><span class="line">cluster.name: cluster</span><br><span class="line">#</span><br><span class="line"># ------------------------------------ <span class="type">Node</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> the node:</span><br><span class="line">#</span><br><span class="line">#node.name: node<span class="number">-2</span></span><br><span class="line">node.name: node<span class="number">-1</span></span><br><span class="line">node.master: <span class="literal">true</span></span><br><span class="line">node.data: <span class="literal">false</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Add</span> custom attributes to the node:</span><br><span class="line">#</span><br><span class="line">#node.attr.rack: r1</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Paths</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to directory where to store the data (separate multiple locations by comma):</span><br><span class="line">#</span><br><span class="line">#path.data: /path/to/data</span><br><span class="line">path.data: /home/hadoop/elasticsearch/data</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to log files:</span><br><span class="line">#</span><br><span class="line">#path.logs: /path/to/logs</span><br><span class="line">path.logs: /home/hadoop/elasticsearch/logs</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Memory</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Lock</span> the memory on startup:</span><br><span class="line">#</span><br><span class="line">#bootstrap.memory_lock: <span class="literal">true</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Make</span> sure that the heap size is set to about half the memory available</span><br><span class="line"># on the system and that the owner of the process is allowed to use <span class="keyword">this</span></span><br><span class="line"># limit.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Elasticsearch</span> performs poorly when the system is swapping the memory.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Network</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> the bind address to a specific <span class="type">IP</span> (<span class="type">IPv4</span> or <span class="type">IPv6</span>):</span><br><span class="line">#</span><br><span class="line">#network.host: <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">network.host: <span class="number">192.168</span><span class="number">.8</span><span class="number">.82</span></span><br><span class="line">network.bind_host: <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> a custom port <span class="keyword">for</span> <span class="type">HTTP</span>:</span><br><span class="line">#</span><br><span class="line">#http.port: <span class="number">9200</span></span><br><span class="line">http.port: <span class="number">9200</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the network module documentation.</span><br><span class="line">#</span><br><span class="line"># --------------------------------- <span class="type">Discovery</span> ----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Pass</span> an initial list of hosts to perform discovery when <span class="keyword">this</span> node is started:</span><br><span class="line"># <span class="type">The</span> <span class="keyword">default</span> list of hosts is [<span class="string">"127.0.0.1"</span>, <span class="string">"[::1]"</span>]</span><br><span class="line">#</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"host1"</span>, <span class="string">"host2"</span>]</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"192.168.8.81"</span>,<span class="string">"192.168.8.82"</span>,<span class="string">"192.168.8.83"</span>]</span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"192.168.8.81"</span>, <span class="string">"192.168.8.82"</span>, <span class="string">"192.168.8.83"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">Bootstrap</span> the cluster using an initial set of master-eligible nodes:</span><br><span class="line">#</span><br><span class="line">#cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#可能成为主的节点</span><br><span class="line">cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the discovery and cluster formation module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Gateway</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Block</span> initial recovery after a full cluster restart until <span class="type">N</span> nodes are started:</span><br><span class="line">#</span><br><span class="line">#gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#当集群内达到<span class="number">3</span>个时开始恢复数据（防止集群启动时部分节点自动恢复数据）</span><br><span class="line">gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the gateway module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Various</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Require</span> explicit names when deleting indices:</span><br><span class="line">#</span><br><span class="line">#action.destructive_requires_name: <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="hadoop03"><a href="#hadoop03" class="headerlink" title="hadoop03"></a>hadoop03</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"># ======================== <span class="type">Elasticsearch</span> <span class="type">Configuration</span> =========================</span><br><span class="line">#</span><br><span class="line"># <span class="type">NOTE</span>: <span class="type">Elasticsearch</span> comes <span class="keyword">with</span> reasonable defaults <span class="keyword">for</span> most settings.</span><br><span class="line">#       <span class="type">Before</span> you set out to tweak and tune the configuration, make sure you</span><br><span class="line">#       understand what are you trying to accomplish and the consequences.</span><br><span class="line">#</span><br><span class="line"># <span class="type">The</span> primary way of configuring a node is via <span class="keyword">this</span> file. <span class="type">This</span> template lists</span><br><span class="line"># the most important settings you may want to configure <span class="keyword">for</span> a production cluster.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Please</span> consult the documentation <span class="keyword">for</span> further information on configuration options:</span><br><span class="line"># https:<span class="comment">//www.elastic.co/guide/en/elasticsearch/reference/index.html</span></span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Cluster</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> your cluster:</span><br><span class="line">#</span><br><span class="line">#cluster.name: my-application</span><br><span class="line">cluster.name: cluster</span><br><span class="line">#</span><br><span class="line"># ------------------------------------ <span class="type">Node</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Use</span> a descriptive name <span class="keyword">for</span> the node:</span><br><span class="line">#</span><br><span class="line">#node.name: node<span class="number">-1</span></span><br><span class="line">node.name: node<span class="number">-3</span></span><br><span class="line">node.master: <span class="literal">false</span></span><br><span class="line">node.data: <span class="literal">true</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Add</span> custom attributes to the node:</span><br><span class="line">#</span><br><span class="line">#node.attr.rack: r1</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Paths</span> ------------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to directory where to store the data (separate multiple locations by comma):</span><br><span class="line">#</span><br><span class="line">#path.data: /path/to/data</span><br><span class="line">path.data: /home/hadoop/elasticsearch/data</span><br><span class="line">#</span><br><span class="line"># <span class="type">Path</span> to log files:</span><br><span class="line">#</span><br><span class="line">#path.logs: /path/to/logs</span><br><span class="line">path.logs: /home/hadoop/elasticsearch/logs</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- <span class="type">Memory</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Lock</span> the memory on startup:</span><br><span class="line">#</span><br><span class="line">#bootstrap.memory_lock: <span class="literal">true</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Make</span> sure that the heap size is set to about half the memory available</span><br><span class="line"># on the system and that the owner of the process is allowed to use <span class="keyword">this</span></span><br><span class="line"># limit.</span><br><span class="line">#</span><br><span class="line"># <span class="type">Elasticsearch</span> performs poorly when the system is swapping the memory.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Network</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> the bind address to a specific <span class="type">IP</span> (<span class="type">IPv4</span> or <span class="type">IPv6</span>):</span><br><span class="line">#</span><br><span class="line">#network.host: <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">network.host: <span class="number">192.168</span><span class="number">.8</span><span class="number">.83</span></span><br><span class="line">network.bind_host: <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">Set</span> a custom port <span class="keyword">for</span> <span class="type">HTTP</span>:</span><br><span class="line">#</span><br><span class="line">#http.port: <span class="number">9200</span></span><br><span class="line">http.port: <span class="number">9200</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the network module documentation.</span><br><span class="line">#</span><br><span class="line"># --------------------------------- <span class="type">Discovery</span> ----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Pass</span> an initial list of hosts to perform discovery when <span class="keyword">this</span> node is started:</span><br><span class="line"># <span class="type">The</span> <span class="keyword">default</span> list of hosts is [<span class="string">"127.0.0.1"</span>, <span class="string">"[::1]"</span>]</span><br><span class="line">#</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"host1"</span>, <span class="string">"host2"</span>]</span><br><span class="line">#discovery.seed_hosts: [<span class="string">"192.168.8.81"</span>,<span class="string">"192.168.8.82"</span>,<span class="string">"192.168.8.83"</span>]</span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"192.168.8.81"</span>, <span class="string">"192.168.8.82"</span>, <span class="string">"192.168.8.83"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">Bootstrap</span> the cluster using an initial set of master-eligible nodes:</span><br><span class="line">#</span><br><span class="line">#cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#可能成为主的节点</span><br><span class="line">cluster.initial_master_nodes: [<span class="string">"node-1"</span>, <span class="string">"node-2"</span>]</span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the discovery and cluster formation module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Gateway</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Block</span> initial recovery after a full cluster restart until <span class="type">N</span> nodes are started:</span><br><span class="line">#</span><br><span class="line">#gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#当集群内达到<span class="number">3</span>个时开始恢复数据（防止集群启动时部分节点自动恢复数据）</span><br><span class="line">gateway.recover_after_nodes: <span class="number">3</span></span><br><span class="line">#</span><br><span class="line"># <span class="type">For</span> more information, consult the gateway module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- <span class="type">Various</span> -----------------------------------</span><br><span class="line">#</span><br><span class="line"># <span class="type">Require</span> explicit names when deleting indices:</span><br><span class="line">#</span><br><span class="line">#action.destructive_requires_name: <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="kibana配置"><a href="#kibana配置" class="headerlink" title="kibana配置"></a>kibana配置</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"># <span class="type">Kibana</span> is served by a back end server. <span class="type">This</span> setting specifies the port to use.</span><br><span class="line">#server.port: <span class="number">5601</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Specifies</span> the address to which the <span class="type">Kibana</span> server will bind. <span class="type">IP</span> addresses and host names are both valid values.</span><br><span class="line"># <span class="type">The</span> <span class="keyword">default</span> is <span class="symbol">'localhos</span>t', which usually means remote machines will not be able to connect.</span><br><span class="line"># <span class="type">To</span> allow connections from remote users, set <span class="keyword">this</span> parameter to a non-loopback address.</span><br><span class="line">server.host: <span class="string">"hadoop01"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Enables</span> you to specify a path to mount <span class="type">Kibana</span> at <span class="keyword">if</span> you are running behind a proxy.</span><br><span class="line"># <span class="type">Use</span> the `server.rewriteBasePath` setting to tell <span class="type">Kibana</span> <span class="keyword">if</span> it should remove the basePath</span><br><span class="line"># from requests it receives, and to prevent a deprecation warning at startup.</span><br><span class="line"># <span class="type">This</span> setting cannot end in a slash.</span><br><span class="line">#server.basePath: <span class="string">""</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Specifies</span> whether <span class="type">Kibana</span> should rewrite requests that are prefixed <span class="keyword">with</span></span><br><span class="line"># `server.basePath` or require that they are rewritten by your reverse proxy.</span><br><span class="line"># <span class="type">This</span> setting was effectively always `<span class="literal">false</span>` before <span class="type">Kibana</span> <span class="number">6.3</span> and will</span><br><span class="line"># <span class="keyword">default</span> to `<span class="literal">true</span>` starting in <span class="type">Kibana</span> <span class="number">7.0</span>.</span><br><span class="line">#server.rewriteBasePath: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="type">The</span> maximum payload size in bytes <span class="keyword">for</span> incoming server requests.</span><br><span class="line">#server.maxPayloadBytes: <span class="number">1048576</span></span><br><span class="line"></span><br><span class="line"># <span class="type">The</span> <span class="type">Kibana</span> server<span class="symbol">'s</span> name.  <span class="type">This</span> is used <span class="keyword">for</span> display purposes.</span><br><span class="line">server.name: <span class="string">"kibana"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">The</span> <span class="type">URLs</span> of the <span class="type">Elasticsearch</span> instances to use <span class="keyword">for</span> all your queries.</span><br><span class="line">elasticsearch.hosts: [<span class="string">"http://hadoop01:9200"</span>]</span><br><span class="line"></span><br><span class="line"># <span class="type">When</span> <span class="keyword">this</span> setting<span class="symbol">'s</span> value is <span class="literal">true</span> <span class="type">Kibana</span> uses the hostname specified in the server.host</span><br><span class="line"># setting. <span class="type">When</span> the value of <span class="keyword">this</span> setting is <span class="literal">false</span>, <span class="type">Kibana</span> uses the hostname of the host</span><br><span class="line"># that connects to <span class="keyword">this</span> <span class="type">Kibana</span> instance.</span><br><span class="line">#elasticsearch.preserveHost: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Kibana</span> uses an index in <span class="type">Elasticsearch</span> to store saved searches, visualizations and</span><br><span class="line"># dashboards. <span class="type">Kibana</span> creates a <span class="keyword">new</span> index <span class="keyword">if</span> the index doesn<span class="symbol">'t</span> already exist.</span><br><span class="line">#kibana.index: <span class="string">".kibana"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">The</span> <span class="keyword">default</span> application to load.</span><br><span class="line">#kibana.defaultAppId: <span class="string">"home"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">If</span> your <span class="type">Elasticsearch</span> is <span class="keyword">protected</span> <span class="keyword">with</span> basic authentication, these settings provide</span><br><span class="line"># the username and password that the <span class="type">Kibana</span> server uses to perform maintenance on the <span class="type">Kibana</span></span><br><span class="line"># index at startup. <span class="type">Your</span> <span class="type">Kibana</span> users still need to authenticate <span class="keyword">with</span> <span class="type">Elasticsearch</span>, which</span><br><span class="line"># is proxied through the <span class="type">Kibana</span> server.</span><br><span class="line">#elasticsearch.username: <span class="string">"user"</span></span><br><span class="line">#elasticsearch.password: <span class="string">"pass"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Enables</span> <span class="type">SSL</span> and paths to the <span class="type">PEM</span>-format <span class="type">SSL</span> certificate and <span class="type">SSL</span> key files, respectively.</span><br><span class="line"># <span class="type">These</span> settings enable <span class="type">SSL</span> <span class="keyword">for</span> outgoing requests from the <span class="type">Kibana</span> server to the browser.</span><br><span class="line">#server.ssl.enabled: <span class="literal">false</span></span><br><span class="line">#server.ssl.certificate: /path/to/your/server.crt</span><br><span class="line">#server.ssl.key: /path/to/your/server.key</span><br><span class="line"></span><br><span class="line"># <span class="type">Optional</span> settings that provide the paths to the <span class="type">PEM</span>-format <span class="type">SSL</span> certificate and key files.</span><br><span class="line"># <span class="type">These</span> files validate that your <span class="type">Elasticsearch</span> backend uses the same key files.</span><br><span class="line">#elasticsearch.ssl.certificate: /path/to/your/client.crt</span><br><span class="line">#elasticsearch.ssl.key: /path/to/your/client.key</span><br><span class="line"></span><br><span class="line"># <span class="type">Optional</span> setting that enables you to specify a path to the <span class="type">PEM</span> file <span class="keyword">for</span> the certificate</span><br><span class="line"># authority <span class="keyword">for</span> your <span class="type">Elasticsearch</span> instance.</span><br><span class="line">#elasticsearch.ssl.certificateAuthorities: [ <span class="string">"/path/to/your/CA.pem"</span> ]</span><br><span class="line"></span><br><span class="line"># <span class="type">To</span> disregard the validity of <span class="type">SSL</span> certificates, change <span class="keyword">this</span> setting<span class="symbol">'s</span> value to <span class="symbol">'non</span>e'.</span><br><span class="line">#elasticsearch.ssl.verificationMode: full</span><br><span class="line"></span><br><span class="line"># <span class="type">Time</span> in milliseconds to wait <span class="keyword">for</span> <span class="type">Elasticsearch</span> to respond to pings. <span class="type">Defaults</span> to the value of</span><br><span class="line"># the elasticsearch.requestTimeout setting.</span><br><span class="line">#elasticsearch.pingTimeout: <span class="number">1500</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Time</span> in milliseconds to wait <span class="keyword">for</span> responses from the back end or <span class="type">Elasticsearch</span>. <span class="type">This</span> value</span><br><span class="line"># must be a positive integer.</span><br><span class="line">#elasticsearch.requestTimeout: <span class="number">30000</span></span><br><span class="line"></span><br><span class="line"># <span class="type">List</span> of <span class="type">Kibana</span> client-side headers to send to <span class="type">Elasticsearch</span>. <span class="type">To</span> send *no* client-side</span><br><span class="line"># headers, set <span class="keyword">this</span> value to [] (an empty list).</span><br><span class="line">#elasticsearch.requestHeadersWhitelist: [ authorization ]</span><br><span class="line"></span><br><span class="line"># <span class="type">Header</span> names and values that are sent to <span class="type">Elasticsearch</span>. <span class="type">Any</span> custom headers cannot be overwritten</span><br><span class="line"># by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.</span><br><span class="line">#elasticsearch.customHeaders: &#123;&#125;</span><br><span class="line"></span><br><span class="line"># <span class="type">Time</span> in milliseconds <span class="keyword">for</span> <span class="type">Elasticsearch</span> to wait <span class="keyword">for</span> responses from shards. <span class="type">Set</span> to <span class="number">0</span> to disable.</span><br><span class="line">#elasticsearch.shardTimeout: <span class="number">30000</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Time</span> in milliseconds to wait <span class="keyword">for</span> <span class="type">Elasticsearch</span> at <span class="type">Kibana</span> startup before retrying.</span><br><span class="line">#elasticsearch.startupTimeout: <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Logs</span> queries sent to <span class="type">Elasticsearch</span>. <span class="type">Requires</span> logging.verbose set to <span class="literal">true</span>.</span><br><span class="line">#elasticsearch.logQueries: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Specifies</span> the path where <span class="type">Kibana</span> creates the process <span class="type">ID</span> file.</span><br><span class="line">#pid.file: /<span class="keyword">var</span>/run/kibana.pid</span><br><span class="line"></span><br><span class="line"># <span class="type">Enables</span> you specify a file where <span class="type">Kibana</span> stores log output.</span><br><span class="line">#logging.dest: stdout</span><br><span class="line"></span><br><span class="line"># <span class="type">Set</span> the value of <span class="keyword">this</span> setting to <span class="literal">true</span> to suppress all logging output.</span><br><span class="line">#logging.silent: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Set</span> the value of <span class="keyword">this</span> setting to <span class="literal">true</span> to suppress all logging output other than error messages.</span><br><span class="line">#logging.quiet: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Set</span> the value of <span class="keyword">this</span> setting to <span class="literal">true</span> to log all events, including system usage information</span><br><span class="line"># and all requests.</span><br><span class="line">#logging.verbose: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Set</span> the interval in milliseconds to sample system and process performance</span><br><span class="line"># metrics. <span class="type">Minimum</span> is <span class="number">100</span>ms. <span class="type">Defaults</span> to <span class="number">5000.</span></span><br><span class="line">#ops.interval: <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Specifies</span> locale to be used <span class="keyword">for</span> all localizable strings, dates and number formats.</span><br><span class="line">#i18n.locale: <span class="string">"en"</span></span><br></pre></td></tr></table></figure><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><p><strong>分别进入每一台机器,运行下面命令，启动es集群</strong>：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/elasticsearch -d</span><br></pre></td></tr></table></figure></p><p><strong>启动kibana</strong>：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./kibana &amp;</span><br></pre></td></tr></table></figure></p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>在浏览器输入如下地址：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="comment">//192.168.8.81:9200/_cluster/health?pretty=true</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong>:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"cluster"</span>,</span><br><span class="line">  <span class="string">"status"</span> : <span class="string">"green"</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"number_of_nodes"</span> : <span class="number">3</span>,</span><br><span class="line">  <span class="string">"number_of_data_nodes"</span> : <span class="number">2</span>,</span><br><span class="line">  <span class="string">"active_primary_shards"</span> : <span class="number">4</span>,</span><br><span class="line">  <span class="string">"active_shards"</span> : <span class="number">8</span>,</span><br><span class="line">  <span class="string">"relocating_shards"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"initializing_shards"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"unassigned_shards"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"delayed_unassigned_shards"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"number_of_pending_tasks"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"number_of_in_flight_fetch"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"task_max_waiting_in_queue_millis"</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="string">"active_shards_percent_as_number"</span> : <span class="number">100.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>green表示集群正确启动完成。   </p><p>在浏览器输入如下地址：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="comment">//192.168.8.81:5601</span></span><br></pre></td></tr></table></figure></p><p>在左边工具栏找到Dev Tools,在开发窗口输入一下命令,查看集群健康状态：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GET</span> /_cat/health?v</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">epoch      timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent</span><br><span class="line"><span class="number">1556175058</span> <span class="number">06</span>:<span class="number">50</span>:<span class="number">58</span>  cluster green           <span class="number">3</span>         <span class="number">2</span>      <span class="number">8</span>   <span class="number">4</span>    <span class="number">0</span>    <span class="number">0</span>        <span class="number">0</span>             <span class="number">0</span>                  -                <span class="number">100.0</span>%</span><br></pre></td></tr></table></figure></p><p><strong>集群的健康状况</strong><br><strong>green</strong>：每个索引的primary shard和replica shard都是active状态的。<br><strong>yellow</strong>：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态 。<br><strong>red</strong>：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了。   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;集群搭建&quot;&gt;&lt;a href=&quot;#集群搭建&quot; class=&quot;headerlink&quot; title=&quot;集群搭建&quot;&gt;&lt;/a&gt;集群搭建&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;版本&lt;/strong&gt;： elasticsearch7.0&lt;/p&gt;
&lt;h2 id=&quot;下载资源包&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="https://tgluon.github.io/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>hive自定义UDTF函数</title>
    <link href="https://tgluon.github.io/2019/04/24/hive%20%E8%87%AA%E5%AE%9A%E4%B9%89UDTF%E5%87%BD%E6%95%B0/"/>
    <id>https://tgluon.github.io/2019/04/24/hive 自定义UDTF函数/</id>
    <published>2019-04-24T06:20:26.477Z</published>
    <updated>2019-04-24T06:20:26.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自定义UDTF函数"><a href="#自定义UDTF函数" class="headerlink" title="自定义UDTF函数"></a>自定义UDTF函数</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>用户自定义表生成函数（UDTF）接受零个或多个输入，然后产生多列或多行的输出。要实现UDTF，需要继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF，同时实现三个方法。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a><br><a href="https://javinjunfeng.top/technicalstack/hive/77" target="_blank" rel="noopener">https://javinjunfeng.top/technicalstack/hive/77</a>    </p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="原始数据和转换后的数据"><a href="#原始数据和转换后的数据" class="headerlink" title="原始数据和转换后的数据"></a>原始数据和转换后的数据</h3><p><strong>原始数据</strong>： </p><blockquote><p>sku,category_name,busi_time,shop_id,flag,sale_num,income<br> 21A5+32B2,烟灶,2018-06-29 00:00:00,21,0,6,16056.0   </p></blockquote><p> <strong>转换后的数据</strong>：</p><blockquote><p>rela_sku,rule_id,sku,category_name,busi_time,shop_id,sale_num,income<br> 21A5,0,21A5+32B2,烟机,2018-06-29 00:00:00,21,6,16056.0<br> 32B2,1,21A5+32B2,灶具,2018-06-29 00:00:00,21,6,16056.0</p></blockquote><h3 id="创建自定义函数"><a href="#创建自定义函数" class="headerlink" title="创建自定义函数"></a>创建自定义函数</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xh.spark.sql.hive.udtf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JDQuotaReduction</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector args)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; resType = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldNames.add(<span class="string">"rela_sku"</span>);</span><br><span class="line">        resType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        fieldNames.add(<span class="string">"rule_id"</span>);</span><br><span class="line">        resType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, resType);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] record)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        List&lt;Object&gt; allQuota = (ArrayList&lt;Object&gt;) record[<span class="number">0</span>];</span><br><span class="line">        List&lt;Object&gt; firstQuotaValueOrder = (ArrayList&lt;Object&gt;) allQuota.get(<span class="number">0</span>);</span><br><span class="line">        String[] sku = firstQuotaValueOrder.get(<span class="number">0</span>).toString().split(<span class="string">"\\+"</span>);</span><br><span class="line">        String cate = firstQuotaValueOrder.get(<span class="number">1</span>).toString();</span><br><span class="line">        String flag = firstQuotaValueOrder.get(<span class="number">2</span>).toString();</span><br><span class="line">        <span class="comment">// 烟灶套餐</span></span><br><span class="line">        <span class="keyword">if</span> (cate != <span class="keyword">null</span> &amp;&amp; <span class="string">"烟灶"</span>.equals(cate)) &#123;</span><br><span class="line">            Object[] result = <span class="keyword">new</span> Object[<span class="number">2</span>];</span><br><span class="line">            <span class="comment">//第一件单品</span></span><br><span class="line">            result[<span class="number">0</span>] = sku[<span class="number">0</span>];</span><br><span class="line">            result[<span class="number">1</span>] = <span class="string">"0"</span>;</span><br><span class="line">            forward(result);</span><br><span class="line">            <span class="comment">//第二件的单品</span></span><br><span class="line">            result[<span class="number">0</span>] = sku[<span class="number">1</span>];</span><br><span class="line">            result[<span class="number">1</span>] = <span class="string">"1"</span>;</span><br><span class="line">            forward(result);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 嵌入式套餐</span></span><br><span class="line">        <span class="keyword">if</span> (cate != <span class="keyword">null</span> &amp;&amp; <span class="string">"嵌入式"</span>.equals(cate)) &#123;</span><br><span class="line">            Object[] result = <span class="keyword">new</span> Object[<span class="number">2</span>];</span><br><span class="line">            <span class="comment">// 第一件单品</span></span><br><span class="line">            result[<span class="number">0</span>] = sku[<span class="number">0</span>];</span><br><span class="line">            result[<span class="number">1</span>] = <span class="string">"2"</span>;</span><br><span class="line">            forward(result);</span><br><span class="line">            <span class="comment">// 第二件及之后的单品</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; sku.length; i++) &#123;</span><br><span class="line">                result[<span class="number">0</span>] = sku[i];</span><br><span class="line">                result[<span class="number">1</span>] = <span class="string">"1"</span>;</span><br><span class="line">                forward(result);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 烟灶消套餐及大套系</span></span><br><span class="line">        <span class="keyword">if</span> (cate != <span class="keyword">null</span> &amp;&amp; (<span class="string">"烟灶消"</span>.equals(cate) || <span class="string">"大套系"</span>.equals(cate))) &#123;</span><br><span class="line">            Object[] result = <span class="keyword">new</span> Object[<span class="number">2</span>];</span><br><span class="line">            <span class="comment">// 第二件及之后的单品</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; sku.length; i++) &#123;</span><br><span class="line">                result[<span class="number">0</span>] = sku[i];</span><br><span class="line">                result[<span class="number">1</span>] = <span class="string">"1"</span>;</span><br><span class="line">                forward(result);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (flag != <span class="keyword">null</span> &amp;&amp; <span class="string">"1"</span>.equals(flag)) &#123;</span><br><span class="line">                <span class="comment">// 第一件+第二件的组合是烟灶套餐</span></span><br><span class="line">                result[<span class="number">0</span>] = sku[<span class="number">0</span>] + <span class="string">"+"</span> + sku[<span class="number">1</span>];</span><br><span class="line">                result[<span class="number">1</span>] = <span class="number">3</span>;</span><br><span class="line">                forward(result);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                result[<span class="number">0</span>] = sku[<span class="number">0</span>];</span><br><span class="line">                result[<span class="number">1</span>] = <span class="string">"2"</span>;</span><br><span class="line">                forward(result);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>  transct_quota(</span><br><span class="line">sku <span class="keyword">string</span>,</span><br><span class="line">category_name <span class="keyword">string</span>,</span><br><span class="line">busi_time <span class="keyword">string</span>,</span><br><span class="line">shop_id <span class="keyword">string</span>,</span><br><span class="line">flag <span class="keyword">string</span>,</span><br><span class="line">sale_num <span class="built_in">int</span>,</span><br><span class="line">income <span class="keyword">double</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">stored</span>  <span class="keyword">as</span> textfile</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> transct_quota <span class="keyword">values</span> (<span class="string">"21A5+32B2"</span>,<span class="string">"烟灶"</span>,<span class="string">"2018-06-29 00:00:00"</span>,<span class="string">"21"</span>,<span class="string">"0"</span>,<span class="number">6</span>,<span class="number">16056.0</span>);</span><br></pre></td></tr></table></figure><h3 id="在hive命令窗口添加函数jar包"><a href="#在hive命令窗口添加函数jar包" class="headerlink" title="在hive命令窗口添加函数jar包"></a>在hive命令窗口添加函数jar包</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/hadoop/sparklearning<span class="number">-1.0</span>-<span class="type">SNAPSHOT</span>.jar;</span><br></pre></td></tr></table></figure><h3 id="在hive命令窗口创建临时函数"><a href="#在hive命令窗口创建临时函数" class="headerlink" title="在hive命令窗口创建临时函数"></a>在hive命令窗口创建临时函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create temporary function combine_sku_reduction_rule as <span class="symbol">'com</span>.xh.spark.sql.hive.udtf.<span class="type">JDQuotaReduction</span>';</span><br></pre></td></tr></table></figure><h3 id="使用自定义函数"><a href="#使用自定义函数" class="headerlink" title="使用自定义函数"></a>使用自定义函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> c1,c2,jd_quota.* <span class="keyword">from</span> transct_quota LATERAL <span class="keyword">VIEW</span> combine_sku_reduction_rule(<span class="built_in">array</span>(named_struct(<span class="string">'sku'</span>, goods_sku_id, <span class="string">'cate'</span>, category_name,<span class="string">'flag'</span>,flag))) sku_tab  <span class="keyword">as</span> c1,c2;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;自定义UDTF函数&quot;&gt;&lt;a href=&quot;#自定义UDTF函数&quot; class=&quot;headerlink&quot; title=&quot;自定义UDTF函数&quot;&gt;&lt;/a&gt;自定义UDTF函数&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>java中调用scala</title>
    <link href="https://tgluon.github.io/2019/04/20/java%E4%B8%AD%E8%B0%83%E7%94%A8scala/"/>
    <id>https://tgluon.github.io/2019/04/20/java中调用scala/</id>
    <published>2019-04-20T11:03:09.154Z</published>
    <updated>2019-04-20T11:03:09.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><p><strong>说明</strong>：主要参照spark源码 </p><h2 id="编写scala类及半生对象"><a href="#编写scala类及半生对象" class="headerlink" title="编写scala类及半生对象"></a>编写scala类及半生对象</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.sql.types</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.annotation.<span class="type">Stable</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The data type representing `NULL` values. Please use the singleton `DataTypes.NullType`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @since 1.3.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Stable</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NullType</span> <span class="title">private</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">DataType</span> </span>&#123;</span><br><span class="line">  <span class="comment">// The companion object and this class is separated so the companion object also subclasses</span></span><br><span class="line">  <span class="comment">// this type. Otherwise, the companion object would be of type "NullType$" in byte code.</span></span><br><span class="line">  <span class="comment">// Defined with a private constructor so the companion object is the only possible instantiation.</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultSize</span></span>: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">asNullable</span></span>: <span class="type">NullType</span> = <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @since 1.3.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Stable</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">NullType</span> <span class="keyword">extends</span> <span class="title">NullType</span></span></span><br></pre></td></tr></table></figure><h1 id="在java中调用scala"><a href="#在java中调用scala" class="headerlink" title="在java中调用scala"></a>在java中调用scala</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.sql.types;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.annotation.Stable;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * To get/create specific data type, users should use singleton objects and factory methods</span></span><br><span class="line"><span class="comment"> * provided by this class.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> 1.3.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Stable</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataTypes</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Gets the NullType object.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> DataType NullType = NullType$.MODULE$;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;案例&quot;&gt;&lt;a href=&quot;#案例&quot; class=&quot;headerlink&quot; title=&quot;案例&quot;&gt;&lt;/a&gt;案例&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：主要参照spark源码 &lt;/p&gt;
&lt;h2 id=&quot;编写scala类及半生对象&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="java" scheme="https://tgluon.github.io/categories/java/"/>
    
    
      <category term="java" scheme="https://tgluon.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>sparksql DataSet算子实战</title>
    <link href="https://tgluon.github.io/2019/04/19/sparksql%20DataSet%E7%AE%97%E5%AD%90%E5%AE%9E%E6%88%98/"/>
    <id>https://tgluon.github.io/2019/04/19/sparksql DataSet算子实战/</id>
    <published>2019-04-19T09:22:37.970Z</published>
    <updated>2019-04-19T09:22:37.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本环境"><a href="#基本环境" class="headerlink" title="基本环境"></a>基本环境</h1><p><strong>scala版本</strong>：scala2.12.8<br><strong>jdk版本</strong>：1.8<br><strong>spark版本</strong>：2.4.1</p><h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><p>sku_sale_amount.csv<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10001</span>,<span class="number">36</span>B0,烟机   ,<span class="number">2015</span>,<span class="number">100</span></span><br><span class="line"><span class="number">10001</span>,<span class="number">27</span>A3,烟机   ,<span class="number">2016</span>,<span class="number">100</span></span><br><span class="line"><span class="number">10001</span>,<span class="number">27</span>A3,烟机   ,<span class="number">2017</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">36</span>B0,烟机   ,<span class="number">2015</span>,<span class="number">100</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">36</span>B0,烟机   ,<span class="number">2016</span>,<span class="number">100</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">36</span>B0,烟机   ,<span class="number">2017</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">58</span>B5,灶具   ,<span class="number">2014</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">58</span>B5,灶具   ,<span class="number">2015</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">58</span>B5,灶具   ,<span class="number">2016</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">58</span>B5,灶具   ,<span class="number">2017</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10003</span>,<span class="number">64</span>B8,洗碗机  ,<span class="number">2014</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10003</span>,<span class="number">727</span>T,智能消毒柜,<span class="number">2014</span>,<span class="number">200</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,净水器  ,<span class="number">2014</span>,<span class="number">150</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,净水器  ,<span class="number">2015</span>,<span class="number">50</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,净水器  ,<span class="number">2016</span>,<span class="number">50</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">45</span>A8,净水器  ,<span class="number">2017</span>,<span class="number">50</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,嵌入式蒸箱,<span class="number">2014</span>,<span class="number">150</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,嵌入式蒸箱,<span class="number">2015</span>,<span class="number">50</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">64</span>B8,嵌入式蒸箱,<span class="number">2016</span>,<span class="number">50</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">45</span>A8,嵌入式蒸箱,<span class="number">2017</span>,<span class="number">50</span></span><br></pre></td></tr></table></figure></p><p>sku_product.csv<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10001</span>,<span class="number">1000</span></span><br><span class="line"><span class="number">10001</span>,<span class="number">2000</span></span><br><span class="line"><span class="number">10001</span>,<span class="number">4000</span></span><br><span class="line"><span class="number">10001</span>,<span class="number">6000</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">8000</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">10000</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">9000</span></span><br><span class="line"><span class="number">10002</span>,<span class="number">6000</span></span><br><span class="line"><span class="number">10003</span>,<span class="number">5000</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">4000</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">3000</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">44000</span></span><br><span class="line"><span class="number">10004</span>,<span class="number">11000</span></span><br></pre></td></tr></table></figure></p><h1 id="准备基本环境"><a href="#准备基本环境" class="headerlink" title="准备基本环境"></a>准备基本环境</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSetSingleOperating</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> skuIncomeDF = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">","</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .load(<span class="string">"src/main/resources/data/sku_sale_amount.csv"</span>)</span><br><span class="line">      .toDF(<span class="string">"goods_id"</span>, <span class="string">"sku"</span>, <span class="string">"category"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> skuProductDF = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">","</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .load(<span class="string">"src/main/resources/data/sku_product.csv"</span>)</span><br><span class="line">      .toDF(<span class="string">"goods_id1"</span>, <span class="string">"volume"</span>)</span><br><span class="line"></span><br><span class="line">    skuIncomeDF.createTempView(<span class="string">"sku_sale_amount"</span>)</span><br><span class="line">    skuIncomeDF.createTempView(<span class="string">"sku_product"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="select-操作"><a href="#select-操作" class="headerlink" title="select 操作"></a>select 操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.select($<span class="string">"goods_id"</span>, $<span class="string">"sku"</span>, $<span class="string">"category"</span>, $<span class="string">"year"</span>.as(<span class="string">"date"</span>), $<span class="string">"amount"</span>)</span><br><span class="line"> skuIncomeDF.select(<span class="symbol">'goods_id</span>, <span class="symbol">'sku</span>, <span class="symbol">'category</span>, <span class="symbol">'year</span>.as(<span class="string">"date"</span>), <span class="symbol">'amount</span>)</span><br><span class="line"> skuIncomeDF.select(col(<span class="string">"goods_id"</span>), col(<span class="string">"sku"</span>), col(<span class="string">"category"</span>), col(<span class="string">"year"</span>).as(<span class="string">"date"</span>), <span class="symbol">'amount</span>)</span><br><span class="line"> skuIncomeDF.select(expr(<span class="string">"goods_id"</span>), expr(<span class="string">"sku"</span>), expr(<span class="string">"category"</span>), expr(<span class="string">"year as date"</span>), expr(<span class="string">"amount+1"</span>))</span><br><span class="line"> spark.sql(</span><br><span class="line">   <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">     |select sku,category,year,amount from sku_sale_amount</span></span><br><span class="line"><span class="string">   "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure><h1 id="selectExpr操作"><a href="#selectExpr操作" class="headerlink" title="selectExpr操作"></a>selectExpr操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.selectExpr(<span class="string">"goods_id"</span>, <span class="string">"sku"</span>, <span class="string">"category"</span>, <span class="string">"year as date"</span>, <span class="string">"amount+1 as amount"</span>)</span><br></pre></td></tr></table></figure><h1 id="filter-操作"><a href="#filter-操作" class="headerlink" title="filter 操作"></a>filter 操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.filter($<span class="string">"amount"</span> &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.filter(col(<span class="string">"amount"</span>) &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.filter(<span class="symbol">'amount</span> &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.filter(<span class="string">"amount &gt; 150"</span>)</span><br><span class="line">skuIncomeDF.filter(row =&gt; row.getInt(<span class="number">3</span>) &gt; <span class="number">150</span>)</span><br></pre></td></tr></table></figure><h1 id="where操作"><a href="#where操作" class="headerlink" title="where操作"></a>where操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.where($<span class="string">"amount"</span> &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.where(col(<span class="string">"amount"</span>) &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.where(<span class="symbol">'amount</span> &gt; <span class="number">150</span>)</span><br><span class="line">skuIncomeDF.where(<span class="string">"amount &gt; 150"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select sku,category,year,amount from sku_sale_amount where amount &gt; 150</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure><h1 id="union-操作"><a href="#union-操作" class="headerlink" title="union 操作"></a>union 操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.union(skuIncomeDF).groupBy(<span class="string">"sku"</span>).count()</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select sku,count(1) from (</span></span><br><span class="line"><span class="string">    |select sku,category,year,amount  from sku_sale_amount</span></span><br><span class="line"><span class="string">    | union all</span></span><br><span class="line"><span class="string">    |select sku,category,year,amount  from sku_sale_amount</span></span><br><span class="line"><span class="string">    |)a group by sku</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">skuIncomeDF.union(skuIncomeDF).distinct().groupBy(<span class="string">"sku"</span>).count()</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select sku,count(1) from (</span></span><br><span class="line"><span class="string">    |select sku,category,year,amount  from sku_sale_amount</span></span><br><span class="line"><span class="string">    | union</span></span><br><span class="line"><span class="string">    |select sku,category,year,amount  from sku_sale_amount</span></span><br><span class="line"><span class="string">    |)a group by sku</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure><h1 id="group-by操作"><a href="#group-by操作" class="headerlink" title="group by操作"></a>group by操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.groupBy(<span class="string">"sku"</span>).count()</span><br><span class="line">skuIncomeDF.groupBy($<span class="string">"sku"</span>).count()</span><br><span class="line">skuIncomeDF.groupBy(<span class="symbol">'sku</span>).count()</span><br><span class="line">skuIncomeDF.groupBy(col(<span class="string">"sku"</span>)).count()</span><br></pre></td></tr></table></figure><h1 id="join操作"><a href="#join操作" class="headerlink" title="join操作"></a>join操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.join(skuIncomeDF, <span class="string">"sku"</span>)</span><br><span class="line">skuIncomeDF.join(skuIncomeDF, <span class="type">Seq</span>(<span class="string">"sku"</span>, <span class="string">"category"</span>))</span><br><span class="line">skuIncomeDF.join(skuIncomeDF, <span class="type">Seq</span>(<span class="string">"sku"</span>), <span class="string">"inner"</span>)</span><br><span class="line"></span><br><span class="line">skuIncomeDF.join(skuProductDF, $<span class="string">"goods_id"</span> === $<span class="string">"goods_id1"</span>)</span><br><span class="line">skuIncomeDF.join(skuProductDF, col(<span class="string">"goods_id"</span>) === col(<span class="string">"goods_id1"</span>))</span><br><span class="line">skuIncomeDF.join(skuProductDF, col(<span class="string">"goods_id"</span>).equalTo(col(<span class="string">"goods_id1"</span>)))</span><br><span class="line">skuIncomeDF.joinWith(skuProductDF, skuIncomeDF(<span class="string">"goods_id"</span>) === skuProductDF(<span class="string">"goods_id1"</span>), <span class="string">"inner"</span>)</span><br></pre></td></tr></table></figure><h1 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h1><p>底层用的还是sort。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.orderBy(<span class="string">"sku"</span>, <span class="string">"category"</span>, <span class="string">"amount"</span>)</span><br><span class="line">skuIncomeDF.orderBy($<span class="string">"sku"</span>, $<span class="string">"category"</span>, $<span class="string">"amount"</span>.desc)</span><br><span class="line">skuIncomeDF.orderBy(col(<span class="string">"sku"</span>), col(<span class="string">"category"</span>), col(<span class="string">"amount"</span>).desc)</span><br></pre></td></tr></table></figure></p><h1 id="sort-操作"><a href="#sort-操作" class="headerlink" title="sort 操作"></a>sort 操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.sort(<span class="string">"sku"</span>, <span class="string">"category"</span>, <span class="string">"amount"</span>)</span><br><span class="line">skuIncomeDF.sort($<span class="string">"sku"</span>, $<span class="string">"category"</span>, $<span class="string">"amount"</span>.desc)</span><br><span class="line">skuIncomeDF.sort(col(<span class="string">"sku"</span>), col(<span class="string">"category"</span>), col(<span class="string">"amount"</span>).desc)</span><br></pre></td></tr></table></figure><h1 id="sortwithinpartition操作"><a href="#sortwithinpartition操作" class="headerlink" title="sortwithinpartition操作"></a>sortwithinpartition操作</h1><p>分区内部进行排序，局部排序。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.sortWithinPartitions(<span class="string">"sku"</span>, <span class="string">"category"</span>)</span><br><span class="line">skuIncomeDF.sortWithinPartitions($<span class="string">"sku"</span>, $<span class="string">"category"</span>, $<span class="string">"amount"</span>.desc)</span><br><span class="line">skuIncomeDF.sortWithinPartitions(col(<span class="string">"sku"</span>), col(<span class="string">"category"</span>).desc)</span><br></pre></td></tr></table></figure></p><h1 id="withColumn-操作"><a href="#withColumn-操作" class="headerlink" title="withColumn 操作"></a>withColumn 操作</h1><p>作用:假如列，存在就替换，不存在新增;对已有的列进行重命名。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.withColumn(<span class="string">"amount"</span>, $<span class="string">"amount"</span> + <span class="number">1</span>)</span><br><span class="line">skuIncomeDF.withColumn(<span class="string">"amount"</span>, <span class="symbol">'amount</span> + <span class="number">1</span>)</span><br><span class="line">skuIncomeDF.withColumnRenamed(<span class="string">"amount"</span>, <span class="string">"amount1"</span>)</span><br></pre></td></tr></table></figure></p><h1 id="foreach操作"><a href="#foreach操作" class="headerlink" title="foreach操作"></a>foreach操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.foreach(row =&gt; println(row.get(<span class="number">0</span>)))</span><br></pre></td></tr></table></figure><h1 id="foreachPartition操作"><a href="#foreachPartition操作" class="headerlink" title="foreachPartition操作"></a>foreachPartition操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.foreachPartition((it: <span class="type">Iterator</span>[<span class="type">Row</span>]) =&gt; &#123;</span><br><span class="line">  it.foreach(row =&gt; println(row.get(<span class="number">0</span>)))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h1 id="distinct操作"><a href="#distinct操作" class="headerlink" title="distinct操作"></a>distinct操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.distinct()</span><br></pre></td></tr></table></figure><h1 id="dropDuplicates操作"><a href="#dropDuplicates操作" class="headerlink" title="dropDuplicates操作"></a>dropDuplicates操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.dropDuplicates(<span class="string">"sku"</span>)</span><br></pre></td></tr></table></figure><h1 id="drop操作"><a href="#drop操作" class="headerlink" title="drop操作"></a>drop操作</h1><p>删除一列，或者多列。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.drop(<span class="string">"sku"</span>, <span class="string">"category"</span>)</span><br></pre></td></tr></table></figure></p><h1 id="cube操作"><a href="#cube操作" class="headerlink" title="cube操作"></a>cube操作</h1><p><strong>说明</strong>：相当于(category,year),(year),(category),() 分别分组然后对amount求sum<br>参考：<a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html" target="_blank" rel="noopener">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html</a>   </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.cube($<span class="string">"category"</span>, $<span class="string">"year"</span>.cast(<span class="string">"string"</span>).as(<span class="string">"year"</span>))</span><br><span class="line">  .agg(sum(<span class="string">"amount"</span>))</span><br><span class="line">  .sort(col(<span class="string">"category"</span>).desc_nulls_first, col(<span class="string">"year"</span>).desc_nulls_first)</span><br></pre></td></tr></table></figure><h1 id="rollup操作"><a href="#rollup操作" class="headerlink" title="rollup操作"></a>rollup操作</h1><p><strong>说明</strong>：等价于分别对(category,year),(year),()进行 groupby 对amount求sum。<br>参考：<a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html" target="_blank" rel="noopener">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-multi-dimensional-aggregation.html</a><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.rollup($<span class="string">"category"</span>, $<span class="string">"year"</span>.cast(<span class="string">"string"</span>).as(<span class="string">"year"</span>))</span><br><span class="line">  .agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>, grouping_id() as <span class="string">"gid"</span>)</span><br><span class="line">  .sort($<span class="string">"category"</span>.desc_nulls_last, $<span class="string">"year"</span>.asc_nulls_last)</span><br></pre></td></tr></table></figure></p><h1 id="pivot操作"><a href="#pivot操作" class="headerlink" title="pivot操作"></a>pivot操作</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skuIncomeDF.groupBy(<span class="string">"category"</span>)</span><br><span class="line">  .pivot(<span class="string">"year"</span>, <span class="type">Seq</span>(<span class="string">"2014"</span>, <span class="string">"2015"</span>, <span class="string">"2016"</span>, <span class="string">"2017"</span>))</span><br><span class="line">  .agg(sum(<span class="string">"amount"</span>))</span><br></pre></td></tr></table></figure><h1 id="转置操作"><a href="#转置操作" class="headerlink" title="转置操作"></a>转置操作</h1><p>文章来自：<a href="http://bailiwick.io/2017/10/21/transpose-data-with-spark/" target="_blank" rel="noopener">http://bailiwick.io/2017/10/21/transpose-data-with-spark/</a><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Import the requisite methods</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">DataFrame</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;array, col, explode, lit, struct&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a dataframe</span></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">)).toDF(<span class="string">"uid"</span>, <span class="string">"col1"</span>, <span class="string">"col2"</span>, <span class="string">"col3"</span>, <span class="string">"col4"</span>, <span class="string">"col5"</span>, <span class="string">"col6"</span>)</span><br><span class="line"></span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the transpose user defined function.</span></span><br><span class="line"><span class="comment">// Imputs:</span></span><br><span class="line"><span class="comment">//   transDF: The dataframe which will be transposed</span></span><br><span class="line"><span class="comment">//   transBy: The column that the dataframe will be transposed by</span></span><br><span class="line"><span class="comment">// Outputs:</span></span><br><span class="line"><span class="comment">//   Dataframe datatype consisting of three columns:</span></span><br><span class="line"><span class="comment">//     transBy</span></span><br><span class="line"><span class="comment">//     column_name</span></span><br><span class="line"><span class="comment">//     column_value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transposeUDF</span></span>(transDF: <span class="type">DataFrame</span>, transBy: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (cols, types) = transDF.dtypes.filter&#123; <span class="keyword">case</span> (c, _) =&gt; !transBy.contains(c)&#125;.unzip</span><br><span class="line">  require(types.distinct.size == <span class="number">1</span>)      </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kvs = explode(array(</span><br><span class="line">    cols.map(c =&gt; struct(lit(c).alias(<span class="string">"column_name"</span>), col(c).alias(<span class="string">"column_value"</span>))): _*</span><br><span class="line">  ))</span><br><span class="line">  <span class="keyword">val</span> byExprs = transBy.map(col(_))</span><br><span class="line"></span><br><span class="line">  transDF</span><br><span class="line">    .select(byExprs :+ kvs.alias(<span class="string">"_kvs"</span>): _*)</span><br><span class="line">    .select(byExprs ++ <span class="type">Seq</span>($<span class="string">"_kvs.column_name"</span>, $<span class="string">"_kvs.column_value"</span>): _*)</span><br><span class="line">&#125;</span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="type">Output</span>:</span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|uid|col1|col2|col3|col4|col5|col6|</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|<span class="number">1</span>  |<span class="number">1</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">4</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">9</span>   |<span class="number">8</span>   |</span><br><span class="line">|<span class="number">3</span>  |<span class="number">6</span>   |<span class="number">1</span>   |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">6</span>   |</span><br><span class="line">|<span class="number">4</span>  |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">6</span>   |<span class="number">9</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">5</span>  |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">3</span>   |</span><br><span class="line">|<span class="number">6</span>  |<span class="number">1</span>   |<span class="number">1</span>   |<span class="number">4</span>   |<span class="number">2</span>   |<span class="number">8</span>   |<span class="number">4</span>   |</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|uid|column_name|column_value|</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|<span class="number">1</span>  |col1       |<span class="number">1</span>           |</span><br><span class="line">|<span class="number">1</span>  |col2       |<span class="number">2</span>           |</span><br><span class="line">|<span class="number">1</span>  |col3       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">1</span>  |col4       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">1</span>  |col5       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">1</span>  |col6       |<span class="number">5</span>           |</span><br><span class="line">|<span class="number">2</span>  |col1       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">2</span>  |col2       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">2</span>  |col3       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">2</span>  |col4       |<span class="number">7</span>           |</span><br><span class="line">|<span class="number">2</span>  |col5       |<span class="number">9</span>           |</span><br><span class="line">|<span class="number">2</span>  |col6       |<span class="number">8</span>           |</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">only showing top <span class="number">12</span> rows</span><br></pre></td></tr></table></figure></p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p><a href="https://legacy.gitbook.com/book/jaceklaskowski/mastering-spark-sql/details" target="_blank" rel="noopener">https://legacy.gitbook.com/book/jaceklaskowski/mastering-spark-sql/details</a><br><a href="https://www.toutiao.com/i6631318012546793992/" target="_blank" rel="noopener">https://www.toutiao.com/i6631318012546793992/</a>   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本环境&quot;&gt;&lt;a href=&quot;#基本环境&quot; class=&quot;headerlink&quot; title=&quot;基本环境&quot;&gt;&lt;/a&gt;基本环境&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;scala版本&lt;/strong&gt;：scala2.12.8&lt;br&gt;&lt;strong&gt;jdk版本&lt;/strong&gt;
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>sparksql读取数据库问题</title>
    <link href="https://tgluon.github.io/2019/04/18/sparksql%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%BA%93%E9%97%AE%E9%A2%98/"/>
    <id>https://tgluon.github.io/2019/04/18/sparksql读取数据库问题/</id>
    <published>2019-04-18T08:10:12.056Z</published>
    <updated>2019-04-18T08:10:12.056Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark-jdbc方式读取数据库，每个excutor都会去拉数据？"><a href="#spark-jdbc方式读取数据库，每个excutor都会去拉数据？" class="headerlink" title="spark jdbc方式读取数据库，每个excutor都会去拉数据？"></a>spark jdbc方式读取数据库，每个excutor都会去拉数据？</h1><p><strong>说明</strong>：该问题来源spark技术分享朋友群<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.read(<span class="string">"jdbc"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, url)</span><br><span class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"pets"</span>)</span><br><span class="line">  .option(<span class="string">"user"</span>, user)</span><br><span class="line">  .option(<span class="string">"password"</span>, password)</span><br><span class="line">  .option(<span class="string">"numPartitions"</span>, <span class="number">10</span>)</span><br><span class="line">  .option(<span class="string">"partitionColumn"</span>, <span class="string">"owner_id"</span>)</span><br><span class="line">  .option(<span class="string">"lowerBound"</span>, <span class="number">1</span>)</span><br><span class="line">  .option(<span class="string">"upperBound"</span>, <span class="number">10000</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure></p><p>这种一般会转换为<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pets <span class="keyword">WHERE</span> owner_id &gt;= <span class="number">1</span> <span class="keyword">and</span> owner_id &lt; <span class="number">1000</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pets <span class="keyword">WHERE</span> owner_id &gt;= <span class="number">1000</span> <span class="keyword">and</span> owner_id &lt; <span class="number">2000</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pets <span class="keyword">WHERE</span> owner_id &gt;= <span class="number">2000</span> <span class="keyword">and</span> owner_id &lt; <span class="number">3000</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;spark-jdbc方式读取数据库，每个excutor都会去拉数据？&quot;&gt;&lt;a href=&quot;#spark-jdbc方式读取数据库，每个excutor都会去拉数据？&quot; class=&quot;headerlink&quot; title=&quot;spark jdbc方式读取数据库，每个excu
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>antlr4学习笔记</title>
    <link href="https://tgluon.github.io/2019/04/18/antlr4%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tgluon.github.io/2019/04/18/antlr4学习笔记/</id>
    <published>2019-04-18T07:57:05.336Z</published>
    <updated>2019-04-18T07:57:05.336Z</updated>
    
    <content type="html"><![CDATA[<h1 id="antlr介绍"><a href="#antlr介绍" class="headerlink" title="antlr介绍"></a>antlr介绍</h1><p><strong>注意</strong>： 文字来自《antlr4权威指南》<br>ANTLR 语言识别的一个工具 (ANother Tool for Language Recognition ) 是一种语言工具，它提供了一个框架，可以通过包含 Java, C++, 或 C# 动作（action）的语法描述来构造语言识别器，编译器和解释器。 计算机语言的解析已经变成了一种非常普遍的工作，在这方面的理论和工具经过近 40 年的发展已经相当成熟，使用 Antlr 等识别工具来识别，解析，构造编译器比手工编程更加容易，同时开发的程序也更易于维护。<br>语言识别的工具有很多种，比如大名鼎鼎的 Lex 和 YACC，Linux 中有他们的开源版本，分别是 Flex 和 Bison。在 Java 社区里，除了 Antlr 外，语言识别工具还有 JavaCC 和 SableCC 等。<br>和大多数语言识别工具一样，Antlr 使用上下文无关文法描述语言。最新的 Antlr 是一个基于 LL(*) 的语言识别器。在 Antlr 中通过解析用户自定义的上下文无关文法，自动生成词法分析器 (Lexer)、语法分析器 (Parser) 和树分析器 (Tree Parser)。   </p><h1 id="antlr-的应用"><a href="#antlr-的应用" class="headerlink" title="antlr 的应用"></a>antlr 的应用</h1><h2 id="编程语言处理"><a href="#编程语言处理" class="headerlink" title="编程语言处理"></a>编程语言处理</h2><p>识别和处理编程语言是 Antlr 的首要任务，编程语言的处理是一项繁重复杂的任务，为了简化处理，一般的编译技术都将语言处理工作分为前端和后端两个部分。其中前端包括词法分析、语法分析、语义分析、中间代码生成等若干步骤，后端包括目标代码生成和代码优化等步骤。</p><p>Antlr 致力于解决编译前端的所有工作。使用 Anltr 的语法可以定义目标语言的词法记号和语法规则，Antlr 自动生成目标语言的词法分析器和语法分析器；此外，如果在语法规则中指定抽象语法树的规则，在生成语法分析器的同时，Antlr 还能够生成抽象语法树；最终使用树分析器遍历抽象语法树，完成语义分析和中间代码生成。整个工作在 Anltr 强大的支持下，将变得非常轻松和愉快。 </p><h2 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h2><p>当需要文本处理时，首先想到的是正则表达式，使用 Anltr 的词法分析器生成器，可以很容易的完成正则表达式能够完成的所有工作；除此之外使用 Anltr 还可以完成一些正则表达式难以完成的工作，比如识别左括号和右括号的成对匹配等。</p><h1 id="基本环境配置"><a href="#基本环境配置" class="headerlink" title="基本环境配置"></a>基本环境配置</h1><h2 id="命令方式生成文件"><a href="#命令方式生成文件" class="headerlink" title="命令方式生成文件"></a>命令方式生成文件</h2><h3 id="下载antlr"><a href="#下载antlr" class="headerlink" title="下载antlr"></a>下载antlr</h3><p><a href="https://www.antlr.org/download/antlr-4.7.2-complete.jar" target="_blank" rel="noopener">https://www.antlr.org/download/antlr-4.7.2-complete.jar</a>   </p><h3 id="启动antlr"><a href="#启动antlr" class="headerlink" title="启动antlr"></a>启动antlr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alias antlr4='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.Tool'</span><br><span class="line"></span><br><span class="line">alias grun='java -Xmx500M -cp "/home/deeplearning/antlr/antlr-4.7.2-complete.jar:$CLASSPATH" org.antlr.v4.gui.TestRig'</span><br><span class="line"></span><br><span class="line">antlr4</span><br></pre></td></tr></table></figure><h3 id="测试案例"><a href="#测试案例" class="headerlink" title="测试案例"></a>测试案例</h3><h4 id="定义antlr-Hello-g4文件"><a href="#定义antlr-Hello-g4文件" class="headerlink" title="定义antlr Hello.g4文件"></a>定义antlr Hello.g4文件</h4><p><strong>注意：</strong> 逗号必须对齐<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">grammar Hello; //定义一个名为Hello的语法</span><br><span class="line">r  : &apos;hello&apos; ID ; // 匹配一个关键字hello和一个紧随其后的标识符</span><br><span class="line">ID : [a-z]+ ;    // 匹配小写字母组成的标识符</span><br><span class="line">WS : [ \t\r\n]+ -&gt; skip; // 忽略空格 Tab 换行</span><br></pre></td></tr></table></figure></p><h4 id="运行生成对应的文件和代码"><a href="#运行生成对应的文件和代码" class="headerlink" title="运行生成对应的文件和代码"></a>运行生成对应的文件和代码</h4><p>antlr4 Hello.g4<br>javac Hello*.java</p><h4 id="运行生成树图"><a href="#运行生成树图" class="headerlink" title="运行生成树图"></a>运行生成树图</h4><p>grun Hello r -gui</p><h2 id="IDEA中antlr4环境配置"><a href="#IDEA中antlr4环境配置" class="headerlink" title="IDEA中antlr4环境配置"></a>IDEA中antlr4环境配置</h2><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://www.baeldung.com/java-antlr" target="_blank" rel="noopener">https://www.baeldung.com/java-antlr</a></p><h3 id="在maven-项目src-main下创建一个目录"><a href="#在maven-项目src-main下创建一个目录" class="headerlink" title="在maven 项目src/main下创建一个目录"></a>在maven 项目src/main下创建一个目录</h3><p>mkdir -p  antlr4/cn/xh/parse</p><h3 id="编写MyParse-g4文件"><a href="#编写MyParse-g4文件" class="headerlink" title="编写MyParse.g4文件"></a>编写MyParse.g4文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">grammar MyParse;</span><br><span class="line"></span><br><span class="line">//parser</span><br><span class="line">prog:stat</span><br><span class="line">;</span><br><span class="line">stat:expr|NEWLINE</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">expr:multExpr((&apos;+&apos;|&apos;-&apos;)multExpr)*</span><br><span class="line">;</span><br><span class="line">multExpr:atom((&apos;*&apos;|&apos;/&apos;)atom)*</span><br><span class="line">;</span><br><span class="line">atom:&apos;(&apos;expr&apos;)&apos;</span><br><span class="line">    |INT</span><br><span class="line">    |ID</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">//lexer</span><br><span class="line">ID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;</span><br><span class="line">INT:&apos;0&apos;..&apos;9&apos;+;</span><br><span class="line">NEWLINE:&apos;\r&apos;?&apos;\n&apos;;</span><br><span class="line">WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;;</span><br></pre></td></tr></table></figure><h3 id="在pom-xml文件中添加如下内容"><a href="#在pom-xml文件中添加如下内容" class="headerlink" title="在pom.xml文件中添加如下内容"></a>在pom.xml文件中添加如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">antlr4.version</span>&gt;</span>4.7.2<span class="tag">&lt;/<span class="name">antlr4.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.antlr<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>antlr4-runtime<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;antlr4.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.antlr<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>antlr4-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>antlr4<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="输入maven命令打包"><a href="#输入maven命令打包" class="headerlink" title="输入maven命令打包"></a>输入maven命令打包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn package</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>在target/generated-sources/antlr4/cn/xh/parse 中产生对应的文件</p><h1 id="antlr基本语法详解"><a href="#antlr基本语法详解" class="headerlink" title="antlr基本语法详解"></a>antlr基本语法详解</h1><p>参考《antlr4权威指南》    </p><h2 id="定义文件头"><a href="#定义文件头" class="headerlink" title="定义文件头"></a>定义文件头</h2><p>使用Antlr 的语法规则来定义算术表达式文法，文件头部是 grammar 关键字，定义文法的名字，必须与文法文件文件的名字相同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grammar MyParse;</span><br></pre></td></tr></table></figure></p><h2 id="自定义一个算术表达式"><a href="#自定义一个算术表达式" class="headerlink" title="自定义一个算术表达式"></a>自定义一个算术表达式</h2><p><strong>表达式规则</strong>：   </p><blockquote><p>算法的优先级需要通过文法规则的嵌套定义来体现，加减法的优先级低于乘除法，表达式 expr 的定义由乘除法表达式 multExpr 和加减法算符 (‘+’|’-‘) 构成；同理，括号的优先级高于乘除法，乘除法表达式 multExpr 通过原子操作数 atom 和乘除法算符 (‘*’|’/’) 构成。   </p></blockquote><p>程序有一个语句构成，语句有表达式或者换行符构成。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prog: stat </span><br><span class="line">; </span><br><span class="line">stat: expr </span><br><span class="line">  |NEWLINE </span><br><span class="line">;</span><br></pre></td></tr></table></figure></p><p>在 Antlr 中语法定义和词法定义通过规则的第一个字符来区别， 规定语法定义符号的第一个字母小写，而词法定义符号的第一个字母大写。算术表达式中用到了 4 类记号 ( 在 Antlr 中被称为 Token)，分别是标识符 ID，表示一个变量；常量 INT，表示一个常数；换行符 NEWLINE 和空格 WS，空格字符在语言处理时将被跳过，skip() 是词法分析器类的一个方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;</span><br><span class="line">INT:&apos;0&apos;..&apos;9&apos;+;</span><br><span class="line">NEWLINE:&apos;\r&apos;?&apos;\n&apos;;</span><br><span class="line">WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="antlr文件编写"><a href="#antlr文件编写" class="headerlink" title="antlr文件编写"></a>antlr文件编写</h3><p>MyParse.g4文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">grammar MyParse;</span><br><span class="line"></span><br><span class="line">//parser</span><br><span class="line">prog:stat</span><br><span class="line">;</span><br><span class="line">stat:expr|NEWLINE</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">expr:multExpr((&apos;+&apos;|&apos;-&apos;)multExpr)*</span><br><span class="line">;</span><br><span class="line">multExpr:atom((&apos;*&apos;|&apos;/&apos;)atom)*</span><br><span class="line">;</span><br><span class="line">atom:&apos;(&apos;expr&apos;)&apos;</span><br><span class="line">    |INT</span><br><span class="line">    |ID</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">//lexer</span><br><span class="line">ID:(&apos;a&apos;..&apos;z&apos;|&apos;A&apos;..&apos;Z&apos;)+;</span><br><span class="line">INT:&apos;0&apos;..&apos;9&apos;+;</span><br><span class="line">NEWLINE:&apos;\r&apos;?&apos;\n&apos;;</span><br><span class="line">WS:(&apos; &apos;|&apos;\t&apos;|&apos;\n&apos;|&apos;\r&apos;)+&#123;skip();&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="使用java调用分析器"><a href="#使用java调用分析器" class="headerlink" title="使用java调用分析器"></a>使用java调用分析器</h3><p>参考spark源码中ParseDriver.scala文件中AbstractSqlParser类parse方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xh.antlr4;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.xh.parse.MyParseLexer;</span><br><span class="line"><span class="keyword">import</span> cn.xh.parse.MyParseParser;</span><br><span class="line"><span class="keyword">import</span> org.antlr.v4.runtime.CharStreams;</span><br><span class="line"><span class="keyword">import</span> org.antlr.v4.runtime.CommonTokenStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.parser.UpperCaseCharStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserMyParse</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String expr)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对每一个输入的字符串，构造一个 CodePointCharStream 流 in</span></span><br><span class="line">        UpperCaseCharStream in = <span class="keyword">new</span> UpperCaseCharStream(CharStreams.fromString(expr));</span><br><span class="line">        <span class="comment">//用 in 构造词法分析器 lexer，词法分析的作用是产生记号</span></span><br><span class="line">        MyParseLexer lexer = <span class="keyword">new</span> MyParseLexer(in);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//用词法分析器 lexer 构造一个记号流 tokens</span></span><br><span class="line">        CommonTokenStream tokens = <span class="keyword">new</span> CommonTokenStream(lexer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//再使用 tokens 构造语法分析器 parser,至此已经完成词法分析和语法分析的准备工作</span></span><br><span class="line">        MyParseParser parser = <span class="keyword">new</span> MyParseParser(tokens);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最终调用语法分析器的规则 prog，完成对表达式的验证</span></span><br><span class="line">        parser.prog();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] testStr = &#123;</span><br><span class="line">                <span class="string">"2"</span>,</span><br><span class="line">                <span class="string">"a+b+3"</span>,</span><br><span class="line">                <span class="string">"(a-b)+3"</span>,</span><br><span class="line">                <span class="string">"a+(b*3)"</span></span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String s : testStr) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Input expr:"</span> + s);</span><br><span class="line">            run(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><img src="https://github.com/tgluon/tgluon.github.io/blob/master/images/myimages/%E6%95%B0%E6%8D%AE%E5%9C%A8%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E4%B8%AD%E7%9A%84%E6%B5%81%E5%8A%A8%E8%BF%87%E7%A8%8B.png?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;antlr介绍&quot;&gt;&lt;a href=&quot;#antlr介绍&quot; class=&quot;headerlink&quot; title=&quot;antlr介绍&quot;&gt;&lt;/a&gt;antlr介绍&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;： 文字来自《antlr4权威指南》&lt;br&gt;ANTLR 语言识
      
    
    </summary>
    
      <category term="antlr" scheme="https://tgluon.github.io/categories/antlr/"/>
    
    
      <category term="antlr" scheme="https://tgluon.github.io/tags/antlr/"/>
    
  </entry>
  
  <entry>
    <title>spark sql窗口函数实战</title>
    <link href="https://tgluon.github.io/2019/04/15/spark%20sql%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98/"/>
    <id>https://tgluon.github.io/2019/04/15/spark sql窗口函数实战/</id>
    <published>2019-04-15T08:48:42.855Z</published>
    <updated>2019-04-15T08:48:42.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p>主要以一些官方文档为参考。<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a>   </p><p><a href="https://help.aliyun.com/document_detail/34994.html?spm=a2c4g.11174283.6.650.6f02590e0d209m#h2-url-1" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/34994.html?spm=a2c4g.11174283.6.650.6f02590e0d209m#h2-url-1</a></p><p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a> </p><p><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html" target="_blank" rel="noopener">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html</a>   </p><p><a href="http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html" target="_blank" rel="noopener">http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html</a>    </p><p><a href="http://cdn2.hubspot.net/hubfs/438089/notebooks/eBook/Introducing_Window_Functions_in_Spark_SQL_Notebook.html" target="_blank" rel="noopener">http://cdn2.hubspot.net/hubfs/438089/notebooks/eBook/Introducing_Window_Functions_in_Spark_SQL_Notebook.html</a>    </p><p><a href="http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/" target="_blank" rel="noopener">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p><p><a href="https://www.cnblogs.com/piaolingzxh/p/5538783.html" target="_blank" rel="noopener">https://www.cnblogs.com/piaolingzxh/p/5538783.html</a></p><h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> <span class="keyword">extends</span> <span class="title">BaseSparkSession</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">      .set(<span class="string">"spark.master"</span>, <span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-01"</span>, <span class="number">500</span>),</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-02"</span>, <span class="number">450</span>),</span><br><span class="line">      (<span class="string">"浙江"</span>, <span class="string">"2018-01-03"</span>, <span class="number">550</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-01"</span>, <span class="number">250</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-02"</span>, <span class="number">290</span>),</span><br><span class="line">      (<span class="string">"湖北"</span>, <span class="string">"2018-01-03"</span>, <span class="number">270</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="平均移动值"><a href="#平均移动值" class="headerlink" title="平均移动值"></a>平均移动值</h1><h2 id="DataFrame-API方式实现"><a href="#DataFrame-API方式实现" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)，每一个滑动的窗口总用有3行</span></span><br><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"> df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  avg($<span class="string">"user_cnt"</span>).over(movinAvgSpec).as(<span class="string">"moving_avg_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现"><a href="#sql方式实现" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h1 id="lag函数"><a href="#lag函数" class="headerlink" title="lag函数"></a>lag函数</h1><p>说明：取当前记录的前x条数据的指定列，如果没有返回null，有就返回真实值。</p><h2 id="DataFrame-API方式实现-1"><a href="#DataFrame-API方式实现-1" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"prevUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(lagwSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  lag($<span class="string">"user_cnt"</span>).over(movinAvgSpec).as(<span class="string">"lag_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-1"><a href="#sql方式实现-1" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lag(user_cnt,1) over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h1 id="lead函数"><a href="#lead函数" class="headerlink" title="lead函数"></a>lead函数</h1><p>说明：取当前记录的后x条数据的指定列，如果没有返回null，有就返回真实值。 </p><h2 id="DataFrame-API方式实现-2"><a href="#DataFrame-API方式实现-2" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> leadwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"lead_user_cnt"</span>, lead(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(leadwSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> leadwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">  df.select(</span><br><span class="line">    $<span class="string">"site"</span>,</span><br><span class="line">    $<span class="string">"date"</span>,</span><br><span class="line">    $<span class="string">"user_cnt"</span>,</span><br><span class="line">    lead($<span class="string">"user_cnt"</span>, <span class="number">1</span>).over(leadwSpec).as(<span class="string">"lead_user_cnt"</span>)</span><br><span class="line">  ).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-2"><a href="#sql方式实现-2" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lead(user_cnt,1) over(partition by site order by date asc ) as lead_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------------+</span><br><span class="line">|site|      date|user_cnt|lead_user_cnt|</span><br><span class="line">+----+----------+--------+-------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|          <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|          <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="literal">null</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|          <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|          <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="literal">null</span>|</span><br><span class="line">+----+----------+--------+-------------+</span><br></pre></td></tr></table></figure><h1 id="FIRST-VALUE函数"><a href="#FIRST-VALUE函数" class="headerlink" title="FIRST_VALUE函数"></a>FIRST_VALUE函数</h1><p>说明：该函数用于获取分组排序后最第一条记录的字段值。 </p><h2 id="DataFrame-API方式实现-3"><a href="#DataFrame-API方式实现-3" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> firstValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"first_value_user_cnt"</span>, first(<span class="string">"user_cnt"</span>).over(firstValuewSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> firstValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  first($<span class="string">"user_cnt"</span>).over(firstValuewSpec).as(<span class="string">"first_value_user_cnt"</span>)).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-3"><a href="#sql方式实现-3" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       first_value(user_cnt) over(partition by site order by date asc ) as first_value_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+--------------------+</span><br><span class="line">|site|      date|user_cnt|first_value_user_cnt|</span><br><span class="line">+----+----------+--------+--------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                 <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                 <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                 <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                 <span class="number">500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                 <span class="number">500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                 <span class="number">500</span>|</span><br><span class="line">+----+----------+--------+--------------------+</span><br></pre></td></tr></table></figure><h1 id="LAST-VALUE函数"><a href="#LAST-VALUE函数" class="headerlink" title="LAST_VALUE函数"></a>LAST_VALUE函数</h1><p>说明：该函数用于获取分组排序后最后一条记录的字段值。 </p><h2 id="DataFrame-API方式实现-4"><a href="#DataFrame-API方式实现-4" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"last_value_user_cnt"</span>, last(<span class="string">"user_cnt"</span>).over(lastValuewSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastValuewSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  last($<span class="string">"user_cnt"</span>).over(lastValuewSpec).as(<span class="string">"last_value_user_cnt"</span>)).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-4"><a href="#sql方式实现-4" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       last_value(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following ) as last_value_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------------------+</span><br><span class="line">|site|      date|user_cnt|last_value_user_cnt|</span><br><span class="line">+----+----------+--------+-------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                <span class="number">270</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                <span class="number">270</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+-------------------+</span><br></pre></td></tr></table></figure><h1 id="COUNT"><a href="#COUNT" class="headerlink" title="COUNT"></a>COUNT</h1><p>说明：该函数用于计算计数值。</p><h2 id="不指定order-by"><a href="#不指定order-by" class="headerlink" title="不指定order by"></a>不指定order by</h2><p>###<br><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"count"</span>, count(<span class="string">"user_cnt"</span>).over(counWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  count($<span class="string">"user_cnt"</span>).over(counWSpec).as(<span class="string">"count"</span>)).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-5"><a href="#sql方式实现-5" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">     <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">       | select  site,</span></span><br><span class="line"><span class="string">       |         date,</span></span><br><span class="line"><span class="string">       |         user_cnt,</span></span><br><span class="line"><span class="string">       |         count(user_cnt) over(partition by site) as count</span></span><br><span class="line"><span class="string">       |from     site_info</span></span><br><span class="line"><span class="string">     "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|count|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">3</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">3</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure><h2 id="指定order-by"><a href="#指定order-by" class="headerlink" title="指定order by"></a>指定order by</h2><p>指定order by时，返回当前窗口内从开始行到当前行的累计计数值。  </p><h2 id="DataFrame-API方式实现-5"><a href="#DataFrame-API方式实现-5" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span>.asc)</span><br><span class="line">df.withColumn(<span class="string">"count"</span>, count(<span class="string">"user_cnt"</span>).over(counWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> counWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  count($<span class="string">"user_cnt"</span>).over(counWSpec).as(<span class="string">"count"</span>))</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-6"><a href="#sql方式实现-6" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    | select  site,</span></span><br><span class="line"><span class="string">    |         date,</span></span><br><span class="line"><span class="string">    |         user_cnt,</span></span><br><span class="line"><span class="string">    |         count(user_cnt) over(partition by site order by date) as count</span></span><br><span class="line"><span class="string">    |from     site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-4"><a href="#结果-4" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|count|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure><h1 id="sum函数"><a href="#sum函数" class="headerlink" title="sum函数"></a>sum函数</h1><p>说明：该函数用于计算汇总值。</p><h2 id="不指定order-by-1"><a href="#不指定order-by-1" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-6"><a href="#DataFrame-API方式实现-6" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"sum_user_cnt"</span>, sum(<span class="string">"user_cnt"</span>).over(sumWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  sum($<span class="string">"user_cnt"</span>).over(sumWSpec).as(<span class="string">"sum_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-7"><a href="#sql方式实现-7" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       sum(user_cnt) over(partition by site ) as sum_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-5"><a href="#结果-5" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|sum_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">810</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|        <span class="number">1500</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h2 id="指定order-by-1"><a href="#指定order-by-1" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-7"><a href="#DataFrame-API方式实现-7" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"sum_user_cnt"</span>, sum(<span class="string">"user_cnt"</span>).over(sumWSpec))</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sumWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  sum($<span class="string">"user_cnt"</span>).over(sumWSpec).as(<span class="string">"sum_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-8"><a href="#sql方式实现-8" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       sum(user_cnt) over(partition by site order by date asc  rows between unbounded preceding and unbounded following ) as sum_user_cnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-6"><a href="#结果-6" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|sum_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">810</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">810</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|        <span class="number">1500</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|        <span class="number">1500</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h1 id="min函数"><a href="#min函数" class="headerlink" title="min函数"></a>min函数</h1><h2 id="不指定order-by-2"><a href="#不指定order-by-2" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-8"><a href="#DataFrame-API方式实现-8" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, min(<span class="string">"user_cnt"</span>).over(minWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  min($<span class="string">"user_cnt"</span>).over(minWSpec)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-9"><a href="#sql方式实现-9" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        min(user_cnt) over(partition by site) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-7"><a href="#结果-7" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|site|      date|user_cnt|min(user_cnt) <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> site unspecifiedframe$())|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                                                       <span class="number">450</span>|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure><h2 id="指定order-by-2"><a href="#指定order-by-2" class="headerlink" title="指定order by"></a>指定order by</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, min(<span class="string">"user_cnt"</span>).over(minWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  min($<span class="string">"user_cnt"</span>).over(minWSpec)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-10"><a href="#sql方式实现-10" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        min(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-8"><a href="#结果-8" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|site|      date|user_cnt|min(user_cnt) <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> site unspecifiedframe$())|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|                                                       <span class="number">250</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|                                                       <span class="number">450</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|                                                       <span class="number">450</span>|</span><br><span class="line">+----+----------+--------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure><h1 id="max函数"><a href="#max函数" class="headerlink" title="max函数"></a>max函数</h1><h2 id="不指定order-by-3"><a href="#不指定order-by-3" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-9"><a href="#DataFrame-API方式实现-9" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, max(<span class="string">"user_cnt"</span>).over(maxWSpec))</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  max($<span class="string">"user_cnt"</span>).over(maxWSpec).as(<span class="string">"max_user_cnt"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-11"><a href="#sql方式实现-11" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">   <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">     |select  site,</span></span><br><span class="line"><span class="string">     |        date,</span></span><br><span class="line"><span class="string">     |        user_cnt,</span></span><br><span class="line"><span class="string">     |        max(user_cnt) over(partition by site ) as min_user_cnt</span></span><br><span class="line"><span class="string">     |from    site_info</span></span><br><span class="line"><span class="string">   "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-9"><a href="#结果-9" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|min_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">290</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h2 id="指定order-by-3"><a href="#指定order-by-3" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-10"><a href="#DataFrame-API方式实现-10" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"min_user_cnt"</span>, max(<span class="string">"user_cnt"</span>).over(maxWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  max($<span class="string">"user_cnt"</span>).over(maxWSpec).as(<span class="string">"max_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-12"><a href="#sql方式实现-12" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        max(user_cnt) over(partition by site order by date asc rows between unbounded preceding and unbounded following) as min_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-10"><a href="#结果-10" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|min_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">290</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">290</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">550</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">550</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h1 id="avg函数"><a href="#avg函数" class="headerlink" title="avg函数"></a>avg函数</h1><p>说明：该函数用于计算平均值。</p><h2 id="不指定order-by-4"><a href="#不指定order-by-4" class="headerlink" title="不指定order by"></a>不指定order by</h2><h3 id="DataFrame-API方式实现-11"><a href="#DataFrame-API方式实现-11" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line"> df.withColumn(<span class="string">"avg_user_cnt"</span>, avg(<span class="string">"user_cnt"</span>).over(avgWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  avg(<span class="string">"user_cnt"</span>).over(avgWSpec).as(<span class="string">"avg_user_cnt"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-13"><a href="#sql方式实现-13" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        avg(user_cnt) over(partition by site ) as avg_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-11"><a href="#结果-11" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|avg_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|       <span class="number">270.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|       <span class="number">500.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h2 id="指定order-by-4"><a href="#指定order-by-4" class="headerlink" title="指定order by"></a>指定order by</h2><h3 id="DataFrame-API方式实现-12"><a href="#DataFrame-API方式实现-12" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.withColumn(<span class="string">"avg_user_cnt"</span>, avg(<span class="string">"user_cnt"</span>).over(avgWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  avg(<span class="string">"user_cnt"</span>).over(avgWSpec).as(<span class="string">"avg_user_cnt"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h3 id="sql方式实现-14"><a href="#sql方式实现-14" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        avg(user_cnt) over(partition by site order by date  asc rows between unbounded preceding and unbounded following ) as avg_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h3 id="结果-12"><a href="#结果-12" class="headerlink" title="结果"></a>结果</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+------------+</span><br><span class="line">|site|      date|user_cnt|avg_user_cnt|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|       <span class="number">270.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|       <span class="number">270.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|       <span class="number">500.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|       <span class="number">500.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h1 id="rank函数"><a href="#rank函数" class="headerlink" title="rank函数"></a>rank函数</h1><p>说明：该函数用于计算排名。</p><h2 id="DataFrame-API方式实现-13"><a href="#DataFrame-API方式实现-13" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'user_cnt</span>.desc)</span><br><span class="line">df.withColumn(<span class="string">"rank"</span>, rank().over(rankWSpec))</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'user_cnt</span>.desc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  rank().over(rankWSpec).as(<span class="string">"rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-15"><a href="#sql方式实现-15" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        rank() over(partition by site order by user_cnt desc) as rank_user_cnt</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-13"><a href="#结果-13" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="row-number-over-函数"><a href="#row-number-over-函数" class="headerlink" title="row_number over 函数"></a>row_number over 函数</h1><h2 id="DataFrame-API方式实现-14"><a href="#DataFrame-API方式实现-14" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowNUmberWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> desc, <span class="symbol">'user_cnt</span> desc)</span><br><span class="line">df.withColumn(<span class="string">"row_num"</span>, row_number().over(rowNUmberWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowNUmberWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> desc, <span class="symbol">'user_cnt</span> desc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  row_number().over(rowNUmberWSpec).as(<span class="string">"row_num"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-16"><a href="#sql方式实现-16" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        row_number() over(partition by site order by date desc , user_cnt desc ) as row_num</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure><h2 id="结果-14"><a href="#结果-14" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-------+</span><br><span class="line">|site|      date|user_cnt|row_num|</span><br><span class="line">+----+----------+--------+-------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|      <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|      <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|      <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|      <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|      <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|      <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+-------+</span><br></pre></td></tr></table></figure><h1 id="dense-rank函数"><a href="#dense-rank函数" class="headerlink" title="dense_rank函数"></a>dense_rank函数</h1><p>说明：该函数用于计算连续排名。</p><h2 id="DataFrame-API方式实现-15"><a href="#DataFrame-API方式实现-15" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> denseRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"dense_rank"</span>, dense_rank() over (denseRankWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> denseRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  dense_rank().over(denseRankWSpec).as(<span class="string">"dense_rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-17"><a href="#sql方式实现-17" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  spark.sql(</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">      |select site,</span></span><br><span class="line"><span class="string">      |       date,</span></span><br><span class="line"><span class="string">      |       user_cnt,</span></span><br><span class="line"><span class="string">      |       dense_rank() over(partition by site order by date asc ) as dense_rank</span></span><br><span class="line"><span class="string">      |from   site_info</span></span><br><span class="line"><span class="string">      |</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-15"><a href="#结果-15" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+----------+</span><br><span class="line">|site|      date|user_cnt|dense_rank|</span><br><span class="line">+----+----------+--------+----------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">2</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">3</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">3</span>|</span><br><span class="line">+----+----------+--------+----------+</span><br></pre></td></tr></table></figure><h1 id="percent-rank函数"><a href="#percent-rank函数" class="headerlink" title="percent_rank函数"></a>percent_rank函数</h1><p>说明：该函数用于计算一组数据中某行的相对排名。</p><h2 id="DataFrame-API方式实现-16"><a href="#DataFrame-API方式实现-16" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> percentRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"percent_rank"</span>, percent_rank() over (percentRankWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> percentRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  percent_rank().over(percentRankWSpec).as(<span class="string">"percent_rank"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-18"><a href="#sql方式实现-18" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  spark.sql(</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">      |select site,</span></span><br><span class="line"><span class="string">      |       date,</span></span><br><span class="line"><span class="string">      |       user_cnt,</span></span><br><span class="line"><span class="string">      |       percent_rank() over(partition by site order by date asc ) as percent_rank</span></span><br><span class="line"><span class="string">      |from   site_info</span></span><br><span class="line"><span class="string">      |</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-16"><a href="#结果-16" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|site|      date|user_cnt|percent_rank|</span><br><span class="line">+----+----------+--------+------------+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|         <span class="number">0.0</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|         <span class="number">0.5</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|         <span class="number">1.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|         <span class="number">0.0</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|         <span class="number">0.5</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|         <span class="number">1.0</span>|</span><br><span class="line">+----+----------+--------+------------+</span><br></pre></td></tr></table></figure><h1 id="ntile函数"><a href="#ntile函数" class="headerlink" title="ntile函数"></a>ntile函数</h1><p>说明：用于将分组数据按照顺序切分成n片，并返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。</p><h2 id="DataFrame-API方式实现-17"><a href="#DataFrame-API方式实现-17" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h2><p><strong>方式一：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ntileRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.withColumn(<span class="string">"ntile"</span>, ntile(<span class="number">2</span>).over(ntileRankWSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ntileRankWSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="symbol">'date</span> asc)</span><br><span class="line">df.select(</span><br><span class="line">  $<span class="string">"site"</span>,</span><br><span class="line">  $<span class="string">"date"</span>,</span><br><span class="line">  $<span class="string">"user_cnt"</span>,</span><br><span class="line">  ntile(<span class="number">2</span>).over(ntileRankWSpec).as(<span class="string">"ntile"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h2 id="sql方式实现-19"><a href="#sql方式实现-19" class="headerlink" title="sql方式实现"></a>sql方式实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select  site,</span></span><br><span class="line"><span class="string">    |        date,</span></span><br><span class="line"><span class="string">    |        user_cnt,</span></span><br><span class="line"><span class="string">    |        ntile(2) over(partition by site order by date) as ntile</span></span><br><span class="line"><span class="string">    |from    site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="结果-17"><a href="#结果-17" class="headerlink" title="结果"></a>结果</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+----------+--------+-----+</span><br><span class="line">|site|      date|user_cnt|ntile|</span><br><span class="line">+----+----------+--------+-----+</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">250</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">290</span>|    <span class="number">1</span>|</span><br><span class="line">|湖北|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">270</span>|    <span class="number">2</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>|     <span class="number">500</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-02</span>|     <span class="number">450</span>|    <span class="number">1</span>|</span><br><span class="line">|浙江|<span class="number">2018</span><span class="number">-01</span><span class="number">-03</span>|     <span class="number">550</span>|    <span class="number">2</span>|</span><br><span class="line">+----+----------+--------+-----+</span><br></pre></td></tr></table></figure><h1 id="致谢！"><a href="#致谢！" class="headerlink" title="致谢！"></a>致谢！</h1><p>本人能力有限，博客错误难免，有错往将错误发送到邮箱(<a href="mailto:t_spider@aliyun.com" target="_blank" rel="noopener">t_spider@aliyun.com</a>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;参考文章&quot;&gt;&lt;a href=&quot;#参考文章&quot; class=&quot;headerlink&quot; title=&quot;参考文章&quot;&gt;&lt;/a&gt;参考文章&lt;/h1&gt;&lt;p&gt;主要以一些官方文档为参考。&lt;br&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>sparksql实战案例</title>
    <link href="https://tgluon.github.io/2019/04/10/spark%20sql%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B/"/>
    <id>https://tgluon.github.io/2019/04/10/spark sql实战案例/</id>
    <published>2019-04-10T12:11:36.045Z</published>
    <updated>2019-04-10T12:11:36.045Z</updated>
    
    <content type="html"><![CDATA[<h1 id="累计统计"><a href="#累计统计" class="headerlink" title="累计统计"></a>累计统计</h1><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>access.csv<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">15</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">8</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">4</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">6</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">16</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">22</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">10</span></span><br><span class="line"><span class="type">A</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">50</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">5</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-01</span>,<span class="number">25</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-02</span>,<span class="number">5</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">23</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-03</span>,<span class="number">1</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">10</span></span><br><span class="line"><span class="type">B</span>,<span class="number">2015</span><span class="number">-04</span>,<span class="number">50</span></span><br></pre></td></tr></table></figure></p><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AccumulatorCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 读取数据</span></span><br><span class="line">    <span class="keyword">val</span> usersDF = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">","</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .load(<span class="string">"src/main/resources/access.csv"</span>)</span><br><span class="line">      .toDF(<span class="string">"name"</span>, <span class="string">"mounth"</span>, <span class="string">"amount"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="具体实现逻辑"><a href="#具体实现逻辑" class="headerlink" title="具体实现逻辑"></a>具体实现逻辑</h2><h3 id="DataFrame-API-方式"><a href="#DataFrame-API-方式" class="headerlink" title="DataFrame API 方式"></a>DataFrame API 方式</h3><p><strong>方式一:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// rowsBetween(Long.MinValue, 0):窗口的大小是按照排序从最小值到当前行</span></span><br><span class="line"><span class="keyword">val</span> accuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="number">0</span>)</span><br><span class="line">usersDF.withColumn(<span class="string">"acc_amount"</span>, sum(usersDF(<span class="string">"amount"</span>)).over(accuCntSpec)).show()</span><br></pre></td></tr></table></figure></p><p><strong>方式二</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(accuCntSpec).as(<span class="string">"acc_amount"</span>)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure></p><h3 id="sql方式"><a href="#sql方式" class="headerlink" title="sql方式"></a>sql方式</h3><p><strong>思路</strong>：根据DF算子意思，找到SqlBase.g4文件，看看是否有该类sql支持。<br>在SqlBase.g4文件中刚好找到如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">windowFrame</span><br><span class="line">    : frameType=RANGE start=frameBound</span><br><span class="line">    | frameType=ROWS start=frameBound</span><br><span class="line">    | frameType=RANGE BETWEEN start=frameBound AND end=frameBound</span><br><span class="line">    | frameType=ROWS BETWEEN start=frameBound AND end=frameBound</span><br><span class="line">    ;</span><br><span class="line"></span><br><span class="line">frameBound</span><br><span class="line">    : UNBOUNDED boundType=(PRECEDING | FOLLOWING)</span><br><span class="line">    | boundType=CURRENT ROW</span><br><span class="line">    | expression boundType=(PRECEDING | FOLLOWING)</span><br><span class="line">    ;</span><br></pre></td></tr></table></figure></p><p>在spark源码sql模块core项目org.apache.spark.sql.execution包中找到SQLWindowFunctionSuite类找到如下测试方法<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">"window function: multiple window expressions in a single expression"</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> nums = sparkContext.parallelize(<span class="number">1</span> to <span class="number">10</span>).map(x =&gt; (x, x % <span class="number">2</span>)).toDF(<span class="string">"x"</span>, <span class="string">"y"</span>)</span><br><span class="line">   nums.createOrReplaceTempView(<span class="string">"nums"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> expected =</span><br><span class="line">     <span class="type">Row</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">55</span>, <span class="number">1</span>, <span class="number">57</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">55</span>, <span class="number">2</span>, <span class="number">60</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">55</span>, <span class="number">4</span>, <span class="number">65</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">4</span>, <span class="number">10</span>, <span class="number">55</span>, <span class="number">6</span>, <span class="number">71</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">55</span>, <span class="number">9</span>, <span class="number">79</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">6</span>, <span class="number">21</span>, <span class="number">55</span>, <span class="number">12</span>, <span class="number">88</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">7</span>, <span class="number">28</span>, <span class="number">55</span>, <span class="number">16</span>, <span class="number">99</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">55</span>, <span class="number">20</span>, <span class="number">111</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">55</span>, <span class="number">25</span>, <span class="number">125</span>) ::</span><br><span class="line">       <span class="type">Row</span>(<span class="number">0</span>, <span class="number">10</span>, <span class="number">55</span>, <span class="number">55</span>, <span class="number">30</span>, <span class="number">140</span>) :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> actual = sql(</span><br><span class="line">     <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">       |SELECT</span></span><br><span class="line"><span class="string">       |  y,</span></span><br><span class="line"><span class="string">       |  x,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w1 AS running_sum,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w2 AS total_sum,</span></span><br><span class="line"><span class="string">       |  sum(x) OVER w3 AS running_sum_per_y,</span></span><br><span class="line"><span class="string">       |  ((sum(x) OVER w1) + (sum(x) OVER w2) + (sum(x) OVER w3)) as combined2</span></span><br><span class="line"><span class="string">       |FROM nums</span></span><br><span class="line"><span class="string">       |WINDOW w1 AS (ORDER BY x ROWS BETWEEN UnBOUNDED PRECEDiNG AND CuRRENT RoW),</span></span><br><span class="line"><span class="string">       |       w2 AS (ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOuNDED FoLLOWING),</span></span><br><span class="line"><span class="string">       |       w3 AS (PARTITION BY y ORDER BY x ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</span></span><br><span class="line"><span class="string">     "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">   checkAnswer(actual, expected)</span><br><span class="line"></span><br><span class="line">   spark.catalog.dropTempView(<span class="string">"nums"</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><p>下面就可以开心的照着案例写sql去了，真嗨皮！！！！<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">usersDF.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select name,</span></span><br><span class="line"><span class="string">    |       mounth,</span></span><br><span class="line"><span class="string">    |       amount,</span></span><br><span class="line"><span class="string">    |      sum(amount) over (partition by name order by mounth asc  rows between unbounded preceding and current row ) as acc_amount</span></span><br><span class="line"><span class="string">    |from   access</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure></p><h2 id="累加N天之前-假设N-3"><a href="#累加N天之前-假设N-3" class="headerlink" title="累加N天之前,假设N=3"></a>累加N天之前,假设N=3</h2><h3 id="DataFrame-API方式"><a href="#DataFrame-API方式" class="headerlink" title="DataFrame API方式"></a>DataFrame API方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> preThreeAccuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="number">-3</span>, <span class="number">0</span>)</span><br><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(preThreeAccuCntSpec).as(<span class="string">"acc_amount"</span>)).show()</span><br></pre></td></tr></table></figure><h3 id="sql方式-1"><a href="#sql方式-1" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select name,</span></span><br><span class="line"><span class="string">    |       mounth,</span></span><br><span class="line"><span class="string">    |       amount,</span></span><br><span class="line"><span class="string">    |      sum(amount) over (partition by name order by mounth asc rows between 3 preceding and current row) as acc_amount</span></span><br><span class="line"><span class="string">    |from   access</span></span><br><span class="line"><span class="string">    |</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="累加前3天，后3天"><a href="#累加前3天，后3天" class="headerlink" title="累加前3天，后3天"></a>累加前3天，后3天</h2><h3 id="API方式"><a href="#API方式" class="headerlink" title="API方式"></a>API方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> preThreeFiveAccuCntSpec = <span class="type">Window</span>.partitionBy(<span class="string">"name"</span>).orderBy(<span class="string">"mounth"</span>).rowsBetween(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">usersDF.select(</span><br><span class="line">  $<span class="string">"name"</span>,</span><br><span class="line">  $<span class="string">"mounth"</span>,</span><br><span class="line">  $<span class="string">"amount"</span>,</span><br><span class="line">  sum($<span class="string">"amount"</span>).over(preThreeFiveAccuCntSpec).as(<span class="string">"acc_amount"</span>))</span><br></pre></td></tr></table></figure><h3 id="sql方式-2"><a href="#sql方式-2" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">   <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">     |select name,</span></span><br><span class="line"><span class="string">     |       mounth,</span></span><br><span class="line"><span class="string">     |       amount,</span></span><br><span class="line"><span class="string">     |      sum(amount) over (partition by name order by mounth asc rows between 3 preceding and 3 following) as acc_amount</span></span><br><span class="line"><span class="string">     |from   access</span></span><br><span class="line"><span class="string">     |</span></span><br><span class="line"><span class="string">   "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h1 id="基本窗口函数案例"><a href="#基本窗口函数案例" class="headerlink" title="基本窗口函数案例"></a>基本窗口函数案例</h1><h2 id="准备环境-1"><a href="#准备环境-1" class="headerlink" title="准备环境"></a>准备环境</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFunctionTest</span> <span class="keyword">extends</span> <span class="title">BaseSparkSession</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"WindowFunctionTest"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-01"</span>, <span class="number">50</span>),</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-02"</span>, <span class="number">45</span>),</span><br><span class="line">      (<span class="string">"位置1"</span>, <span class="string">"2018-01-03"</span>, <span class="number">55</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-01"</span>, <span class="number">25</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-02"</span>, <span class="number">29</span>),</span><br><span class="line">      (<span class="string">"位置2"</span>, <span class="string">"2018-01-03"</span>, <span class="number">27</span>)</span><br><span class="line">    ).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="平均移动值"><a href="#平均移动值" class="headerlink" title="平均移动值"></a>平均移动值</h2><h3 id="DataFrame-API方式实现"><a href="#DataFrame-API方式实现" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窗口定义从 -1(前一行)到 1(后一行)，每一个滑动的窗口总用有3行</span></span><br><span class="line"> <span class="keyword">val</span> movinAvgSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    df.withColumn(<span class="string">"MovingAvg"</span>, avg(df(<span class="string">"user_cnt"</span>)).over(movinAvgSpec)).show()</span><br></pre></td></tr></table></figure><h3 id="sql方式实现"><a href="#sql方式实现" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       avg(user_cnt) over(partition by site order by date rows between 1 preceding and 1 following) as moving_avg</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="前一行数据"><a href="#前一行数据" class="headerlink" title="前一行数据"></a>前一行数据</h2><h3 id="DataFrame-API方式实现-1"><a href="#DataFrame-API方式实现-1" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lagwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line">df.withColumn(<span class="string">"prevUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>), <span class="number">1</span>).over(lagwSpec)).show()</span><br></pre></td></tr></table></figure><h3 id="sql方式实现-1"><a href="#sql方式实现-1" class="headerlink" title="sql方式实现"></a>sql方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"site_info"</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       lag(user_cnt,1) over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><h3 id="DataFrame-API方式实现-2"><a href="#DataFrame-API方式实现-2" class="headerlink" title="DataFrame API方式实现"></a>DataFrame API方式实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rankwSpec = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"> df.withColumn(<span class="string">"rank"</span>, rank().over(rankwSpec))</span><br></pre></td></tr></table></figure><h3 id="sql方式-3"><a href="#sql方式-3" class="headerlink" title="sql方式"></a>sql方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select site,</span></span><br><span class="line"><span class="string">    |       date,</span></span><br><span class="line"><span class="string">    |       user_cnt,</span></span><br><span class="line"><span class="string">    |       rank() over(partition by  site order by date asc ) as prevUserCnt</span></span><br><span class="line"><span class="string">    |from   site_info</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin).show()</span><br></pre></td></tr></table></figure><h1 id="分组topn和分组取最小"><a href="#分组topn和分组取最小" class="headerlink" title="分组topn和分组取最小"></a>分组topn和分组取最小</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupBy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> rows = spark.sparkContext.parallelize(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-02-22"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-02-27"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-03-13"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-03-20"</span>, <span class="number">5</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-03-27"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-03"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-10"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop1"</span>, <span class="string">"2018-04-17"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-28"</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-05"</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="string">"shop2"</span>, <span class="string">"2018-04-09"</span>, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">val</span> rowRDD = rows.map(t =&gt; <span class="type">Row</span>(t._1, t._2, t._3))</span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">Array</span>(</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"shop"</span>, <span class="type">StringType</span>),</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"ycd_date"</span>, <span class="type">StringType</span>),</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"ycd_num"</span>, <span class="type">IntegerType</span>)))</span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"ycd_order"</span>)</span><br><span class="line">    <span class="comment">// 分组topN</span></span><br><span class="line">    <span class="keyword">val</span> topN = spark.sql(<span class="string">"select * from (SELECT o.shop,o.ycd_date, row_number() over (PARTITION BY o.shop ORDER BY o.ycd_date DESC) rank FROM ycd_order as o) o1 where  rank &lt; 2"</span>)</span><br><span class="line">    <span class="comment">// 根据某一个字段分组,取某一个字段的最小值</span></span><br><span class="line">    <span class="keyword">val</span> groupMin = spark.sql(<span class="string">"select o.shop,min(o.ycd_num) as min_num from ycd_order as o group by o.shop order by min_num "</span>)</span><br><span class="line">    topN.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="优雅方式定义scheme"><a href="#优雅方式定义scheme" class="headerlink" title="优雅方式定义scheme"></a>优雅方式定义scheme</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getScheme</span></span>(): <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> schemaString = <span class="string">"store_id:String,order_date:String,sale_amount: Int"</span></span><br><span class="line">    <span class="keyword">val</span> fields = schemaString.split(<span class="string">","</span>)</span><br><span class="line">      .map(fieldName =&gt;</span><br><span class="line">        <span class="type">StructField</span>(fieldName.split(<span class="string">":"</span>)(<span class="number">0</span>).trim,</span><br><span class="line">          fieldName.split(<span class="string">":"</span>)(<span class="number">1</span>).trim <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"String"</span> =&gt; <span class="type">StringType</span></span><br><span class="line">            <span class="keyword">case</span> <span class="string">"Int"</span> =&gt; <span class="type">IntegerType</span></span><br><span class="line">          &#125;, <span class="literal">true</span>))</span><br><span class="line">    <span class="type">StructType</span>(fields)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h1 id="保存小数点后n位"><a href="#保存小数点后n位" class="headerlink" title="保存小数点后n位"></a>保存小数点后n位</h1><p>10表示总的位数，2表示保留几位小数，10要&gt;=实际的位数，否则为NULL<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"select cast(sale_amount as decimal(10, 2))from ycd"</span>).show()</span><br></pre></td></tr></table></figure></p><h1 id="重命名行"><a href="#重命名行" class="headerlink" title="重命名行"></a>重命名行</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">SQLContext</span>, <span class="type">Row</span>, <span class="type">DataFrame</span>, <span class="type">Column</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">val</span> firstDF = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">)).toDF()</span><br><span class="line"><span class="keyword">val</span> colNames = <span class="type">Seq</span>(<span class="string">"uid"</span>, <span class="string">"col1"</span>, <span class="string">"col2"</span>, <span class="string">"col3"</span>, <span class="string">"col4"</span>, <span class="string">"col5"</span>, <span class="string">"col6"</span>)</span><br><span class="line"><span class="keyword">val</span> secondDF = firstDF.toDF(colNames: _*)</span><br></pre></td></tr></table></figure><h1 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">DataFrame</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;array, col, explode, lit, struct&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">)).toDF(<span class="string">"uid"</span>, <span class="string">"col1"</span>, <span class="string">"col2"</span>, <span class="string">"col3"</span>, <span class="string">"col4"</span>, <span class="string">"col5"</span>, <span class="string">"col6"</span>)</span><br><span class="line"></span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the transpose user defined function.</span></span><br><span class="line"><span class="comment">// Imputs:</span></span><br><span class="line"><span class="comment">//   transDF: The dataframe which will be transposed</span></span><br><span class="line"><span class="comment">//   transBy: The column that the dataframe will be transposed by</span></span><br><span class="line"><span class="comment">// Outputs:</span></span><br><span class="line"><span class="comment">//   Dataframe datatype consisting of three columns:</span></span><br><span class="line"><span class="comment">//     transBy</span></span><br><span class="line"><span class="comment">//     column_name</span></span><br><span class="line"><span class="comment">//     column_value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transposeUDF</span></span>(transDF: <span class="type">DataFrame</span>, transBy: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (cols, types) = transDF.dtypes.filter&#123; <span class="keyword">case</span> (c, _) =&gt; !transBy.contains(c)&#125;.unzip</span><br><span class="line">  require(types.distinct.size == <span class="number">1</span>)      </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kvs = explode(array(</span><br><span class="line">    cols.map(c =&gt; struct(lit(c).alias(<span class="string">"column_name"</span>), col(c).alias(<span class="string">"column_value"</span>))): _*</span><br><span class="line">  ))</span><br><span class="line">  <span class="keyword">val</span> byExprs = transBy.map(col(_))</span><br><span class="line"></span><br><span class="line">  transDF</span><br><span class="line">    .select(byExprs :+ kvs.alias(<span class="string">"_kvs"</span>): _*)</span><br><span class="line">    .select(byExprs ++ <span class="type">Seq</span>($<span class="string">"_kvs.column_name"</span>, $<span class="string">"_kvs.column_value"</span>): _*)</span><br><span class="line">&#125;</span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="type">Output</span>:</span><br><span class="line">df.show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|uid|col1|col2|col3|col4|col5|col6|</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line">|<span class="number">1</span>  |<span class="number">1</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">4</span>   |<span class="number">3</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">9</span>   |<span class="number">8</span>   |</span><br><span class="line">|<span class="number">3</span>  |<span class="number">6</span>   |<span class="number">1</span>   |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">3</span>   |<span class="number">6</span>   |</span><br><span class="line">|<span class="number">4</span>  |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">6</span>   |<span class="number">9</span>   |<span class="number">4</span>   |<span class="number">5</span>   |</span><br><span class="line">|<span class="number">5</span>  |<span class="number">9</span>   |<span class="number">2</span>   |<span class="number">7</span>   |<span class="number">8</span>   |<span class="number">7</span>   |<span class="number">3</span>   |</span><br><span class="line">|<span class="number">6</span>  |<span class="number">1</span>   |<span class="number">1</span>   |<span class="number">4</span>   |<span class="number">2</span>   |<span class="number">8</span>   |<span class="number">4</span>   |</span><br><span class="line">+---+----+----+----+----+----+----+</span><br><span class="line"></span><br><span class="line">transposeUDF(df, <span class="type">Seq</span>(<span class="string">"uid"</span>)).show(<span class="number">12</span>,<span class="literal">false</span>)</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|uid|column_name|column_value|</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">|<span class="number">1</span>  |col1       |<span class="number">1</span>           |</span><br><span class="line">|<span class="number">1</span>  |col2       |<span class="number">2</span>           |</span><br><span class="line">|<span class="number">1</span>  |col3       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">1</span>  |col4       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">1</span>  |col5       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">1</span>  |col6       |<span class="number">5</span>           |</span><br><span class="line">|<span class="number">2</span>  |col1       |<span class="number">4</span>           |</span><br><span class="line">|<span class="number">2</span>  |col2       |<span class="number">3</span>           |</span><br><span class="line">|<span class="number">2</span>  |col3       |<span class="number">8</span>           |</span><br><span class="line">|<span class="number">2</span>  |col4       |<span class="number">7</span>           |</span><br><span class="line">|<span class="number">2</span>  |col5       |<span class="number">9</span>           |</span><br><span class="line">|<span class="number">2</span>  |col6       |<span class="number">8</span>           |</span><br><span class="line">+---+-----------+------------+</span><br><span class="line">only showing top <span class="number">12</span> rows</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;累计统计&quot;&gt;&lt;a href=&quot;#累计统计&quot; class=&quot;headerlink&quot; title=&quot;累计统计&quot;&gt;&lt;/a&gt;累计统计&lt;/h1&gt;&lt;h2 id=&quot;准备数据&quot;&gt;&lt;a href=&quot;#准备数据&quot; class=&quot;headerlink&quot; title=&quot;准备数据&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>java concurrent包类详解</title>
    <link href="https://tgluon.github.io/2019/02/13/java%20concurrent%E5%8C%85%E7%B1%BB%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/02/13/java concurrent包类详解/</id>
    <published>2019-02-13T15:34:37.000Z</published>
    <updated>2019-02-13T09:41:05.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h1><p>原文：<a href="https://zhuanlan.zhihu.com/p/27314456" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27314456</a><br>Semaphore(信号量)是java.util.concurrent下的一个工具类.<strong>用来控制可同时访问特定资源的线程数</strong>.内部是通过维护父类(AQS)的 int state值实现.<br>Semaphore中有一个”许可”的概念:</p><p>访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。<br>访问资源后，使用release()释放许可。</p><p>这个许可在构造时传入,赋给state值,它等同于state.</p><p><strong>Semaphore应用场景</strong><br>系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。<br>Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。<br><strong>使用案例：</strong><br>spark LiveListenerBus类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> val eventLock = <span class="keyword">new</span> Semaphore(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> val listenerThread = <span class="keyword">new</span> Thread(name) &#123;</span><br><span class="line">  setDaemon(<span class="keyword">true</span>)</span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryOrStopSparkContext(sparkContext) &#123;</span><br><span class="line">    LiveListenerBus.withinListenerThread.withValue(<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        eventLock.acquire()</span><br><span class="line">        self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">          processingEvent = <span class="keyword">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 消费消息</span></span><br><span class="line">          val event = eventQueue.poll</span><br><span class="line">          <span class="keyword">if</span> (event == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Get out of the while loop and shutdown the daemon thread</span></span><br><span class="line">            <span class="keyword">if</span> (!stopped.get) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Polling `null` from eventQueue means"</span> +</span><br><span class="line">                <span class="string">" the listener bus has been stopped. So `stopped` must be true"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">          &#125;</span><br><span class="line">          postToAll(event)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">            processingEvent = <span class="keyword">false</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Semaphore属于一种较常见的限流手段，Google Guava封装了一层。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//JDK API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> Semaphore semaphore = <span class="keyword">new</span> Semaphore(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        semaphore.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">        semaphore.release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Google Guava API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> RateLimiter rateLimiter = RateLimiter.create(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        rateLimiter.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Semaphore&quot;&gt;&lt;a href=&quot;#Semaphore&quot; class=&quot;headerlink&quot; title=&quot;Semaphore&quot;&gt;&lt;/a&gt;Semaphore&lt;/h1&gt;&lt;p&gt;原文：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/2
      
    
    </summary>
    
      <category term="java" scheme="https://tgluon.github.io/categories/java/"/>
    
    
      <category term="java" scheme="https://tgluon.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>DynamicVariable详解</title>
    <link href="https://tgluon.github.io/2019/02/13/DynamicVariable%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/02/13/DynamicVariable详解/</id>
    <published>2019-02-13T15:34:37.000Z</published>
    <updated>2019-04-10T10:31:26.649Z</updated>
    
    <content type="html"><![CDATA[<p>内容来源于：<a href="https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable" target="_blank" rel="noopener">https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable</a>    </p><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> _response   = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">HttpServletResponse</span>](<span class="literal">null</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> _request    = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">HttpServletRequest</span>](<span class="literal">null</span>)</span><br></pre></td></tr></table></figure><p>DynamicVariable是贷款和动态范围模式的实现。 DynamicVariable的使用情况与Java中的ThreadLocal非常相似(事实上，DynamicVariable在后台使用InheritableThreadLocal) – 当需要在一个封闭的作用域内进行计算时，每个线程都有自己的副本的变量值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dynamicVariable.withValue(value)&#123; valueInContext =&gt;</span><br><span class="line">  <span class="comment">// value used in the context</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>由于DynamicVariable使用可继承的ThreadLocal，变量的值被传递给上下文中生成的线程：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dynamicVariable.withValue(value)&#123; valueInContext =&gt;</span><br><span class="line">  spawn&#123;</span><br><span class="line">    <span class="comment">// value is passed to the spawned thread</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>DynamicVariable(和ThreadLocal)在Scalatra中使用的原因与在许多其他框架(Lift，Spring，Struts等)中使用的相同 – 它是一种非侵入性的方式来存储和传递上下文(线程)特定的信息。</p><p>使HttpServletResponse和HttpServletRequest动态变量(并因此，绑定到处理请求的特定线程)只是最简单的方法来获取它们在代码中的任何地方(不通过方法参数或任何其他显式)。</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dyn = <span class="keyword">new</span> <span class="type">DynamicVariable</span>[<span class="type">String</span>](<span class="string">"withoutValue"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print=println</span></span>(dyn.value)</span><br><span class="line">print</span><br><span class="line">dyn.withValue(<span class="string">"withValue"</span>) &#123;</span><br><span class="line">  print</span><br><span class="line">&#125;</span><br><span class="line">print</span><br></pre></td></tr></table></figure><p><strong><em>结果</em></strong><br>withoutValue<br>withValue<br>withoutValue   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;内容来源于：&lt;a href=&quot;https://stackoverflow.com/questions/5116352/when-we-should-use-scala-util-dynamicvariable&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala枚举</title>
    <link href="https://tgluon.github.io/2019/01/09/scala%E6%9E%9A%E4%B8%BE/"/>
    <id>https://tgluon.github.io/2019/01/09/scala枚举/</id>
    <published>2019-01-09T15:34:37.000Z</published>
    <updated>2019-04-10T10:31:26.664Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义枚举对象"><a href="#定义枚举对象" class="headerlink" title="定义枚举对象"></a>定义枚举对象</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 定义一个枚举类 */</span></span><br><span class="line"><span class="keyword">private</span>[deploy] <span class="class"><span class="keyword">object</span> <span class="title">SparkSubmitAction</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 声明枚举对外暴露的变量类型</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">SparkSubmitAction</span> </span>= <span class="type">Value</span></span><br><span class="line">  <span class="comment">// 枚举的定义</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">SUBMIT</span>, <span class="type">KILL</span>, <span class="type">REQUEST_STATUS</span> = <span class="type">Value</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="使用枚举"><a href="#使用枚举" class="headerlink" title="使用枚举"></a>使用枚举</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 定义一个枚举类 */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSubmitAction</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 声明枚举对外暴露的变量类型</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">SparkSubmitAction</span> </span>= <span class="type">Value</span></span><br><span class="line">  <span class="comment">// 枚举的定义</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">SUBMIT</span>, <span class="type">KILL</span>, <span class="type">REQUEST_STATUS</span> = <span class="type">Value</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAction</span></span>(action: <span class="type">SparkSubmitAction</span>)&#123;</span><br><span class="line">  action <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SUBMIT</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">KILL</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">REQUEST_STATUS</span> =&gt; println (<span class="string">"action is "</span> + action)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println (<span class="string">"Unknown type"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EnumerationTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> action = <span class="type">SparkSubmitAction</span>.apply(<span class="number">1</span>)</span><br><span class="line">    <span class="type">SparkSubmitAction</span>.getAction(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> action1 = <span class="type">SparkSubmitAction</span>.withName(<span class="string">"quit"</span>)</span><br><span class="line">    <span class="type">SparkSubmitAction</span>.getAction(action1)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;定义枚举对象&quot;&gt;&lt;a href=&quot;#定义枚举对象&quot; class=&quot;headerlink&quot; title=&quot;定义枚举对象&quot;&gt;&lt;/a&gt;定义枚举对象&lt;/h1&gt;&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala学习笔记</title>
    <link href="https://tgluon.github.io/2019/01/09/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tgluon.github.io/2019/01/09/scala学习笔记/</id>
    <published>2019-01-09T15:34:37.000Z</published>
    <updated>2019-04-10T10:31:26.654Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高级函数"><a href="#高级函数" class="headerlink" title="高级函数"></a>高级函数</h1><p>高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。    </p><h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x: <span class="type">Int</span>) =&gt; x * <span class="number">3</span></span><br></pre></td></tr></table></figure><h2 id="带函数参数的函数"><a href="#带函数参数的函数" class="headerlink" title="带函数参数的函数"></a>带函数参数的函数</h2><p>Scala集合类（collections）的高阶函数map。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">B</span>, <span class="type">That</span>](f: <span class="type">A</span> =&gt; <span class="type">B</span>)(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span> = &#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">builder</span> </span>= &#123;</span><br><span class="line">     <span class="keyword">val</span> b = bf(repr)</span><br><span class="line">     b.sizeHint(<span class="keyword">this</span>)</span><br><span class="line">     b</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">val</span> b = builder</span><br><span class="line">   <span class="keyword">for</span> (x &lt;- <span class="keyword">this</span>) b += f(x)</span><br><span class="line">   b.result</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">doubleSalary</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">     x * <span class="number">2</span></span><br><span class="line">   &#125;</span><br><span class="line">   seq.map(doubleSalary).foreach(x =&gt; println(x))</span><br><span class="line">   seq.map(x =&gt; x * <span class="number">2</span>)</span><br><span class="line">   seq.map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line"><span class="keyword">val</span> first = sum(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> second = first(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h1 id="嵌套方法"><a href="#嵌套方法" class="headerlink" title="嵌套方法"></a>嵌套方法</h1><p>在Scala中可以嵌套定义方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fact</span></span>(x: <span class="type">Int</span>, accumulator: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (x &lt;= <span class="number">1</span>) accumulator</span><br><span class="line">      <span class="keyword">else</span> fact(x - <span class="number">1</span>, x * accumulator)</span><br><span class="line">    &#125;  </span><br><span class="line">    fact(x, <span class="number">1</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> println(<span class="string">"Factorial of 2: "</span> + factorial(<span class="number">2</span>))</span><br><span class="line"> println(<span class="string">"Factorial of 3: "</span> + factorial(<span class="number">3</span>))</span><br></pre></td></tr></table></figure></p><h1 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h1><p>柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。<br><strong>接受两个参数的过程：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x:<span class="type">Int</span> , y: <span class="type">Int</span> )= x * y</span><br></pre></td></tr></table></figure></p><p><strong>接受一个参数的过程</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>scala支持简写成如下的柯里化：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line"></span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>在Scala集合中定义的特质TraversableOnce[+A]。<br>Traversable： 能横过的；能越过的；可否定的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> result = z</span><br><span class="line">    <span class="keyword">this</span> foreach (x =&gt; result = op(result, x))</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br><span class="line">```   </span><br><span class="line">foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：</span><br><span class="line"></span><br><span class="line">从初值<span class="number">0</span>开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。  </span><br><span class="line">```scala</span><br><span class="line"><span class="keyword">val</span> numbers = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> res = numbers.foldLeft(<span class="number">0</span>)((m, n) =&gt; m + n)</span><br><span class="line"><span class="keyword">val</span> res2 = numbers.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">println(res) <span class="comment">// 55</span></span><br><span class="line">pringln(res2)</span><br></pre></td></tr></table></figure></p><p>多参数列表有更复杂的调用语法，因此应该谨慎使用。<br><strong>建议的使用场景包括:</strong>   </p><ol><li><p>单一的函数参数。<br>在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numbers.foldLeft(<span class="number">0</span>, &#123;(m: <span class="type">Int</span>, n: <span class="type">Int</span>) =&gt; m + n&#125;)</span><br></pre></td></tr></table></figure></li><li><p>隐式（IMPLICIT）参数。     </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(arg: <span class="type">Int</span>)(<span class="keyword">implicit</span> ec: <span class="type">ExecutionContext</span>) = ???</span><br></pre></td></tr></table></figure></li></ol><h1 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h1><p>match对应java里的Switch,不过它写在选择器表达式之后。 </p><p>选择器 match {备选项}<br>取代了：<br>switch(选择器){备选项}  </p><h2 id="match与switch的比较"><a href="#match与switch的比较" class="headerlink" title="match与switch的比较"></a>match与switch的比较</h2><p>匹配表达式可以被看做java风格switch的泛化。<br>match的不同：<br>1、match是scala表达式，也就是说，它始终以值作为结果;<br>2、 scala的备选项表达式永远不会”掉到”下一个case;<br>3、 如果没有模式匹配，MatchErro异常会被抛出。</p><h1 id="提取器对象"><a href="#提取器对象" class="headerlink" title="提取器对象"></a>提取器对象</h1><p>提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"> <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// 调用工厂构造方法，构造出对象实例</span></span><br><span class="line">   <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"Spark"</span>, <span class="number">6</span>)</span><br><span class="line">   <span class="comment">// 这种写法居然可以正常编译</span></span><br><span class="line">   <span class="keyword">val</span> <span class="type">Person</span>(name, age) = person</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 使用了提取器，</span></span><br><span class="line"><span class="comment">    * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(name + <span class="string">" : "</span> + age) <span class="comment">//正常输出: Spark 6</span></span><br><span class="line">   </span><br><span class="line">   person <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">     <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><p><strong>自定义unapply方法</strong><br>主要用于模式匹配中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val salary: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>, salary: <span class="type">Int</span>):<span class="type">Person</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Person</span>(name, salary)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(money: <span class="type">Person</span>): <span class="type">Option</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    <span class="keyword">if</span>(money == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Some</span>(money.name, money.salary)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"spark"</span>, <span class="number">800</span>);</span><br><span class="line">     person <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="for-表达式"><a href="#for-表达式" class="headerlink" title="for 表达式"></a>for 表达式</h1><p>Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">userBase</span> </span>= <span class="type">List</span>(<span class="type">User</span>(<span class="string">"Travis"</span>, <span class="number">28</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Kelly"</span>, <span class="number">33</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Jennifer"</span>, <span class="number">44</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Dennis"</span>, <span class="number">23</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> twentySomethings = <span class="keyword">for</span> (user &lt;- userBase <span class="keyword">if</span> (user.age &gt;=<span class="number">20</span> &amp;&amp; user.age &lt; <span class="number">30</span>))</span><br><span class="line">  <span class="keyword">yield</span> user.name  </span><br><span class="line"></span><br><span class="line">twentySomethings.foreach(name =&gt; println(name))</span><br></pre></td></tr></table></figure></p><p>这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user <- userbase="" 是生成器，if="" (user.age="">=20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。</-></p><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><p>泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。<br>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。  </p><h2 id="定义一个泛型类"><a href="#定义一个泛型类" class="headerlink" title="定义一个泛型类"></a>定义一个泛型类</h2><p>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> elements: <span class="type">List</span>[<span class="type">A</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">push</span></span>(x: <span class="type">A</span>) &#123; elements = x :: elements &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">peek</span></span>: <span class="type">A</span> = elements.head</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pop</span></span>(): <span class="type">A</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> currentTop = peek</span><br><span class="line">    elements = elements.tail</span><br><span class="line">    currentTop</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。   </p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>要使用一个泛型类，将一个具体类型放到方括号中来代替 A。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stack = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">Int</span>]</span><br><span class="line">stack.push(<span class="number">1</span>)</span><br><span class="line">stack.push(<span class="number">2</span>)</span><br><span class="line">println(stack.pop)  <span class="comment">// prints 2</span></span><br><span class="line">println(stack.pop)  <span class="comment">// prints 1</span></span><br></pre></td></tr></table></figure></p><h1 id="型变"><a href="#型变" class="headerlink" title="型变"></a>型变</h1><p>型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>[+<span class="type">A</span>] <span class="title">//</span> <span class="title">一个协变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Bar</span>[-<span class="type">A</span>] <span class="title">//</span> <span class="title">一个逆变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Baz</span>[<span class="type">A</span>]  <span class="title">//</span> <span class="title">一个不变类</span></span></span><br></pre></td></tr></table></figure></p><h2 id="协变"><a href="#协变" class="headerlink" title="协变"></a>协变</h2><p>使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。   </p><p>考虑以下简单的类结构：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Dog</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br></pre></td></tr></table></figure></p><p>类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。</p><p>在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CovarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printAnimalNames</span></span>(animals: <span class="type">List</span>[<span class="type">Animal</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    animals.foreach &#123; animal =&gt;</span><br><span class="line">      println(animal.name)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> cats: <span class="type">List</span>[<span class="type">Cat</span>] = <span class="type">List</span>(<span class="type">Cat</span>(<span class="string">"Whiskers"</span>), <span class="type">Cat</span>(<span class="string">"Tom"</span>))</span><br><span class="line">  <span class="keyword">val</span> dogs: <span class="type">List</span>[<span class="type">Dog</span>] = <span class="type">List</span>(<span class="type">Dog</span>(<span class="string">"Fido"</span>), <span class="type">Dog</span>(<span class="string">"Rex"</span>))</span><br><span class="line"></span><br><span class="line">  printAnimalNames(cats)</span><br><span class="line">  <span class="comment">// Whiskers</span></span><br><span class="line">  <span class="comment">// Tom</span></span><br><span class="line">  printAnimalNames(dogs)</span><br><span class="line">  <span class="comment">// Fido</span></span><br><span class="line">  <span class="comment">// Rex</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="逆变"><a href="#逆变" class="headerlink" title="逆变"></a>逆变</h2><p>通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。   </p><p>考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Printer</span>[-<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnimalPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Animal</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(animal: <span class="type">Animal</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The animal's name is: "</span> + animal.name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CatPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Cat</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(cat: <span class="type">Cat</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The cat's name is: "</span> + cat.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ContravarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> myCat: <span class="type">Cat</span> = <span class="type">Cat</span>(<span class="string">"Boots"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printMyCat</span></span>(printer: <span class="type">Printer</span>[<span class="type">Cat</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printer.print(myCat)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> catPrinter: <span class="type">Printer</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">CatPrinter</span></span><br><span class="line">  <span class="keyword">val</span> animalPrinter: <span class="type">Printer</span>[<span class="type">Animal</span>] = <span class="keyword">new</span> <span class="type">AnimalPrinter</span></span><br><span class="line"></span><br><span class="line">  printMyCat(catPrinter)</span><br><span class="line">  printMyCat(animalPrinter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个程序的输出如下：   </p><blockquote><p>The cat’s name is: Boots<br>The animal’s name is: Boots    </p></blockquote><h1 id="不变"><a href="#不变" class="headerlink" title="不变"></a>不变</h1><p>默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Container</span>[<span class="type">A</span>](<span class="params">value: <span class="type">A</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _value: <span class="type">A</span> = value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">A</span> = _value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setValue</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _value = value</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> catContainer: <span class="type">Container</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">Container</span>(<span class="type">Cat</span>(<span class="string">"Felix"</span>))</span><br><span class="line"><span class="keyword">val</span> animalContainer: <span class="type">Container</span>[<span class="type">Animal</span>] = catContainer</span><br><span class="line">animalContainer.setValue(<span class="type">Dog</span>(<span class="string">"Spot"</span>))</span><br><span class="line"><span class="keyword">val</span> cat: <span class="type">Cat</span> = catContainer.getValue</span><br></pre></td></tr></table></figure></p><p>  糟糕，我们最终会将一只狗作为值分配给一只猫<br>幸运的是，编译器在此之前就会阻止我们。</p><h2 id="上界"><a href="#上界" class="headerlink" title="上界"></a>上界</h2><p>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Pet</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Cat"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Dog"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lion</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Lion"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PetContainer</span>[<span class="type">P</span> &lt;: <span class="type">Pet</span>](<span class="params">p: <span class="type">P</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pet</span></span>: <span class="type">P</span> = p</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dogContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Dog</span>](<span class="keyword">new</span> <span class="type">Dog</span>)</span><br><span class="line"><span class="keyword">val</span> catContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Cat</span>](<span class="keyword">new</span> <span class="type">Cat</span>)</span><br><span class="line"><span class="comment">// this would not compile</span></span><br><span class="line"><span class="keyword">val</span> lionContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Lion</span>](<span class="keyword">new</span> <span class="type">Lion</span>)</span><br></pre></td></tr></table></figure></p><p>类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">arguments</span> [<span class="type">Lion</span>] <span class="title">do</span> <span class="title">not</span> <span class="title">conform</span> <span class="title">to</span> <span class="title">class</span> <span class="title">PetContainer</span>'<span class="title">s</span> <span class="title">type</span> <span class="title">parameter</span> <span class="title">bounds</span> [<span class="type">P</span> &lt;: <span class="type">Pet</span>]</span></span><br></pre></td></tr></table></figure></p><p>这是因为Lion并不是Pet的子类。   </p><h2 id="下界"><a href="#下界" class="headerlink" title="下界"></a>下界</h2><p>类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。  </p><p>下面看一个适合用类型下界的例子：<br>下面看一个适合用类型下界的例子：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">Node</span>[<span class="type">B</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。</p><p>但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。</p><p>要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">Node</span>[<span class="type">U</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>现在我们像下面这么做：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">AfricanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">EuropeanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">africanSwallowList=</span> <span class="title">ListNode</span>[<span class="type">AfricanSwallow</span>](<span class="params"><span class="type">AfricanSwallow</span>(</span>), <span class="title">Nil</span>(<span class="params"></span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">birdList</span></span>: <span class="type">Node</span>[<span class="type">Bird</span>] = africanSwallowList</span><br><span class="line">birdList.prepend(<span class="keyword">new</span> <span class="type">EuropeanSwallow</span>)</span><br></pre></td></tr></table></figure></p><p>可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。</p><h1 id="抽象类型"><a href="#抽象类型" class="headerlink" title="抽象类型"></a>抽象类型</h1><p>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">T</span></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">element</span></span>: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。<br>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span> <span class="keyword">extends</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span></span></span><br><span class="line"><span class="class">  <span class="title">type</span> <span class="title">T</span> <span class="title">&lt;</span></span>: <span class="type">Seq</span>[<span class="type">U</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。</p><p>含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSeqBuffer</span> <span class="keyword">extends</span> <span class="title">SeqBuffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span> </span>= <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(elem1: <span class="type">Int</span>, elem2: <span class="type">Int</span>): <span class="type">IntSeqBuffer</span> =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">IntSeqBuffer</span> &#123;</span><br><span class="line">       <span class="class"><span class="keyword">type</span> <span class="title">T</span> </span>= <span class="type">List</span>[<span class="type">U</span>]</span><br><span class="line">       <span class="keyword">val</span> element = <span class="type">List</span>(elem1, elem2)</span><br><span class="line">     &#125;</span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p><p>这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。</p><p>把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span>[+<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> element: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span>[<span class="type">U</span>, +<span class="type">T</span> &lt;: <span class="type">Seq</span>[<span class="type">U</span>]] <span class="keyword">extends</span> <span class="title">Buffer</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(e1: <span class="type">Int</span>, e2: <span class="type">Int</span>): <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]] &#123;</span><br><span class="line">    <span class="keyword">val</span> element = <span class="type">List</span>(e1, e2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p><p>需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。</p><h1 id="复合类型"><a href="#复合类型" class="headerlink" title="复合类型"></a>复合类型</h1><p><strong>需求</strong>:<br>有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。</p><p><strong>案例</strong>：<br>假设我们有两个特质 Cloneable 和 Resetable：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Cloneable</span> <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Cloneable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clone</span></span>(): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.clone().asInstanceOf[<span class="type">Cloneable</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Resetable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>: <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: ?): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> cloned = obj.clone()</span><br><span class="line">  obj.reset</span><br><span class="line">  cloned</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。</p><p>以下是更新后的方法：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: <span class="type">Cloneable</span> <span class="keyword">with</span> <span class="type">Resetable</span>): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement }</p><p>关于使用细化的例子参考 通过混入（mixin）来组合类。</p><h1 id="自类型"><a href="#自类型" class="headerlink" title="自类型"></a>自类型</h1><p>自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。</p><p>自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。</p><p>要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">username</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Tweeter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>: <span class="type">User</span> =&gt;  <span class="comment">// 重新赋予 this 的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tweet</span></span>(tweetText: <span class="type">String</span>) = println(<span class="string">s"<span class="subst">$username</span>: <span class="subst">$tweetText</span>"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VerifiedTweeter</span>(<span class="params">val username_ : <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Tweeter</span> <span class="keyword">with</span> <span class="title">User</span> </span>&#123;  <span class="comment">// 我们混入特质 User 因为 Tweeter 需要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">username</span> </span>= <span class="string">s"real <span class="subst">$username_</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> realBeyoncé = <span class="keyword">new</span> <span class="type">VerifiedTweeter</span>(<span class="string">"Beyoncé"</span>)</span><br><span class="line">realBeyoncé.tweet(<span class="string">"Just spilled my glass of lemonade"</span>)  <span class="comment">// 打印出 "real Beyoncé: Just spilled my glass of lemonade"</span></span><br></pre></td></tr></table></figure></p><p>因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。<br><strong>自类型别名</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  self =&gt;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(num1: <span class="type">Int</span>, num2: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    num1 + num2</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(self.sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h1><p><strong>定义</strong>：指的是那种以implicit关键字声明的带有单个参数的函数。<br>通过隐式转换，程序员可以在编写Scala程序时故意漏掉一些信息，让编译器去尝试在编译期间自动推导出这些信息来，这种特性可以极大的减少代码量，忽略那些冗长，过于细节的代码。<br>1.将方法或变量标记为implicit<br>2.将方法的参数列表标记为implicit<br>3.将类标记为implicit   </p><p>Scala支持两种形式的隐式转换：<br>隐式值：用于给方法提供参数<br>隐式视图：用于类型间转换或使针对某类型的方法能调用成功   </p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="给File类增加read方法"><a href="#给File类增加read方法" class="headerlink" title="给File类增加read方法"></a>给File类增加read方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">val f: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>() = <span class="type">Source</span>.fromFile(f).mkString</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFilePredef</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">fileToRichFile</span></span>(f: <span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile</span>(f)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>使用</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"><span class="keyword">import</span> com.tm.scala.implic.<span class="type">RichFilePredef</span>._</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFile</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">     <span class="keyword">val</span> f = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"/home/hadoop/sample_movielens_data.txt"</span>)</span><br><span class="line">    <span class="comment">// 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object</span></span><br><span class="line">    <span class="keyword">val</span> contents = f.read()</span><br><span class="line">    println(contents)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="排序中的使用案例"><a href="#排序中的使用案例" class="headerlink" title="排序中的使用案例"></a>排序中的使用案例</h3><h4 id="Ordered方式"><a href="#Ordered方式" class="headerlink" title="Ordered方式"></a>Ordered方式</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Girl</span>(<span class="params">val name: <span class="type">String</span>, var faceValue: <span class="type">Int</span>, var age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p><strong>定义比较规则</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 视图定界</span></span><br><span class="line"><span class="comment">  * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered</span></span><br><span class="line"><span class="comment">  * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。</span></span><br><span class="line"><span class="comment">  * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。</span></span><br><span class="line"><span class="comment">  * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderedChooser</span>[<span class="type">T</span> &lt;% <span class="type">Ordered</span>[<span class="type">T</span>]] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (first &gt; second) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderedPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 方式一</span></span><br><span class="line"> <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">gilrToOrdered</span></span>(girl: <span class="type">Girl</span>):<span class="type">Ordered</span>[<span class="type">Girl</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">            girl.age - that.age</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            girl.faceValue - that.faceValue</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdered = (girl: <span class="type">Girl</span>) =&gt; <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">        girl.age - that.age</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        girl.faceValue - that.faceValue</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdered</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> girl1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"spark"</span>, <span class="number">100</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> girl2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"mxnet"</span>, <span class="number">90</span>,<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderedPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> chooser = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = chooser.choose(girl1, girl2)</span><br><span class="line">    println(g.faceValue)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="Ordering方式"><a href="#Ordering方式" class="headerlink" title="Ordering方式"></a>Ordering方式</h4><p><strong>定义比较规则</strong>  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 文本定界</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderingChoose</span>[<span class="type">T</span>: <span class="type">Ordering</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ord = implicitly[<span class="type">Ordering</span>[<span class="type">T</span>]]</span><br><span class="line">    <span class="keyword">if</span> (ord.gt(first, second)) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderingPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">//  方式一</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdering = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//  方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">GilrToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式三</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 参考：Ordering中的下面方法</span></span><br><span class="line"><span class="comment">    * trait IntOrdering extends Ordering[Int] &#123;</span></span><br><span class="line"><span class="comment">    * def compare(x: Int, y: Int) =</span></span><br><span class="line"><span class="comment">    * if (x &lt; y) -1</span></span><br><span class="line"><span class="comment">    * else if (x == y) 0</span></span><br><span class="line"><span class="comment">    * else 1</span></span><br><span class="line"><span class="comment">    * &#125;</span></span><br><span class="line"><span class="comment">    * implicit object Int extends IntOrdering</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">trait</span> <span class="title">GirlToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">Girl</span> <span class="keyword">extends</span> <span class="title">GirlToOrdering</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure></p><p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdering</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> g1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"zhangsan"</span>, <span class="number">50</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> g2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"lisi"</span>, <span class="number">500</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderingPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> choose = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = choose.choose(g1, g2)</span><br><span class="line">    println(g.name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="scala导入类并取别名"><a href="#scala导入类并取别名" class="headerlink" title="scala导入类并取别名"></a>scala导入类并取别名</h1><p>导入Map,并取别名<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Map</span> =&gt; <span class="type">JMap</span>&#125;</span><br></pre></td></tr></table></figure></p><h1 id="scala-Try的使用"><a href="#scala-Try的使用" class="headerlink" title="scala Try的使用"></a>scala Try的使用</h1><p>spark Utils中的使用案例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classIsLoadable</span></span>(clazz: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   <span class="comment">// scalastyle:off classforname</span></span><br><span class="line">   <span class="type">Try</span> &#123;</span><br><span class="line">     <span class="type">Class</span>.forName(clazz, <span class="literal">false</span>, getContextOrSparkClassLoader)</span><br><span class="line">   &#125;.isSuccess</span><br><span class="line">   <span class="comment">// scalastyle:on classforname</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><h1 id="quasiquotes-q字符串-用于代码生成"><a href="#quasiquotes-q字符串-用于代码生成" class="headerlink" title="quasiquotes(q字符串)用于代码生成"></a>quasiquotes(q字符串)用于代码生成</h1><p>官方文档： <a href="https://docs.scala-lang.org/overviews/quasiquotes/intro.html" target="_blank" rel="noopener">https://docs.scala-lang.org/overviews/quasiquotes/intro.html</a><br>参考文档：<a href="https://www.cnblogs.com/shishanyuan/p/8455786.html" target="_blank" rel="noopener">https://www.cnblogs.com/shishanyuan/p/8455786.html</a><br><strong>作用</strong>： Quasiquotes允许在Scala语言中对抽象语法树（AST）进行编程式构建，然后在运行时将其提供给Scala编译器以生成字节码。<br>以q开头的字符串是quasiquotes，虽然它们看起来像字符串，但它们在编译时由Scala编译器解析，并代表其代码的AST。   </p><pre><code class="scala"><span class="keyword">val</span> tree = <span class="string">q"i am { a quasiquote }"</span>  tree: universe.<span class="type">Tree</span> = i.am(a.quasiquote)   </code></pre><h1 id="符号详解"><a href="#符号详解" class="headerlink" title="符号详解"></a>符号详解</h1><h2 id="和-的区别"><a href="#和-的区别" class="headerlink" title="== 和===的区别"></a>== 和===的区别</h2><p>参考：<a href="http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie" target="_blank" rel="noopener">http://landcareweb.com/questions/25833/scala-sparkzhong-he-zhi-jian-de-qu-bie</a>      </p><blockquote><p>== 返回一个布尔值<br>=== 返回一列（包含两列元素比较的结果）</p></blockquote><h1 id="call-by-value-and-call-by-name"><a href="#call-by-value-and-call-by-name" class="headerlink" title="call-by-value and call-by-name"></a>call-by-value and call-by-name</h1><p>传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；<br>传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部   </p><h1 id="半生对象"><a href="#半生对象" class="headerlink" title="半生对象"></a>半生对象</h1><p>object中的构造器在第一次调用执行一次，以后调用的话不会多次执行。<br>object会有自己的构造方法，默认是没有参数的构造方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;高级函数&quot;&gt;&lt;a href=&quot;#高级函数&quot; class=&quot;headerlink&quot; title=&quot;高级函数&quot;&gt;&lt;/a&gt;高级函数&lt;/h1&gt;&lt;p&gt;高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。    &lt;/p&gt;
&lt;h2 id=&quot;匿名函数&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>sparksql详解</title>
    <link href="https://tgluon.github.io/2019/01/04/sparksql%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/01/04/sparksql详解/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-02-26T07:55:37.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-SQL的Dataset基本操作"><a href="#Spark-SQL的Dataset基本操作" class="headerlink" title="Spark SQL的Dataset基本操作"></a>Spark SQL的Dataset基本操作</h1><p><a href="https://www.toutiao.com/i6631318012546793992/" target="_blank" rel="noopener">https://www.toutiao.com/i6631318012546793992/</a><br>版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解   </p><h2 id="基本简介和准备工作"><a href="#基本简介和准备工作" class="headerlink" title="基本简介和准备工作"></a>基本简介和准备工作</h2><p>Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。</p><p>1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">DataFrame</span> </span>= <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure></p><p>结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。<br>下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。</p><p>首先要创建一个SparkSession:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(<span class="keyword">this</span>.getClass.getName)</span><br><span class="line">.setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">.set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"localhost"</span>) </span><br><span class="line"><span class="comment">// executor的实例数</span></span><br><span class="line">.set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>) </span><br><span class="line">.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"4"</span>) </span><br><span class="line"><span class="comment">// sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。</span></span><br><span class="line">.set(<span class="string">"spark.sql.shuffle.partitions"</span>,<span class="string">"4"</span>)</span><br><span class="line">.setJars(<span class="type">List</span>(<span class="string">"/Users/meitu/Desktop/sparkjar/bigdata.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span> </span><br><span class="line"> ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span> </span><br><span class="line">.builder() </span><br><span class="line">.config(sparkConf) </span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure></p><p>创建dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(</span><br><span class="line"><span class="type">Seq</span>( (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">150</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p><p>使用函数的时候要导入包：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col,expr&#125;</span><br></pre></td></tr></table></figure></p><h2 id="select"><a href="#select" class="headerlink" title="select"></a>select</h2><p>列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。<br>使用col函数，可以直接对列进行一些逻辑操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.select(<span class="string">"city"</span>,<span class="string">"year"</span>,<span class="string">"amount"</span>).show(<span class="number">1</span>)</span><br><span class="line">sales.select(col(<span class="string">"city"</span>),col(<span class="string">"amount"</span>)+<span class="number">1</span>).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h2 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h2><p>参数是字符串，且直接可以使用表达式。<br>也可以使用select+expr函数来替代。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.selectExpr(<span class="string">"city"</span>,<span class="string">"year as date"</span>,<span class="string">"amount+1"</span>).show(<span class="number">10</span>)</span><br><span class="line">sales.select(expr(<span class="string">"city"</span>),expr(<span class="string">"year as date"</span>),expr(<span class="string">"amount+1"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.filter(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.filter(row=&gt;&#123; row.getInt(<span class="number">2</span>)&gt;<span class="number">150</span>&#125;).show(<span class="number">10</span>)</span><br><span class="line">sales.filter(<span class="string">"amount &gt; 150 "</span>).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="where"><a href="#where" class="headerlink" title="where"></a>where</h2><p>类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.where(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.where(<span class="string">"amount &gt; 150 "</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h2><p>主要是以count和agg聚合函数为例讲解groupby函数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.groupBy(<span class="string">"city"</span>).count().show(<span class="number">10</span>)</span><br><span class="line">sales.groupBy(col(<span class="string">"city"</span>)).agg(sum(<span class="string">"amount"</span>).as(<span class="string">"total"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.union(sales).groupBy(<span class="string">"city"</span>).count().show()</span><br></pre></td></tr></table></figure></p><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>join操作相对比较复杂，具体如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 相同的列进行join</span></span><br><span class="line"> sales.join(sales,<span class="string">"city"</span>).show(<span class="number">10</span>)</span><br><span class="line"> <span class="comment">// 多列join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>)).show() </span><br><span class="line"><span class="comment">/* 指定join类型， join 类型可以选择: </span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, </span></span><br><span class="line"><span class="comment">`left`, `left_outer`, `right`, `right_outer`, </span></span><br><span class="line"><span class="comment">`left_semi`, `left_anti`. </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="comment">// 内部join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>),<span class="string">"inner"</span>).show() </span><br><span class="line"><span class="comment">/* join条件 ：</span></span><br><span class="line"><span class="comment">可以在join方法里放入join条件，</span></span><br><span class="line"><span class="comment">也可以使用where,这两种情况都要求字段名称不一样。</span></span><br><span class="line"><span class="comment">*/</span> </span><br><span class="line">sales.join(sales, col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() sales.join(sales).where(col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() </span><br><span class="line"> <span class="comment">/* </span></span><br><span class="line"><span class="comment">dataset的self join 此处使用where作为条件，</span></span><br><span class="line"><span class="comment">需要增加配置.set("spark.sql.crossJoin.enabled","true") </span></span><br><span class="line"><span class="comment">也可以加第三个参数，join类型，可以选择如下： </span></span><br><span class="line"><span class="comment">`inner`, `cross`, `outer`, `full`, `full_outer`, `left`,</span></span><br><span class="line"><span class="comment"> `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.join(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show() sales.join(sales).where(sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show()</span><br><span class="line"><span class="comment">/* joinwith,可以指定第三个参数，join类型，</span></span><br><span class="line"><span class="comment">类型可以选择如下：</span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, </span></span><br><span class="line"><span class="comment"> `right`, `right_outer`。 </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.joinWith(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>),<span class="string">"inner"</span>).show()</span><br></pre></td></tr></table></figure></p><p>输出结果： </p><h2 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h2><p>orderby 全局有序，其实用的还是sort<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.orderBy(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><p>全局排序，直接替换掉8小结的orderby即可。</p><h2 id="sortwithinpartition"><a href="#sortwithinpartition" class="headerlink" title="sortwithinpartition"></a>sortwithinpartition</h2><p>在分区内部进行排序，局部排序。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.sortWithinPartitions(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.sortWithinPartitions(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p><p>可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。</p><h2 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* withColumn </span></span><br><span class="line"><span class="comment">假如列，存在就替换，不存在新增 </span></span><br><span class="line"><span class="comment">withColumnRenamed </span></span><br><span class="line"><span class="comment">对已有的列进行重命名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//相当于给原来amount列，+1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 对amount列+1，然后将值增加到一个新列 amount1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount1"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 将amount列名，修改为amount1</span></span><br><span class="line">sales.withColumnRenamed(<span class="string">"amount"</span>,<span class="string">"amount1"</span>).show()</span><br></pre></td></tr></table></figure><h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><p>这个跟rdd的foreach一样，元素类型是row。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.foreach(row=&gt;&#123; </span><br><span class="line">println(row.getString(<span class="number">0</span>))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p><h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><p>跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sales.foreachPartition(partition=&gt;&#123; </span><br><span class="line"> <span class="comment">//打开数据库链接等 </span></span><br><span class="line"> partition.foreach(each=&gt;&#123; </span><br><span class="line"> println(each.getString(<span class="number">0</span>))</span><br><span class="line"> <span class="comment">//插入数据库 </span></span><br><span class="line"> &#125;)</span><br><span class="line"> <span class="comment">//关闭数据库链接 </span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><p>针对dataset的行去重，返回的是所有行都不重复的dataset。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.distinct().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="dropDuplicates"><a href="#dropDuplicates" class="headerlink" title="dropDuplicates"></a>dropDuplicates</h2><p>这个适用于dataset有唯一的主键，然后对主键进行去重。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> before = sales.count()</span><br><span class="line"><span class="keyword">val</span> after = sales.dropDuplicates(<span class="string">"city"</span>).count()</span><br><span class="line">println(<span class="string">"before ====&gt; "</span> +before)</span><br><span class="line">println(<span class="string">"after ====&gt; "</span>+after)</span><br></pre></td></tr></table></figure></p><h2 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h2><p>删除一列，或者多列，这是一个变参数算子。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sales.drop(<span class="string">"city"</span>).show()</span><br><span class="line">打印出来schema信息如下：</span><br><span class="line">root</span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><h2 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h2><p>输出dataset的schema信息<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sales.printSchema()</span><br><span class="line"></span><br><span class="line">输出结果如下：</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"></span><br><span class="line">|-- city: string (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><h2 id="explain"><a href="#explain" class="headerlink" title="explain()"></a>explain()</h2><p>打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).explain()</span><br><span class="line">执行计划输出如下：</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">1</span>) <span class="type">Sort</span> [year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>], <span class="literal">true</span>, <span class="number">0</span></span><br><span class="line">+- <span class="type">Exchange</span> rangepartitioning(year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>, <span class="number">3</span>)</span><br><span class="line">+- <span class="type">LocalTableScan</span> [city#<span class="number">6</span>, year#<span class="number">7</span>, amount#<span class="number">8</span>]</span><br></pre></td></tr></table></figure></p><h1 id="Spark-SQL入门到精通之第二篇Dataset的复杂操作"><a href="#Spark-SQL入门到精通之第二篇Dataset的复杂操作" class="headerlink" title="Spark SQL入门到精通之第二篇Dataset的复杂操作"></a>Spark SQL入门到精通之第二篇Dataset的复杂操作</h1><p>本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：<br>cube，rollup，pivot操作。</p><h2 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h2><p>简单的理解就是维度及度量组成的数据体。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sales.cube(<span class="string">"city"</span>,<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>)) </span><br><span class="line">.sort(col(<span class="string">"city"</span>).desc_nulls_first,col(<span class="string">"year"</span>).desc_nulls_first) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。<br>实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> city_year = sales</span><br><span class="line">.groupBy(<span class="string">"city"</span>,<span class="string">"year"</span>).</span><br><span class="line">agg(sum(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> city = sales.groupBy(<span class="string">"city"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"city"</span>), lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> year = sales.groupBy(<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select( lit(<span class="literal">null</span>) as <span class="string">"city"</span>,col(<span class="string">"year"</span>), col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = sales .groupBy()</span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"city"</span>, lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>)) </span><br><span class="line">city_year.union(city).union(year).union(none) </span><br><span class="line">.sort(desc_nulls_first(<span class="string">"city"</span>), desc_nulls_first(<span class="string">"year"</span>)) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><h2 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h2><p>这里也是以案例开始，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> expenses = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line">((<span class="number">2012</span>, <span class="type">Month</span>.<span class="type">DECEMBER</span>, <span class="number">12</span>), <span class="number">5</span>), </span><br><span class="line"> ((<span class="number">2016</span>, <span class="type">Month</span>.<span class="type">AUGUST</span>, <span class="number">13</span>), <span class="number">10</span>), </span><br><span class="line"> ((<span class="number">2017</span>, <span class="type">Month</span>.<span class="type">MAY</span>, <span class="number">27</span>), <span class="number">15</span>)) </span><br><span class="line">.map &#123; <span class="keyword">case</span> ((yy, mm, dd), a) =&gt; (<span class="type">LocalDate</span>.of(yy, mm, dd), a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (d.toString, a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (<span class="type">Date</span>.valueOf(d), a) &#125;).toDF(<span class="string">"date"</span>, <span class="string">"amount"</span>)</span><br><span class="line"><span class="comment">// rollup time!</span></span><br><span class="line"><span class="keyword">val</span> res = expenses </span><br><span class="line">.rollup(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> year_month = expenses</span><br><span class="line">.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>)</span><br><span class="line"><span class="keyword">val</span> yearOnly = expenses.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"year"</span>), lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = expenses.groupBy() </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"year"</span>, lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line">year_month.union(yearOnly).union(none) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><h2 id="pivot"><a href="#pivot" class="headerlink" title="pivot"></a>pivot</h2><p>旋转操作<br><a href="https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw</a><br>Pivot 算子是 spark 1.6 版本开始引入的，在 spark2.4版本中功能做了增强，还是比较强大的，做过数据清洗ETL工作的都知道，行列转换是一个常见的数据整理需求。spark 中的Pivot 可以根据枢轴点(Pivot Point) 把多行的值归并到一行数据的不同列，这个估计不太好理解，我们下面使用例子说明，看看pivot 这个算子在处理复杂数据时候的威力。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line">(<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">150</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Toronto"</span>, <span class="number">2017</span>, <span class="number">50</span>,<span class="string">"Toronto"</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>,<span class="string">"test"</span>)</span><br><span class="line">sales.groupBy(<span class="string">"year"</span>)</span><br><span class="line">.pivot(<span class="string">"city"</span>,<span class="type">Seq</span>(<span class="string">"Warsaw"</span>,<span class="string">"Boston"</span>,<span class="string">"Toronto"</span>)) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>思路就是首先对year分组，然后旋转city字段，只取:<br>“Warsaw”,”Boston”,”Toronto”<br>然后对amount进行聚合操作<br><strong>案例：</strong><br>使用Pivot 来统计天气走势。<br>下面是西雅图的天气数据表，每行代表一天的天气最高值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  <span class="type">Date</span>         <span class="type">Temp</span> (°<span class="type">F</span>)   </span><br><span class="line"><span class="number">07</span><span class="number">-22</span><span class="number">-2018</span><span class="number">86</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-23</span><span class="number">-2018</span><span class="number">90</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-24</span><span class="number">-2018</span><span class="number">91</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-25</span><span class="number">-2018</span><span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-26</span><span class="number">-2018</span><span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-27</span><span class="number">-2018</span><span class="number">88</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-28</span><span class="number">-2018</span><span class="number">85</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-29</span><span class="number">-2018</span><span class="number">94</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-30</span><span class="number">-2018</span><span class="number">89</span></span><br></pre></td></tr></table></figure></p><p>如果我们想看下最近几年的天气走势，如果这样一天一行数据，是很难看出趋势来的，最直观的方式是 按照年来分行，然后每一列代表一个月的平均天气，这样一行数据，就可以看到这一年12个月的一个天气走势，下面我们使用 pivot 来构造这样一个查询结果：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (</span><br><span class="line">      <span class="number">1</span> JAN, <span class="number">2</span> FEB, <span class="number">3</span> MAR, <span class="number">4</span> APR, <span class="number">5</span> MAY, <span class="number">6</span> JUN, <span class="number">7</span> JUL, </span><br><span class="line">      <span class="number">8</span> AUG, <span class="number">9</span> SEP, <span class="number">10</span> <span class="keyword">OCT</span>, <span class="number">11</span> NOV, <span class="number">12</span> <span class="built_in">DEC</span></span><br><span class="line">    )</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult1.png" alt=""><br>是不是很直观，第一行就代表 2018 年，从1月到12月的平均天气，能看出一年的天气走势。<br>我们来看下这个 sql 是怎么玩的，首先是一个 子查询语句，我们最终关心的是 年份，月份，和最高天气值，所以先使用子查询对原始数据进行处理，从日期里面抽取出来年份和月份。</p><p>下面在子查询的结果上使用 pivot 语句，pivot 第一个参数是一个聚合语句，这个代表聚合出来一个月30天的一个平均气温，第二个参数是 FOR month，这个是指定以哪个列为枢轴列，第三个 In 子语句指定我们需要进行行列转换的具体的 枢轴点(Pivot Point)的值，上面的例子中 1到12月份都包含了，而且给了一个别名，如果只指定 1到6月份，结果就如下了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult2.png" alt=""><br>上面sql语句里面有个特别的点需要注意， 就是聚合的时候有个隐含的维度字段，就是 年份，按理来讲，我们没有写  group-by year， 为啥结果表里面不同年份区分在了不同的行，原因是，FORM 子查询出来的每行有 3个列， year，month，tmp，如果一个列既不出现在进行聚合计算的列中（temp 是聚合计算的列）， 也不作为枢轴列 ， 就会作为聚合的时候一个隐含的维度。我们的例子中算平均值聚合操作的维度是 (year, month)，一个是隐含维度，一个是 枢轴列维度， 这一点一定要注意，如果不需要按照 year 来区分，FORM 查询的时候就不要加上这个列。<br><strong><em>指定多个聚合语句</em></strong><br>上文中只有一个聚合语句，就是计算平均天气，其实是可以加多个聚合语句的，比如我们需要看到 7，8，9 月份每个月的最大气温和平均气温，就可以用以下SQL语句。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">avg</span>, </span><br><span class="line">    <span class="keyword">max</span>(temp) <span class="keyword">max</span> <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN, <span class="number">7</span> JUL, <span class="number">8</span> AUG, <span class="number">9</span> SEP)</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>上文中指定了两个聚合语句，查询后， 枢轴点(Pivot Point)  和 聚合语句 的笛卡尔积作为结果的不同列，也就是  <value>_<aggexpr>， 看下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult3.png" alt=""><br><strong><em>聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处</em></strong><br>现在假如我们有西雅图每天的最低温数据，我们需要把最高温和最低温放在同一张表里面看对比着看.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Date</span>   <span class="type">Temp</span> (°<span class="type">F</span>)</span><br><span class="line">…        …</span><br><span class="line"><span class="number">08</span><span class="number">-01</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-02</span><span class="number">-2018</span><span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-03</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-04</span><span class="number">-2018</span><span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-05</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-06</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line">…         …</span><br></pre></td></tr></table></figure></aggexpr></value></p><p>我们使用  UNION ALL 把两张表做一个合并：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'H'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  high_temps </span><br><span class="line"><span class="keyword">UNION</span> ALL </span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'L'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  low_temps;</span><br></pre></td></tr></table></figure></p><p>现在使用 pivot 来进行处理：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN,<span class="number">7</span> JUL,<span class="number">8</span> AUG , <span class="number">9</span> SEP)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span> ,<span class="string">`H/L`</span> <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure></p><p>我们统计了 4年中  7，8，9 月份最低温和最高温的平均值，这里要注意的是，我们把 year 和 一个最低最高的标记（H/L）都作为隐含维度，不然算出来的就是最低最高温度在一起的平均值了。结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult4.png" alt=""><br>上面的查询中，我们把最低最高的标记（H/L）也作为了一个隐含维度，group-by 的维度就变成了 （year， H/L, month）, 但是year 和 H/L 体现在了不同行上面，month 体现在了不同的列上面，这就是  聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处。</p><p>再接再厉，我们把 H/L 作为  枢轴列（Pivot Columns） 进行查询：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> (<span class="keyword">month</span>,flag) <span class="keyword">in</span> (</span><br><span class="line">(<span class="number">6</span>,<span class="string">'H'</span>) JUN_hi,(<span class="number">6</span>,<span class="string">'L'</span>) JUN_lo,</span><br><span class="line">(<span class="number">7</span>,<span class="string">'H'</span>) JUl_hi,(<span class="number">7</span>,<span class="string">'L'</span>) JUl_lo,</span><br><span class="line">(<span class="number">8</span>,<span class="string">'H'</span>) AUG_hi,(<span class="number">8</span>,<span class="string">'L'</span>) AUG_lo,</span><br><span class="line">(<span class="number">9</span>,<span class="string">'H'</span>) SEP_hi,(<span class="number">9</span>,<span class="string">'L'</span>) SEP_lo</span><br><span class="line">)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>结果的展现方式就和上面的不同了，虽然每个单元格值都是相同的，但是把  H/L 和 month 笛卡尔积作为列，H/L 维度体现在了列上面了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult5.png" alt="">  </p><h1 id="源码-Spark-SQL-分区特性第一弹"><a href="#源码-Spark-SQL-分区特性第一弹" class="headerlink" title="源码:Spark SQL 分区特性第一弹"></a>源码:Spark SQL 分区特性第一弹</h1><h2 id="常见RDD分区"><a href="#常见RDD分区" class="headerlink" title="常见RDD分区"></a>常见RDD分区</h2><p>Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。<br>举几个浪尖在星球里分享比较多的例子，比如：<br>Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。<br>Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。<br>普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。<br>这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。  </p><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>首先是由Seq数据集合生成一个Dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">80</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">130</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">160</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">170</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">180</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">20</span>)</span><br><span class="line">   )).toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p><p>将Dataset存处为partquet格式的hive表，分两种情况：<br>用city和year字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>,<span class="string">"year"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCityAndYear"</span>)</span><br></pre></td></tr></table></figure></p><p>用city字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCity"</span>)</span><br></pre></td></tr></table></figure></p><p>读取数据采用的是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> res = spark.read.parquet(<span class="string">"/user/hive/warehouse/parquettestcity"</span>)</span><br></pre></td></tr></table></figure></p><p>直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。   </p><h3 id="1-spark-default-parallelism-40"><a href="#1-spark-default-parallelism-40" class="headerlink" title="1. spark.default.parallelism =40"></a>1. spark.default.parallelism =40</h3><p>Dataset的分区数是由参数：<br>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行<br>这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。   </p><h3 id="2-spark-default-parallelism-4"><a href="#2-spark-default-parallelism-4" class="headerlink" title="2. spark.default.parallelism =4"></a>2. spark.default.parallelism =4</h3><p>Dataset的分区数是由参数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">"partition size = "</span>+res.rdd.partitions.length)</span><br></pre></td></tr></table></figure></p><p>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。<br><img src="/2019/01/04/sparksql详解/images/dataset分区.jpg" alt=""><br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。</p><p>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createNonBucketedReadRDD</span></span>(</span><br><span class="line">                                       readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">                                       selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">                                       fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">     selectedPartitions 的大小代表目录数目</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(<span class="string">"selectedPartitions.size : "</span>+ selectedPartitions.size)</span><br><span class="line">   <span class="keyword">val</span> defaultMaxSplitBytes =</span><br><span class="line">     fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br><span class="line">   <span class="keyword">val</span> openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br><span class="line"></span><br><span class="line">   <span class="comment">// spark.default.parallelism</span></span><br><span class="line">   <span class="keyword">val</span> defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 计算文件总大小，单位字节数</span></span><br><span class="line">   <span class="keyword">val</span> totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br><span class="line"></span><br><span class="line">   <span class="comment">//计算平均每个并行度读取数据大小</span></span><br><span class="line">   <span class="keyword">val</span> bytesPerCore = totalBytes / defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值</span></span><br><span class="line">   <span class="comment">// 然后，比较spark.sql.files.maxPartitionBytes 取小者</span></span><br><span class="line">   <span class="keyword">val</span> maxSplitBytes = <span class="type">Math</span>.min(defaultMaxSplitBytes, <span class="type">Math</span>.max(openCostInBytes, bytesPerCore))</span><br><span class="line">   logInfo(<span class="string">s"Planning scan with bin packing, max size: <span class="subst">$maxSplitBytes</span> bytes, "</span> +</span><br><span class="line">     <span class="string">s"open cost is considered as scanning <span class="subst">$openCostInBytes</span> bytes."</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 这对目录遍历</span></span><br><span class="line">   <span class="keyword">val</span> splitFiles = selectedPartitions.flatMap &#123; partition =&gt;</span><br><span class="line">     partition.files.flatMap &#123; file =&gt;</span><br><span class="line">       <span class="keyword">val</span> blockLocations = getBlockLocations(file)</span><br><span class="line"></span><br><span class="line">       <span class="comment">//判断文件类型是否支持分割，以parquet为例，是支持分割的</span></span><br><span class="line">       <span class="keyword">if</span> (fsRelation.fileFormat.isSplitable(</span><br><span class="line">         fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// eg. 0 until 2不包括 2。相当于</span></span><br><span class="line">    <span class="comment">// println(0 until(10) by 3) 输出 Range(0, 3, 6, 9)</span></span><br><span class="line">         (<span class="number">0</span>L until file.getLen by maxSplitBytes).map &#123; offset =&gt;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 计算文件剩余的量</span></span><br><span class="line">           <span class="keyword">val</span> remaining = file.getLen - offset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区</span></span><br><span class="line">           <span class="keyword">val</span> size = <span class="keyword">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="keyword">else</span> remaining</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 位置信息</span></span><br><span class="line">           <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, offset, size)</span><br><span class="line">           <span class="type">PartitionedFile</span>(</span><br><span class="line">             partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// 不可分割的话，那即是一个文件一个分区</span></span><br><span class="line">         <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, <span class="number">0</span>, file.getLen)</span><br><span class="line">         <span class="type">Seq</span>(<span class="type">PartitionedFile</span>(</span><br><span class="line">           partition.values, file.getPath.toUri.toString, <span class="number">0</span>, file.getLen, hosts))</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;.toArray.sortBy(_.length)(implicitly[<span class="type">Ordering</span>[<span class="type">Long</span>]].reverse)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> partitions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">FilePartition</span>]</span><br><span class="line">   <span class="keyword">val</span> currentFiles = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">PartitionedFile</span>]</span><br><span class="line">   <span class="keyword">var</span> currentSize = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Close the current partition and move to the next. */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">closePartition</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">if</span> (currentFiles.nonEmpty) &#123;</span><br><span class="line">       <span class="keyword">val</span> newPartition =</span><br><span class="line">         <span class="type">FilePartition</span>(</span><br><span class="line">           partitions.size,</span><br><span class="line">           currentFiles.toArray.toSeq) <span class="comment">// Copy to a new Array.</span></span><br><span class="line">       partitions += newPartition</span><br><span class="line">     &#125;</span><br><span class="line">     currentFiles.clear()</span><br><span class="line">     currentSize = <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Assign files to partitions using "Next Fit Decreasing"</span></span><br><span class="line">   splitFiles.foreach &#123; file =&gt;</span><br><span class="line">     <span class="keyword">if</span> (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class="line">       closePartition()</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Add the given file to the current partition.</span></span><br><span class="line">     currentSize += file.length + openCostInBytes</span><br><span class="line">     currentFiles += file</span><br><span class="line">   &#125;</span><br><span class="line">   closePartition()</span><br><span class="line"></span><br><span class="line">   println(<span class="string">"FileScanRDD partitions size : "</span>+partitions.size)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, partitions)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><h1 id="找到一列中的中位数"><a href="#找到一列中的中位数" class="headerlink" title="找到一列中的中位数"></a>找到一列中的中位数</h1><p><a href="https://spark.apache.org/docs/latest/api/sql/index.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/api/sql/index.html</a><br>df函数： approxQuantile<br>sql函数： percentile_approx  </p><h1 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h1><p>ServiceLoader<br><a href="https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w</a><br>ServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。<br>要配置在相关项目的固定目录下：<br>resources/META-INF/services/接口全称。<br>这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：<br>SparkContext类<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getClusterManager</span></span>(url: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">ExternalClusterManager</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="keyword">val</span> serviceLoaders =</span><br><span class="line">      <span class="type">ServiceLoader</span>.load(classOf[<span class="type">ExternalClusterManager</span>], loader).asScala.filter(_.canCreate(url))</span><br><span class="line">    <span class="keyword">if</span> (serviceLoaders.size &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">        <span class="string">s"Multiple external cluster managers registered for the url <span class="subst">$url</span>: <span class="subst">$serviceLoaders</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    serviceLoaders.headOption</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>配置是在<br><img src="/2019/01/04/sparksql详解/images/ExternalClusterManager.png" alt=""><br>spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。<br>简单测试<br>首先实现一个接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="comment">//可以制定实现类名加载</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后将接口配置在resources/META-INF/services/<br>bigdata.spark.services.DoSomething文件<br>内容：<br>实现该接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SayHello</span> <span class="keyword">implements</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="string">"SayHello"</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(<span class="string">"hello !!!"</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>测试</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceLoader;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line">   <span class="keyword">static</span> ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class);</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">       <span class="keyword">for</span>(DoSomething sayhello : loader)&#123;</span><br><span class="line">        <span class="comment">//要加载的类名称我们可以制定</span></span><br><span class="line">           <span class="keyword">if</span>(sayhello.shortName().equalsIgnoreCase(<span class="string">"SayHello"</span>))&#123;</span><br><span class="line">               sayhello.doSomeThing();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个主要是为讲自定义数据源作准备。    </p><p><a href="https://articles.zsxq.com/id_702s32f46zet.html" target="_blank" rel="noopener">https://articles.zsxq.com/id_702s32f46zet.html</a><br>首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Given a provider name, look up the data source class definition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lookupDataSource</span></span>(provider: <span class="type">String</span>): <span class="type">Class</span>[_] = &#123;</span><br><span class="line">    <span class="keyword">val</span> provider1 = backwardCompatibilityMap.getOrElse(provider, provider)</span><br><span class="line">    <span class="comment">// 指定路径加载，默认加载类名是DefaultSource</span></span><br><span class="line">    <span class="keyword">val</span> provider2 = <span class="string">s"<span class="subst">$provider1</span>.DefaultSource"</span></span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="comment">// ServiceLoader加载</span></span><br><span class="line">    <span class="keyword">val</span> serviceLoader = <span class="type">ServiceLoader</span>.load(classOf[<span class="type">DataSourceRegister</span>], loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// the provider format did not match any given registered aliases</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Try</span>(loader.loadClass(provider1)).orElse(<span class="type">Try</span>(loader.loadClass(provider2))) <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Success</span>(dataSource) =&gt;</span><br><span class="line">                <span class="comment">// Found the data source using fully qualified path</span></span><br><span class="line">                dataSource</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Failure</span>(error) =&gt;</span><br><span class="line">                <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"orc"</span> ||</span><br><span class="line">                  provider1.startsWith(<span class="string">"org.apache.spark.sql.hive.orc"</span>)) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">"The ORC data source must be used with Hive support enabled"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"avro"</span> ||</span><br><span class="line">                  provider1 == <span class="string">"com.databricks.spark.avro"</span>) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$&#123;provider1.toLowerCase&#125;</span>. Please find an Avro "</span> +</span><br><span class="line">                      <span class="string">"package at http://spark.apache.org/third-party-projects.html"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$provider1</span>. Please find packages at "</span> +</span><br><span class="line">                      <span class="string">"http://spark.apache.org/third-party-projects.html"</span>,</span><br><span class="line">                    error)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt; <span class="comment">// This one won't be caught by Scala NonFatal</span></span><br><span class="line">              <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">              <span class="keyword">val</span> className = e.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">              <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"<span class="subst">$className</span> was removed in Spark 2.0. "</span> +</span><br><span class="line">                  <span class="string">"Please check if your library is compatible with Spark 2.0"</span>, e)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> e</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> head :: <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="comment">// there is exactly one registered alias</span></span><br><span class="line">          head.getClass</span><br><span class="line">        <span class="keyword">case</span> sources =&gt;</span><br><span class="line">          <span class="comment">// There are multiple registered aliases for the input</span></span><br><span class="line">          sys.error(<span class="string">s"Multiple sources found for <span class="subst">$provider1</span> "</span> +</span><br><span class="line">            <span class="string">s"(<span class="subst">$&#123;sources.map(_.getClass.getName).mkString(", ")&#125;</span>), "</span> +</span><br><span class="line">            <span class="string">"please specify the fully qualified class name."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ServiceConfigurationError</span> <span class="keyword">if</span> e.getCause.isInstanceOf[<span class="type">NoClassDefFoundError</span>] =&gt;</span><br><span class="line">        <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">        <span class="keyword">val</span> className = e.getCause.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">        <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"Detected an incompatible DataSourceRegister. "</span> +</span><br><span class="line">            <span class="string">"Please remove the incompatible library from classpath or upgrade it. "</span> +</span><br><span class="line">            <span class="string">s"Error: <span class="subst">$&#123;e.getMessage&#125;</span>"</span>, e)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">throw</span> e</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p><p>其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。</p><p>主要思路是：</p><ol><li><p>实现DefaultSource。</p></li><li><p>实现工厂类。</p></li><li><p>实现具体的数据加载类。</p></li></ol><p>首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.&#123;<span class="type">DataSourceOptions</span>, <span class="type">DataSourceV2</span>, <span class="type">ReadSupport</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span>  <span class="keyword">extends</span> <span class="title">DataSourceV2</span> <span class="keyword">with</span> <span class="title">ReadSupport</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(options: <span class="type">DataSourceOptions</span>) = <span class="keyword">new</span> <span class="type">SimpleDataSourceReader</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后是，要实现DataSourceReader，负责创建阅读器工厂：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReaderFactory</span>, <span class="type">DataSourceReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span></span>() = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> factoryList = <span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">DataReaderFactory</span>[<span class="type">Row</span>]]</span><br><span class="line">    factoryList.add(<span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>())</span><br><span class="line">    factoryList</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>数据源的具体实现类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReader</span>, <span class="type">DataReaderFactory</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReaderFactory</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">  <span class="title">DataReaderFactory</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReader</span> </span>= <span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>()</span><br><span class="line">  <span class="keyword">val</span> values = <span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span> </span>= index &lt; values.length</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> row = <span class="type">Row</span>(values(index))</span><br><span class="line">    index = index + <span class="number">1</span></span><br><span class="line">    row</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() = <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用我们默认的数据源：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getName).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">      .set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"mt-mdh.local"</span>)</span><br><span class="line">      .set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>)</span><br><span class="line">      .setJars(<span class="type">List</span>(<span class="string">"/opt/sparkjar/bigdata.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span>))</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> simpleDf = spark.read</span><br><span class="line">      .format(<span class="string">"bigdata.spark.SparkSQL.DataSources"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    simpleDf.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。</p><h1 id="删除多个字段"><a href="#删除多个字段" class="headerlink" title="删除多个字段"></a>删除多个字段</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropColumns</span></span>(columns: <span class="type">Seq</span>[<span class="type">String</span>]):<span class="type">DataFAME</span>=&#123;</span><br><span class="line">  columns.foldLeft(dataFame)((df,column)=&gt; df.drop(column))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="spark-sql-窗口函数"><a href="#spark-sql-窗口函数" class="headerlink" title="spark sql 窗口函数"></a>spark sql 窗口函数</h1><p><a href="https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw</a><br>最近理了下spark sql 中窗口函数的知识，打算开几篇文章讲讲，窗口函数有很多应用场景，比如说炒股的时候有个5日移动均线，或者让你对一个公司所有销售所有部门按照销售业绩进行排名等等，都是要用到窗口函数，在spark sql中，窗口函数和聚合函数的区别，之前文章中也提到过，就是聚合函数是按照你聚合的维度，每个分组中算出来一个聚合值，而窗口函数是对每一行，都根据当前窗口（5日均线就是今日往前的5天组成的窗口），都聚合出一个值。</p><p>1  开胃菜，spark sql 窗口函数的基本概念和使用姿势<br>2  spark sql 中窗口函数深入理解<br>3  不是UDF，也不是UDAF，教你自定义一个窗口函数（UDWF）<br>4  从 spark sql 源码层面理解窗口函数   </p><h2 id="什么是简单移动平均值"><a href="#什么是简单移动平均值" class="headerlink" title="什么是简单移动平均值"></a>什么是简单移动平均值</h2><p>简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local"</span>)</span><br><span class="line">    .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">    .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 这个 window spec 中，数据根据用户(customer)来分去。</span></span><br><span class="line"><span class="comment">    * 每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行)</span></span><br><span class="line"><span class="comment">    * ，每一个滑动的窗口总用有3行</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  df.withColumn(<span class="string">"movinAvg"</span>, avg(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line">  spark.stop()</span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong><br>2~3~2<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------------------+</span><br><span class="line">|site|     date|user_cnt|          movinAvg|</span><br><span class="line">+----+---------+--------+------------------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|              <span class="number">70.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|              <span class="number">80.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|              <span class="number">95.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|              <span class="number">65.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|<span class="number">56.666666666666664</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|              <span class="number">50.0</span>|</span><br><span class="line">+----+---------+--------+------------------+</span><br></pre></td></tr></table></figure></p><h2 id="窗口函数和窗口特征定义"><a href="#窗口函数和窗口特征定义" class="headerlink" title="窗口函数和窗口特征定义"></a>窗口函数和窗口特征定义</h2><p>正如上述例子中，窗口函数主要包含两个部分：</p><p> 指定窗口特征（wSpec）：</p><p>“partitionyBY” 定义数据如何分组；在上面的例子中，是用户 site</p><p>“orderBy” 定义分组中的排序</p><p>“rowsBetween” 定义窗口的大小</p><p>指定窗口函数函数：</p><p>指定窗口函数函数，你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数.</p><h2 id="累计汇总"><a href="#累计汇总" class="headerlink" title="累计汇总"></a>累计汇总</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .master(<span class="string">"local"</span>)</span><br><span class="line">  .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">  .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line"><span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">"cumsum"</span>, sum(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------+</span><br><span class="line">|site|     date|user_cnt|cumsum|</span><br><span class="line">+----+---------+--------+------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">140</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">130</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">170</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">170</span>|</span><br><span class="line">+----+---------+--------+------+</span><br></pre></td></tr></table></figure></p><h2 id="前一行数据"><a href="#前一行数据" class="headerlink" title="前一行数据"></a>前一行数据</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"preUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>),<span class="number">1</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----------+</span><br><span class="line">|site|     date|user_cnt|preUserCnt|</span><br><span class="line">+----+---------+--------+----------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|        <span class="number">50</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|        <span class="number">90</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|        <span class="number">70</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|        <span class="number">60</span>|</span><br><span class="line">+----+---------+--------+----------+</span><br></pre></td></tr></table></figure></p><p>如果计算环比的时候，是不是特别有用啊？！</p><p>在介绍几个常用的行数：</p><p>first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到</p><p>lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件</p><h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"rank"</span>, rank().over(wSpce)).show()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----+</span><br><span class="line">|site|     date|user_cnt|rank|</span><br><span class="line">+----+---------+--------+----+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">3</span>|</span><br><span class="line">+----+---------+--------+----+</span><br></pre></td></tr></table></figure></p><p>这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。</p><h2 id="自定义窗口函数"><a href="#自定义窗口函数" class="headerlink" title="自定义窗口函数"></a>自定义窗口函数</h2><p><a href="https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ</a><br><a href="https://github.com/zheniantoushipashi/spark-udwf-session" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session</a>  </p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在使用 spark sql 的时候，有时候默认提供的sql 函数可能满足不了需求，这时候可以自定义一些函数，可以自定义 UDF 或者UDAF。</p><p>UDF 在sql中只是简单的处理转换一些字段，类似默认的trim 函数把一个字符串类型的列的头尾空格去掉， UDAF函数不同于UDF，是在sql聚合语句中使用的函数，必须配合 GROUP BY 一同使用，类似默认提供的count，sum函数，但是还有一种自定义函数叫做 UDWF， 这种一般人就不知道了，这种叫做窗口自定义函数，不了解窗口函数的，可以参考 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势 ，或者官方的介绍 <a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a>    </p><p>窗口函数是 SQL 中一类特别的函数。和聚合函数相似，<strong>窗口函数的输入也是多行记录</strong>。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口</p><p>这里怎么理解一个窗口呢，spark君在这里得好好的解释解释，一个窗口是怎么定义的，</p><p>窗口语句中，partition by用来指定分区的列，在同一个分区的行属于同一个窗口</p><p>order by用来指定窗口内的多行，如何排序</p><p>windowing_clause 用来指定开窗方式，在spark sql 中开窗方式有那么几种    </p><ul><li><p>一个分区中的所有行作为一个窗口:UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING（上下都没有边界)，这种情况下，spark sql 会把所有行作为一个输入，进行一次求值</p></li><li><p>Growing frame:UNBOUNDED PRECEDING AND ….（上无边界）, 这种就是不断的把当前行加入的窗口中，而不删除， 例子：.rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行，在数据迭代过程中，不断的把当前行加入的窗口中。</p></li><li><p>Shrinking frame：… AND UNBOUNDED FOLLOWING（下无边界）和Growing frame 相反，窗口不断的把迭代到的当前行从窗口中删除掉。</p></li><li><p>Moving frame：滑动的窗口，举例：.rowsBetween(-1, 1) 就是指窗口定义从 -1(当前行前一行)到 1(当前行后一行) ，每一个滑动的窗口总用有3行</p></li><li><p>Offset frame： 窗口中只有一条数据，就是偏移当前行一定距离的那一行，举例：lag(field, n): 就是取从当前行往前的n个行的字段值</p></li></ul><p>这里就针对窗口函数就介绍这么多，如果不懂请参考相关文档，加强理解，我们在平时使用 spark sql 的过程中，会发现有很多教你自定义 UDF 和 UDAF 的教程，却没有针对UDWF的教程，这是为啥呢，这是因为 UDF 和UDAF 都作为上层API暴露给用户了，使用scala很简单就可以写一个函数出来，但是UDWF没有对上层用户暴露，只能使用 Catalyst expressions. 也就是Catalyst框架底层的表达式语句才可以定义，如果没有对源码有很深入的研究，根本就搞不出来。spark 君在工作中写了一些UDWF的函数，但是都比较复杂，不太好单独抽出来作为一个简明的例子给大家讲解，挑一个网上的例子来进行说明，这个例子 spark君亲测可用。</p><h3 id="窗口函数的使用场景"><a href="#窗口函数的使用场景" class="headerlink" title="窗口函数的使用场景"></a>窗口函数的使用场景</h3><p>我们来举个实际例子来说明 窗口函数的使用场景，在网站的统计指标中，有一个概念叫做用户会话，什么叫做用户会话呢，我来说明一下，我们在网站服务端使用用户session来管理用户状态，过程如下</p><p>1) 服务端session是用户第一次访问应用时，服务器就会创建的对象，代表用户的一次会话过程，可以用来存放数据。服务器为每一个session都分配一个唯一的sessionid，以保证每个用户都有一个不同的session对象。</p><p>2）服务器在创建完session后，会把sessionid通过cookie返回给用户所在的浏览器，这样当用户第二次及以后向服务器发送请求的时候，就会通过cookie把sessionid传回给服务器，以便服务器能够根据sessionid找到与该用户对应的session对象。</p><p>3）session通常有失效时间的设定，比如1个小时。当失效时间到，服务器会销毁之前的session，并创建新的session返回给用户。但是只要用户在失效时间内，有发送新的请求给服务器，通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长1个小时。</p><p>也就是说如果用户在1个超过一个小时不产生用户事件，当前会话就结束了，如果后续再产生用户事件，就当做新的用户会话，我们现在就使用spark sql 来统计用户的会话数，首先我们先加一个列，作为当前列的session，然后再 distinct 这个列就得到用户的会话数了，所以关键是怎么加上这个newSession 这个列，这种场景就很适合使用窗口函数来做统计，因为判断当前是否是一个新会话的依据，需要依赖当前行的前一行的时间戳和当前行的时间戳的间隔来判断，下面的表格可以帮助你理解这个概念，例子中有3列数据，用户，event字段代表用户访问了一个页面产生了一个用户事件，time字段代表访问页面的时间戳：  </p><table><thead><tr><th>user</th><th>event</th><th>time</th><th>session </th></tr></thead><tbody><tr><td>user1</td><td>page1</td><td>10:12</td><td>session1(new session)</td><td></td></tr><tr><td>user1</td><td>page2</td><td>10:20</td><td>session1(same session,8 minutes from last event)</td><td></td></tr><tr><td>user1</td><td>page1</td><td>11:13</td><td>session1(same session,53 minutes from last event)</td><td></td></tr><tr><td>user1</td><td>page3</td><td>14:12</td><td>session1(new session,3 minutes from last event)</td><td></td></tr></tbody></table><p>上面只有一个用户，如果多个用户，可以使用 partition by 来进行分区。</p><h3 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h3><p><strong>构造数据</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActivityData</span>(<span class="params">user:<span class="type">String</span>, ts:<span class="type">Long</span>, session:<span class="type">String</span></span>)   </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">st</span> </span>= <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">  <span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="string">"a CustomWindowFunction"</span> should <span class="string">"correctly create a session "</span> in &#123;</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.createDataFrame(sc.parallelize(d))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">    <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br></pre></td></tr></table></figure></p><p>怎么使用 spark sql 来统计会话数目呢，因为不同用户产生的是不同的会话，首先使用user字段进行分区，然后按照时间戳进行排序，然后我们需要一个自定义函数来加一个列，这个列的值的逻辑如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IF (no previous event) <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> (<span class="keyword">if</span> cerrent <span class="keyword">event</span> was past <span class="keyword">session</span> window)</span><br><span class="line"><span class="keyword">THEN</span> <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> <span class="keyword">use</span> <span class="keyword">current</span> <span class="keyword">session</span></span><br></pre></td></tr></table></figure></p><p>运行结果如下：<br><img src="/2019/01/04/sparksql详解/./images/UDWFResult.jpg" alt=""><br>我们使用 UUID 来作为会话id， 当后一行的时间戳和前一行的时间戳间隔大于1小时的时候，就创建一个新的会话id作为列值，否则使用老的会话id作为列值。</p><p>这种就涉及到状态，我们在内部需要维护的状态数据</p><ul><li><p>当前的session ID</p></li><li><p>当前session的最后活动事件的时间戳</p></li></ul><p>下面我们就看看这个怎样自定义 caculateSession 这个窗口函数，先自定义一个静态对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Column</span>, <span class="type">Row</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">Add</span>, <span class="type">AggregateWindowFunction</span>, <span class="type">AttributeReference</span>, <span class="type">Expression</span>, <span class="type">If</span>, <span class="type">IsNotNull</span>, <span class="type">LessThanOrEqual</span>, <span class="type">Literal</span>, <span class="type">ScalaUDF</span>, <span class="type">Subtract</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.unsafe.types.<span class="type">UTF8String</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyUDWF</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> defaultMaxSessionLengthms = <span class="number">3600</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SessionUDWF</span>(<span class="params">timestamp:<span class="type">Expression</span>, session:<span class="type">Expression</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                         sessionWindow:<span class="type">Expression</span> = <span class="type">Literal</span>(defaultMaxSessionLengthms</span>)) <span class="keyword">extends</span> <span class="title">AggregateWindowFunction</span> </span>&#123;</span><br><span class="line">    self: <span class="type">Product</span> =&gt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">children</span></span>: <span class="type">Seq</span>[<span class="type">Expression</span>] = <span class="type">Seq</span>(timestamp, session)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> zero = <span class="type">Literal</span>( <span class="number">0</span>L )</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> nullString = <span class="type">Literal</span>(<span class="literal">null</span>:<span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> curentSession = <span class="type">AttributeReference</span>(<span class="string">"currentSession"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>)()</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> previousTs =    <span class="type">AttributeReference</span>(<span class="string">"lastTs"</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>)()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> aggBufferAttributes: <span class="type">Seq</span>[<span class="type">AttributeReference</span>] =  curentSession  :: previousTs :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> assignSession =  <span class="type">If</span>(<span class="type">LessThanOrEqual</span>(<span class="type">Subtract</span>(timestamp, aggBufferAttributes(<span class="number">1</span>)), sessionWindow),</span><br><span class="line">      aggBufferAttributes(<span class="number">0</span>), <span class="comment">// if</span></span><br><span class="line">      <span class="type">ScalaUDF</span>( createNewSession, <span class="type">StringType</span>, children = <span class="type">Nil</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> initialValues: <span class="type">Seq</span>[<span class="type">Expression</span>] =  nullString :: zero :: <span class="type">Nil</span></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> updateExpressions: <span class="type">Seq</span>[<span class="type">Expression</span>] =</span><br><span class="line">      <span class="type">If</span>(<span class="type">IsNotNull</span>(session), session, assignSession) ::</span><br><span class="line">        timestamp ::</span><br><span class="line">        <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> evaluateExpression: <span class="type">Expression</span> = aggBufferAttributes(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">prettyName</span></span>: <span class="type">String</span> = <span class="string">"makeSession"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">val</span>  createNewSession = () =&gt; org.apache.spark.unsafe.types.<span class="type">UTF8String</span>.fromString(<span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, <span class="type">Literal</span>(defaultMaxSessionLengthms)) &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>, sessionWindow:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, sessionWindow.expr) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withExpr</span></span>(expr: <span class="type">Expression</span>): <span class="type">Column</span> = <span class="keyword">new</span> <span class="type">Column</span>(expr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码说明：</p><ul><li><p>状态保存在 Seq[AttributeReference]中</p></li><li><p>重写 initialValues方法进行初始化</p></li><li><p>updateExpressions 函数针对每一行数据都会调用</p></li></ul><blockquote><p>spark sql 在迭代处理每一行数据的时候，都会调用 updateExpressions 函数来处理，根据当后一行的时间戳和前一行的时间戳间隔大于1小时来进行不同的逻辑处理，如果不大于，就使用 aggBufferAttributes(0) 中保存的老的sessionid，如果大于，就把 createNewSession 包装为一个scalaUDF作为一个子表达式来创建一个新的sessionID，并且每次都把当前行的时间戳作为用户活动的最后时间戳。</p></blockquote><p>最后包装为静态对象的方法，就可以在spark sql中使用这个自定义窗口函数了，下面是两个重载的方法，一个最大间隔时间使用默认值，一个可以运行用户自定义，perfect</p><p>现在，我们就可以拿来用在我们的main函数中了。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> st = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = spark.createDataFrame(sc.parallelize(d))</span><br><span class="line">  <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">  <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br><span class="line">  df.show(<span class="number">20</span>)</span><br><span class="line">  res.show(<span class="number">20</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><p>如果我们学会了自定义 spark 窗口函数，原理是就可以处理一切这种对前后数据依赖的统计需求，不过这种自定义毕竟需要对 spark 源码有很深入的研究才可以，这就需要功力了，希望 spark君的读者可以跟着spark君日益精进。文章中的代码可能不太清楚，完整demo参考  <a href="https://github.com/zheniantoushipashi/spark-udwf-session。" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session。</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spark-SQL的Dataset基本操作&quot;&gt;&lt;a href=&quot;#Spark-SQL的Dataset基本操作&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL的Dataset基本操作&quot;&gt;&lt;/a&gt;Spark SQL的Dataset基本操作&lt;/
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>scala特殊符号使用</title>
    <link href="https://tgluon.github.io/2019/01/04/scala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8/"/>
    <id>https://tgluon.github.io/2019/01/04/scala特殊符号使用/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-04-10T10:31:26.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="符号"><a href="#符号" class="headerlink" title="_*符号"></a>_*符号</h1><p>作用：它告诉编译器将序列类型的单个参数视为可变参数序列。<br>举例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">ProcessBuilder</span>(<span class="type">String</span>... command) &#123;</span><br><span class="line">       <span class="keyword">this</span>.command = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(command.length);</span><br><span class="line">       <span class="keyword">for</span> (<span class="type">String</span> arg : command)</span><br><span class="line">           <span class="keyword">this</span>.command.add(arg);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> commandSeq = <span class="type">Seq</span>(command.mainClass) ++ command.arguments</span><br><span class="line"><span class="keyword">val</span> builder = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(commandSeq: _*)</span><br></pre></td></tr></table></figure><h1 id=""><a href="#" class="headerlink" title="::"></a>::</h1><p>该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    args.toList <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> workerUrl :: userJar :: mainClass :: extraArgs =&gt;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">"Driver"</span>,</span><br><span class="line">          <span class="type">Utils</span>.localHostName(), <span class="number">0</span>, conf, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf))</span><br><span class="line">        rpcEnv.setupEndpoint(<span class="string">"workerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(rpcEnv, workerUrl))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> currentLoader = <span class="type">Thread</span>.currentThread.getContextClassLoader</span><br><span class="line">        <span class="keyword">val</span> userJarUrl = <span class="keyword">new</span> <span class="type">File</span>(userJar).toURI().toURL()</span><br><span class="line">        <span class="keyword">val</span> loader =</span><br><span class="line">          <span class="keyword">if</span> (sys.props.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delegate to supplied main class</span></span><br><span class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(mainClass)</span><br><span class="line">        <span class="keyword">val</span> mainMethod = clazz.getMethod(<span class="string">"main"</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line">        mainMethod.invoke(<span class="literal">null</span>, extraArgs.toArray[<span class="type">String</span>])</span><br><span class="line"></span><br><span class="line">        rpcEnv.shutdown()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="comment">// scalastyle:off println</span></span><br><span class="line">        <span class="type">System</span>.err.println(<span class="string">"Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]"</span>)</span><br><span class="line">        <span class="comment">// scalastyle:on println</span></span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;符号&quot;&gt;&lt;a href=&quot;#符号&quot; class=&quot;headerlink&quot; title=&quot;_*符号&quot;&gt;&lt;/a&gt;_*符号&lt;/h1&gt;&lt;p&gt;作用：它告诉编译器将序列类型的单个参数视为可变参数序列。&lt;br&gt;举例：&lt;br&gt;&lt;figure class=&quot;highlight sc
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala通过反射获取当前类类名</title>
    <link href="https://tgluon.github.io/2019/01/04/scala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D/"/>
    <id>https://tgluon.github.io/2019/01/04/scala通过反射获取当前类类名/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-04-10T10:31:26.661Z</updated>
    
    <content type="html"><![CDATA[<h1 id="先编写HelloWord-scala文件"><a href="#先编写HelloWord-scala文件" class="headerlink" title="先编写HelloWord.scala文件"></a>先编写HelloWord.scala文件</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 获取当前类类名</span></span><br><span class="line">    println(<span class="keyword">this</span>.getClass.getName.stripSuffix(<span class="string">"$"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class </p><h1 id="spark源码中的案例"><a href="#spark源码中的案例" class="headerlink" title="spark源码中的案例"></a>spark源码中的案例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"> * this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"> * (the "License"); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"> * the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.spark.internal</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">LogManager</span>, <span class="type">PropertyConfigurator</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.&#123;<span class="type">Logger</span>, <span class="type">LoggerFactory</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.impl.<span class="type">StaticLoggerBinder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">Utils</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows</span></span><br><span class="line"><span class="comment"> * logging messages at different levels using methods that only evaluate parameters lazily if the</span></span><br><span class="line"><span class="comment"> * log level is enabled.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make the log field transient so that objects with Logging can</span></span><br><span class="line">  <span class="comment">// be serialized and used on another machine</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> log_ : <span class="type">Logger</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method to get the logger name for this object</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= &#123;</span><br><span class="line">    <span class="comment">// Ignore trailing $'s in the class names for Scala objects</span></span><br><span class="line">    <span class="keyword">this</span>.getClass.getName.stripSuffix(<span class="string">"$"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method to get or create the logger for this object</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>: <span class="type">Logger</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (log_ == <span class="literal">null</span>) &#123;</span><br><span class="line">      initializeLogIfNecessary(<span class="literal">false</span>)</span><br><span class="line">      log_ = <span class="type">LoggerFactory</span>.getLogger(logName)</span><br><span class="line">    &#125;</span><br><span class="line">    log_</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Log methods that take only a String</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isInfoEnabled) log.info(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logDebug</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled) log.debug(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logTrace</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isTraceEnabled) log.trace(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isWarnEnabled) log.warn(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isErrorEnabled) log.error(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Log methods that take Throwables (Exceptions/Errors) too</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isInfoEnabled) log.info(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logDebug</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled) log.debug(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logTrace</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isTraceEnabled) log.trace(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isWarnEnabled) log.warn(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isErrorEnabled) log.error(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">isTraceEnabled</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    log.isTraceEnabled</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLogIfNecessary</span></span>(isInterpreter: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="type">Logging</span>.initialized) &#123;</span><br><span class="line">      <span class="type">Logging</span>.initLock.synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="type">Logging</span>.initialized) &#123;</span><br><span class="line">          initializeLogging(isInterpreter)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLogging</span></span>(isInterpreter: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Don't use a logger in here, as this is itself occurring during initialization of a logger</span></span><br><span class="line">    <span class="comment">// If Log4j 1.2 is being used, but is not initialized, load a default properties file</span></span><br><span class="line">    <span class="keyword">val</span> binderClass = <span class="type">StaticLoggerBinder</span>.getSingleton.getLoggerFactoryClassStr</span><br><span class="line">    <span class="comment">// This distinguishes the log4j 1.2 binding, currently</span></span><br><span class="line">    <span class="comment">// org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently</span></span><br><span class="line">    <span class="comment">// org.apache.logging.slf4j.Log4jLoggerFactory</span></span><br><span class="line">    <span class="keyword">val</span> usingLog4j12 = <span class="string">"org.slf4j.impl.Log4jLoggerFactory"</span>.equals(binderClass)</span><br><span class="line">    <span class="keyword">if</span> (usingLog4j12) &#123;</span><br><span class="line">      <span class="keyword">val</span> log4j12Initialized = <span class="type">LogManager</span>.getRootLogger.getAllAppenders.hasMoreElements</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      <span class="keyword">if</span> (!log4j12Initialized) &#123;</span><br><span class="line">        <span class="keyword">val</span> defaultLogProps = <span class="string">"org/apache/spark/log4j-defaults.properties"</span></span><br><span class="line">        <span class="type">Option</span>(<span class="type">Utils</span>.getSparkClassLoader.getResource(defaultLogProps)) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(url) =&gt;</span><br><span class="line">            <span class="type">PropertyConfigurator</span>.configure(url)</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">s"Using Spark's default log4j profile: <span class="subst">$defaultLogProps</span>"</span>)</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">s"Spark was unable to load <span class="subst">$defaultLogProps</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isInterpreter) &#123;</span><br><span class="line">        <span class="comment">// Use the repl's main class to define the default log level when running the shell,</span></span><br><span class="line">        <span class="comment">// overriding the root logger's config if they're different.</span></span><br><span class="line">        <span class="keyword">val</span> rootLogger = <span class="type">LogManager</span>.getRootLogger()</span><br><span class="line">        <span class="keyword">val</span> replLogger = <span class="type">LogManager</span>.getLogger(logName)</span><br><span class="line">        <span class="keyword">val</span> replLevel = <span class="type">Option</span>(replLogger.getLevel()).getOrElse(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">        <span class="keyword">if</span> (replLevel != rootLogger.getEffectiveLevel()) &#123;</span><br><span class="line">          <span class="type">System</span>.err.printf(<span class="string">"Setting default log level to \"%s\".\n"</span>, replLevel)</span><br><span class="line">          <span class="type">System</span>.err.println(<span class="string">"To adjust logging level use sc.setLogLevel(newLevel). "</span> +</span><br><span class="line">            <span class="string">"For SparkR, use setLogLevel(newLevel)."</span>)</span><br><span class="line">          rootLogger.setLevel(replLevel)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Logging</span>.initialized = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Force a call into slf4j to initialize it. Avoids this happening from multiple threads</span></span><br><span class="line">    <span class="comment">// and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html</span></span><br><span class="line">    log</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initialized = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">val</span> initLock = <span class="keyword">new</span> <span class="type">Object</span>()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// We use reflection here to handle the case where users remove the</span></span><br><span class="line">    <span class="comment">// slf4j-to-jul bridge order to route their logs to JUL.</span></span><br><span class="line">    <span class="keyword">val</span> bridgeClass = <span class="type">Utils</span>.classForName(<span class="string">"org.slf4j.bridge.SLF4JBridgeHandler"</span>)</span><br><span class="line">    bridgeClass.getMethod(<span class="string">"removeHandlersForRootLogger"</span>).invoke(<span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">val</span> installed = bridgeClass.getMethod(<span class="string">"isInstalled"</span>).invoke(<span class="literal">null</span>).asInstanceOf[<span class="type">Boolean</span>]</span><br><span class="line">    <span class="keyword">if</span> (!installed) &#123;</span><br><span class="line">      bridgeClass.getMethod(<span class="string">"install"</span>).invoke(<span class="literal">null</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt; <span class="comment">// can't log anything yet so just fail silently</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;先编写HelloWord-scala文件&quot;&gt;&lt;a href=&quot;#先编写HelloWord-scala文件&quot; class=&quot;headerlink&quot; title=&quot;先编写HelloWord.scala文件&quot;&gt;&lt;/a&gt;先编写HelloWord.scala文件&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
</feed>
