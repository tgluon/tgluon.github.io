<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog</title>
  
  <subtitle>追梦青年</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tgluon.github.io/"/>
  <updated>2019-02-13T09:41:05.450Z</updated>
  <id>https://tgluon.github.io/</id>
  
  <author>
    <name>tang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>java concurrent包类详解</title>
    <link href="https://tgluon.github.io/2019/02/13/java%20concurrent%E5%8C%85%E7%B1%BB%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/02/13/java concurrent包类详解/</id>
    <published>2019-02-13T15:34:37.000Z</published>
    <updated>2019-02-13T09:41:05.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h1><p>原文：<a href="https://zhuanlan.zhihu.com/p/27314456" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27314456</a><br>Semaphore(信号量)是java.util.concurrent下的一个工具类.<strong>用来控制可同时访问特定资源的线程数</strong>.内部是通过维护父类(AQS)的 int state值实现.<br>Semaphore中有一个”许可”的概念:</p><p>访问特定资源前，先使用acquire(1)获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。<br>访问资源后，使用release()释放许可。</p><p>这个许可在构造时传入,赋给state值,它等同于state.</p><p><strong>Semaphore应用场景</strong><br>系统中某类资源比较紧张,只能被有限的线程访问,此时适合使用信号量。<br>Semaphore用来控制访问某资源的线程数,比如数据库连接.假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有20个，这时就必须控制最多只有20个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。<br><strong>使用案例：</strong><br>spark LiveListenerBus类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> val eventLock = <span class="keyword">new</span> Semaphore(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> val listenerThread = <span class="keyword">new</span> Thread(name) &#123;</span><br><span class="line">  setDaemon(<span class="keyword">true</span>)</span><br><span class="line">  <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryOrStopSparkContext(sparkContext) &#123;</span><br><span class="line">    LiveListenerBus.withinListenerThread.withValue(<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        eventLock.acquire()</span><br><span class="line">        self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">          processingEvent = <span class="keyword">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 消费消息</span></span><br><span class="line">          val event = eventQueue.poll</span><br><span class="line">          <span class="keyword">if</span> (event == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Get out of the while loop and shutdown the daemon thread</span></span><br><span class="line">            <span class="keyword">if</span> (!stopped.get) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Polling `null` from eventQueue means"</span> +</span><br><span class="line">                <span class="string">" the listener bus has been stopped. So `stopped` must be true"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">          &#125;</span><br><span class="line">          postToAll(event)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          self.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">            processingEvent = <span class="keyword">false</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Semaphore属于一种较常见的限流手段，Google Guava封装了一层。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//JDK API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> Semaphore semaphore = <span class="keyword">new</span> Semaphore(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        semaphore.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">        semaphore.release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Google Guava API：流速控制在每秒执行100个任务</span></span><br><span class="line"><span class="keyword">final</span> RateLimiter rateLimiter = RateLimiter.create(<span class="number">100</span>);</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">submitTasks</span><span class="params">(List&lt;Runnable&gt; tasks, Executor executor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Runnable task : tasks) &#123;</span><br><span class="line">        rateLimiter.acquire(); <span class="comment">// 也许需要等待</span></span><br><span class="line">        executor.execute(task);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Semaphore&quot;&gt;&lt;a href=&quot;#Semaphore&quot; class=&quot;headerlink&quot; title=&quot;Semaphore&quot;&gt;&lt;/a&gt;Semaphore&lt;/h1&gt;&lt;p&gt;原文：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/2
      
    
    </summary>
    
      <category term="java" scheme="https://tgluon.github.io/categories/java/"/>
    
    
      <category term="java" scheme="https://tgluon.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>scala学习笔记</title>
    <link href="https://tgluon.github.io/2019/01/09/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tgluon.github.io/2019/01/09/scala学习笔记/</id>
    <published>2019-01-09T15:34:37.000Z</published>
    <updated>2019-01-09T03:40:26.008Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高级函数"><a href="#高级函数" class="headerlink" title="高级函数"></a>高级函数</h1><p>高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。    </p><h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x: <span class="type">Int</span>) =&gt; x * <span class="number">3</span></span><br></pre></td></tr></table></figure><h2 id="带函数参数的函数"><a href="#带函数参数的函数" class="headerlink" title="带函数参数的函数"></a>带函数参数的函数</h2><p>Scala集合类（collections）的高阶函数map。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">B</span>, <span class="type">That</span>](f: <span class="type">A</span> =&gt; <span class="type">B</span>)(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span> = &#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">builder</span> </span>= &#123;</span><br><span class="line">     <span class="keyword">val</span> b = bf(repr)</span><br><span class="line">     b.sizeHint(<span class="keyword">this</span>)</span><br><span class="line">     b</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">val</span> b = builder</span><br><span class="line">   <span class="keyword">for</span> (x &lt;- <span class="keyword">this</span>) b += f(x)</span><br><span class="line">   b.result</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">doubleSalary</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">     x * <span class="number">2</span></span><br><span class="line">   &#125;</span><br><span class="line">   seq.map(doubleSalary).foreach(x =&gt; println(x))</span><br><span class="line">   seq.map(x =&gt; x * <span class="number">2</span>)</span><br><span class="line">   seq.map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line"><span class="keyword">val</span> first = sum(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> second = first(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h1 id="嵌套方法"><a href="#嵌套方法" class="headerlink" title="嵌套方法"></a>嵌套方法</h1><p>在Scala中可以嵌套定义方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fact</span></span>(x: <span class="type">Int</span>, accumulator: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (x &lt;= <span class="number">1</span>) accumulator</span><br><span class="line">      <span class="keyword">else</span> fact(x - <span class="number">1</span>, x * accumulator)</span><br><span class="line">    &#125;  </span><br><span class="line">    fact(x, <span class="number">1</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> println(<span class="string">"Factorial of 2: "</span> + factorial(<span class="number">2</span>))</span><br><span class="line"> println(<span class="string">"Factorial of 3: "</span> + factorial(<span class="number">3</span>))</span><br></pre></td></tr></table></figure></p><h1 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h1><p>柯里化：指将原来接受两个参数的函数变成新的接受一个参数的过程。<br><strong>接受两个参数的过程：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x:<span class="type">Int</span> , y: <span class="type">Int</span> )= x * y</span><br></pre></td></tr></table></figure></p><p><strong>接受一个参数的过程</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>scala支持简写成如下的柯里化：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line"></span><br><span class="line">mul(<span class="number">1</span>)(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>在Scala集合中定义的特质TraversableOnce[+A]。<br>Traversable： 能横过的；能越过的；可否定的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> result = z</span><br><span class="line">    <span class="keyword">this</span> foreach (x =&gt; result = op(result, x))</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br><span class="line">```   </span><br><span class="line">foldLeft从左到右，以此将一个二元运算op应用到初始值z和该迭代器（traversable)的所有元素上。以下是该函数的一个用例：</span><br><span class="line"></span><br><span class="line">从初值<span class="number">0</span>开始, 这里 foldLeft 将函数 (m, n) =&gt; m + n 依次应用到列表中的每一个元素和之前累积的值上。  </span><br><span class="line">```scala</span><br><span class="line"><span class="keyword">val</span> numbers = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> res = numbers.foldLeft(<span class="number">0</span>)((m, n) =&gt; m + n)</span><br><span class="line"><span class="keyword">val</span> res2 = numbers.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">println(res) <span class="comment">// 55</span></span><br><span class="line">pringln(res2)</span><br></pre></td></tr></table></figure></p><p>多参数列表有更复杂的调用语法，因此应该谨慎使用。<br><strong>建议的使用场景包括:</strong>   </p><ol><li><p>单一的函数参数。<br>在某些情况下存在单一的函数参数时，例如上述例子foldLeft中的op，多参数列表可以使得传递匿名函数作为参数的语法更为简洁。如果不使用多参数列表，代码可能像这样：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numbers.foldLeft(<span class="number">0</span>, &#123;(m: <span class="type">Int</span>, n: <span class="type">Int</span>) =&gt; m + n&#125;)</span><br></pre></td></tr></table></figure></li><li><p>隐式（IMPLICIT）参数。     </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(arg: <span class="type">Int</span>)(<span class="keyword">implicit</span> ec: <span class="type">ExecutionContext</span>) = ???</span><br></pre></td></tr></table></figure></li></ol><h1 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h1><p>match对应java里的Switch,不过它写在选择器表达式之后。 </p><p>选择器 match {备选项}<br>取代了：<br>switch(选择器){备选项}  </p><h2 id="match与switch的比较"><a href="#match与switch的比较" class="headerlink" title="match与switch的比较"></a>match与switch的比较</h2><p>匹配表达式可以被看做java风格switch的泛化。<br>match的不同：<br>1、match是scala表达式，也就是说，它始终以值作为结果;<br>2、 scala的备选项表达式永远不会”掉到”下一个case;<br>3、 如果没有模式匹配，MatchErro异常会被抛出。</p><h1 id="提取器对象"><a href="#提取器对象" class="headerlink" title="提取器对象"></a>提取器对象</h1><p>提取器对象是一个包含有 unapply 方法的单例对象。apply 方法就像一个构造器，接受参数然后创建一个实例对象，反之 unapply 方法接受一个实例对象然后返回最初创建它所用的参数。提取器常用在模式匹配和偏函数中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"> <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// 调用工厂构造方法，构造出对象实例</span></span><br><span class="line">   <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"Spark"</span>, <span class="number">6</span>)</span><br><span class="line">   <span class="comment">// 这种写法居然可以正常编译</span></span><br><span class="line">   <span class="keyword">val</span> <span class="type">Person</span>(name, age) = person</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 使用了提取器，</span></span><br><span class="line"><span class="comment">    * 调用了unapply方法，把实例person中的name和age提取出来赋值给了Person类</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(name + <span class="string">" : "</span> + age) <span class="comment">//正常输出: Spark 6</span></span><br><span class="line">   </span><br><span class="line">   person <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">     <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><p><strong>自定义unapply方法</strong><br>主要用于模式匹配中。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val salary: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>, salary: <span class="type">Int</span>):<span class="type">Person</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Person</span>(name, salary)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(money: <span class="type">Person</span>): <span class="type">Option</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    <span class="keyword">if</span>(money == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Some</span>(money.name, money.salary)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="type">Person</span>(<span class="string">"spark"</span>, <span class="number">800</span>);</span><br><span class="line">     person <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// match过程就是调用提取器的过程</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Person</span>(name, age) =&gt; println(<span class="string">"Wow, "</span> + name + <span class="string">" : "</span> + age)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="for-表达式"><a href="#for-表达式" class="headerlink" title="for 表达式"></a>for 表达式</h1><p>Scala 提供一个轻量级的标记方式用来表示 序列推导。推导使用形式为 for (enumerators) yield e 的 for 表达式，此处 enumerators 指一组以分号分隔的枚举器。一个 enumerator 要么是一个产生新变量的生成器，要么是一个过滤器。for 表达式在枚举器产生的每一次绑定中都会计算 e 值，并在循环结束后返回这些值组成的序列。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">userBase</span> </span>= <span class="type">List</span>(<span class="type">User</span>(<span class="string">"Travis"</span>, <span class="number">28</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Kelly"</span>, <span class="number">33</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Jennifer"</span>, <span class="number">44</span>),</span><br><span class="line">  <span class="type">User</span>(<span class="string">"Dennis"</span>, <span class="number">23</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> twentySomethings = <span class="keyword">for</span> (user &lt;- userBase <span class="keyword">if</span> (user.age &gt;=<span class="number">20</span> &amp;&amp; user.age &lt; <span class="number">30</span>))</span><br><span class="line">  <span class="keyword">yield</span> user.name  </span><br><span class="line"></span><br><span class="line">twentySomethings.foreach(name =&gt; println(name))</span><br></pre></td></tr></table></figure></p><p>这里 for 循环后面使用的 yield 语句实际上会创建一个 List。因为当我们说 yield user.name 的时候，它实际上是一个 List[String]。 user <- userbase="" 是生成器，if="" (user.age="">=20 &amp;&amp; user.age &lt; 30) 是过滤器用来过滤掉那些年龄不是20多岁的人。</-></p><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><p>泛型类指可以接受类型参数的类。泛型类在集合类中被广泛使用。<br>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。  </p><h2 id="定义一个泛型类"><a href="#定义一个泛型类" class="headerlink" title="定义一个泛型类"></a>定义一个泛型类</h2><p>泛型类使用方括号 [] 来接受类型参数。一个惯例是使用字母 A 作为参数标识符，当然你可以使用任何参数名称。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> elements: <span class="type">List</span>[<span class="type">A</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">push</span></span>(x: <span class="type">A</span>) &#123; elements = x :: elements &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">peek</span></span>: <span class="type">A</span> = elements.head</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pop</span></span>(): <span class="type">A</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> currentTop = peek</span><br><span class="line">    elements = elements.tail</span><br><span class="line">    currentTop</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面的 Stack 类的实现中接受类型参数 A。 这表示其内部的列表，var elements: List[A] = Nil，只能够存储类型 A 的元素。方法 def push 只接受类型 A 的实例对象作为参数(注意：elements = x :: elements 将 elements 放到了一个将元素 x 添加到 elements 的头部而生成的新列表中)。   </p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>要使用一个泛型类，将一个具体类型放到方括号中来代替 A。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stack = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">Int</span>]</span><br><span class="line">stack.push(<span class="number">1</span>)</span><br><span class="line">stack.push(<span class="number">2</span>)</span><br><span class="line">println(stack.pop)  <span class="comment">// prints 2</span></span><br><span class="line">println(stack.pop)  <span class="comment">// prints 1</span></span><br></pre></td></tr></table></figure></p><h1 id="型变"><a href="#型变" class="headerlink" title="型变"></a>型变</h1><p>型变是复杂类型的子类型关系与其组件类型的子类型关系的相关性。 Scala支持泛型类的类型参数的型变注释，允许它们是协变的，逆变的，或在没有使用注释的情况下是不变的。在类型系统中使用型变允许我们在复杂类型之间建立直观的连接，而缺乏型变则会限制类抽象的重用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>[+<span class="type">A</span>] <span class="title">//</span> <span class="title">一个协变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Bar</span>[-<span class="type">A</span>] <span class="title">//</span> <span class="title">一个逆变类</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Baz</span>[<span class="type">A</span>]  <span class="title">//</span> <span class="title">一个不变类</span></span></span><br></pre></td></tr></table></figure></p><h2 id="协变"><a href="#协变" class="headerlink" title="协变"></a>协变</h2><p>使用注释 +A，可以使一个泛型类的类型参数 A 成为协变。 对于某些类 class List[+A]，使 A 成为协变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 List[A] 就是 List[B] 的子类型。 这允许我们使用泛型来创建非常有用和直观的子类型关系。   </p><p>考虑以下简单的类结构：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Dog</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span></span></span><br></pre></td></tr></table></figure></p><p>类型 Cat 和 Dog 都是 Animal 的子类型。 Scala 标准库有一个通用的不可变的类 sealed abstract class List[+A]，其中类型参数 A 是协变的。 这意味着 List[Cat] 是 List[Animal]，List[Dog] 也是 List[Animal]。 直观地说，猫的列表和狗的列表都是动物的列表是合理的，你应该能够用它们中的任何一个替换 List[Animal]。</p><p>在下例中，方法 printAnimalNames 将接受动物列表作为参数，并且逐行打印出它们的名称。 如果 List[A] 不是协变的，最后两个方法调用将不能编译，这将严重限制 printAnimalNames 方法的适用性。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CovarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printAnimalNames</span></span>(animals: <span class="type">List</span>[<span class="type">Animal</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    animals.foreach &#123; animal =&gt;</span><br><span class="line">      println(animal.name)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> cats: <span class="type">List</span>[<span class="type">Cat</span>] = <span class="type">List</span>(<span class="type">Cat</span>(<span class="string">"Whiskers"</span>), <span class="type">Cat</span>(<span class="string">"Tom"</span>))</span><br><span class="line">  <span class="keyword">val</span> dogs: <span class="type">List</span>[<span class="type">Dog</span>] = <span class="type">List</span>(<span class="type">Dog</span>(<span class="string">"Fido"</span>), <span class="type">Dog</span>(<span class="string">"Rex"</span>))</span><br><span class="line"></span><br><span class="line">  printAnimalNames(cats)</span><br><span class="line">  <span class="comment">// Whiskers</span></span><br><span class="line">  <span class="comment">// Tom</span></span><br><span class="line">  printAnimalNames(dogs)</span><br><span class="line">  <span class="comment">// Fido</span></span><br><span class="line">  <span class="comment">// Rex</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="逆变"><a href="#逆变" class="headerlink" title="逆变"></a>逆变</h2><p>通过使用注释 -A，可以使一个泛型类的类型参数 A 成为逆变。 与协变类似，这会在类及其类型参数之间创建一个子类型关系，但其作用与协变完全相反。 也就是说，对于某个类 class Writer[-A] ，使 A 逆变意味着对于两种类型 A 和 B，如果 A 是 B 的子类型，那么 Writer[B] 是 Writer[A] 的子类型。   </p><p>考虑在下例中使用上面定义的类 Cat，Dog 和 Animal ：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Printer</span>[-<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里 Printer[A] 是一个简单的类，用来打印出某种类型的 A。 让我们定义一些特定的子类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnimalPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Animal</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(animal: <span class="type">Animal</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The animal's name is: "</span> + animal.name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CatPrinter</span> <span class="keyword">extends</span> <span class="title">Printer</span>[<span class="type">Cat</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(cat: <span class="type">Cat</span>): <span class="type">Unit</span> =</span><br><span class="line">    println(<span class="string">"The cat's name is: "</span> + cat.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>如果 Printer[Cat] 知道如何在控制台打印出任意 Cat，并且 Printer[Animal] 知道如何在控制台打印出任意 Animal，那么 Printer[Animal] 也应该知道如何打印出 Cat 就是合理的。 反向关系不适用，因为 Printer[Cat] 并不知道如何在控制台打印出任意 Animal。 因此，如果我们愿意，我们应该能够用 Printer[Animal] 替换 Printer[Cat]，而使 Printer[A] 逆变允许我们做到这一点。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ContravarianceTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> myCat: <span class="type">Cat</span> = <span class="type">Cat</span>(<span class="string">"Boots"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printMyCat</span></span>(printer: <span class="type">Printer</span>[<span class="type">Cat</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printer.print(myCat)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> catPrinter: <span class="type">Printer</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">CatPrinter</span></span><br><span class="line">  <span class="keyword">val</span> animalPrinter: <span class="type">Printer</span>[<span class="type">Animal</span>] = <span class="keyword">new</span> <span class="type">AnimalPrinter</span></span><br><span class="line"></span><br><span class="line">  printMyCat(catPrinter)</span><br><span class="line">  printMyCat(animalPrinter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个程序的输出如下：   </p><blockquote><p>The cat’s name is: Boots<br>The animal’s name is: Boots    </p></blockquote><h1 id="不变"><a href="#不变" class="headerlink" title="不变"></a>不变</h1><p>默认情况下，Scala中的泛型类是不变的。 这意味着它们既不是协变的也不是逆变的。 在下例中，类 Container 是不变的。 Container[Cat] 不是 Container[Animal]，反之亦然。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Container</span>[<span class="type">A</span>](<span class="params">value: <span class="type">A</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _value: <span class="type">A</span> = value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">A</span> = _value</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setValue</span></span>(value: <span class="type">A</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _value = value</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可能看起来一个 Container[Cat] 自然也应该是一个 Container[Animal]，但允许一个可变的泛型类成为协变并不安全。 在这个例子中，Container 是不变的非常重要。 假设 Container 实际上是协变的，下面的情况可能会发生：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> catContainer: <span class="type">Container</span>[<span class="type">Cat</span>] = <span class="keyword">new</span> <span class="type">Container</span>(<span class="type">Cat</span>(<span class="string">"Felix"</span>))</span><br><span class="line"><span class="keyword">val</span> animalContainer: <span class="type">Container</span>[<span class="type">Animal</span>] = catContainer</span><br><span class="line">animalContainer.setValue(<span class="type">Dog</span>(<span class="string">"Spot"</span>))</span><br><span class="line"><span class="keyword">val</span> cat: <span class="type">Cat</span> = catContainer.getValue</span><br></pre></td></tr></table></figure></p><p>  糟糕，我们最终会将一只狗作为值分配给一只猫<br>幸运的是，编译器在此之前就会阻止我们。</p><h2 id="上界"><a href="#上界" class="headerlink" title="上界"></a>上界</h2><p>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br>在Scala中，类型参数和抽象类型都可以有一个类型边界约束。这种类型边界在限制类型变量实际取值的同时还能展露类型成员的更多信息。比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A的子类。下面的例子展示了类PetContainer的一个类型参数的类型上界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Pet</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Cat"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Pet</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Dog"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lion</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">"Lion"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PetContainer</span>[<span class="type">P</span> &lt;: <span class="type">Pet</span>](<span class="params">p: <span class="type">P</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pet</span></span>: <span class="type">P</span> = p</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dogContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Dog</span>](<span class="keyword">new</span> <span class="type">Dog</span>)</span><br><span class="line"><span class="keyword">val</span> catContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Cat</span>](<span class="keyword">new</span> <span class="type">Cat</span>)</span><br><span class="line"><span class="comment">// this would not compile</span></span><br><span class="line"><span class="keyword">val</span> lionContainer = <span class="keyword">new</span> <span class="type">PetContainer</span>[<span class="type">Lion</span>](<span class="keyword">new</span> <span class="type">Lion</span>)</span><br></pre></td></tr></table></figure></p><p>类PetContainer接受一个必须是Pet子类的类型参数P。因为Dog和Cat都是Pet的子类，所以可以构造PetContainer[Dog]和PetContainer[Cat]。但在尝试构造PetContainer[Lion]的时候会得到下面的错误信息：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">arguments</span> [<span class="type">Lion</span>] <span class="title">do</span> <span class="title">not</span> <span class="title">conform</span> <span class="title">to</span> <span class="title">class</span> <span class="title">PetContainer</span>'<span class="title">s</span> <span class="title">type</span> <span class="title">parameter</span> <span class="title">bounds</span> [<span class="type">P</span> &lt;: <span class="type">Pet</span>]</span></span><br></pre></td></tr></table></figure></p><p>这是因为Lion并不是Pet的子类。   </p><h2 id="下界"><a href="#下界" class="headerlink" title="下界"></a>下界</h2><p>类型上界 将类型限制为另一种类型的子类型，而 类型下界 将类型声明为另一种类型的超类型。 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。  </p><p>下面看一个适合用类型下界的例子：<br>下面看一个适合用类型下界的例子：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">Node</span>[<span class="type">B</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>(elem: <span class="type">B</span>): <span class="type">ListNode</span>[<span class="type">B</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该程序实现了一个单链表。 Nil 表示空元素（即空列表）。 class ListNode 是一个节点，它包含一个类型为 B (head) 的元素和一个对列表其余部分的引用 (tail)。 class Node 及其子类型是协变的，因为我们定义了 +B。</p><p>但是，这个程序 不能 编译，因为方法 prepend 中的参数 elem 是协变的 B 类型。 这会出错，因为函数的参数类型是逆变的，而返回类型是协变的。</p><p>要解决这个问题，我们需要将方法 prepend 的参数 elem 的型变翻转。 我们通过引入一个新的类型参数 U 来实现这一点，该参数具有 B 作为类型下界。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Node</span>[+<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">Node</span>[<span class="type">U</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>[+<span class="type">B</span>](<span class="params">h: <span class="type">B</span>, t: <span class="type">Node</span>[<span class="type">B</span>]</span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">B</span> = h</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Node</span>[<span class="type">B</span>] = t</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Nil</span>[+<span class="type">B</span>](<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Node</span>[<span class="type">B</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">prepend</span></span>[<span class="type">U</span> &gt;: <span class="type">B</span>](elem: <span class="type">U</span>): <span class="type">ListNode</span>[<span class="type">U</span>] = <span class="type">ListNode</span>(elem, <span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>现在我们像下面这么做：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">AfricanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">EuropeanSwallow</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Bird</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">africanSwallowList=</span> <span class="title">ListNode</span>[<span class="type">AfricanSwallow</span>](<span class="params"><span class="type">AfricanSwallow</span>(</span>), <span class="title">Nil</span>(<span class="params"></span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">birdList</span></span>: <span class="type">Node</span>[<span class="type">Bird</span>] = africanSwallowList</span><br><span class="line">birdList.prepend(<span class="keyword">new</span> <span class="type">EuropeanSwallow</span>)</span><br></pre></td></tr></table></figure></p><p>可以为 Node[Bird] 赋值 africanSwallowList，然后再加入一个 EuropeanSwallow。</p><h1 id="抽象类型"><a href="#抽象类型" class="headerlink" title="抽象类型"></a>抽象类型</h1><p>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">T</span></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">element</span></span>: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。<br>特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span> <span class="keyword">extends</span> <span class="title">Buffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span></span></span><br><span class="line"><span class="class">  <span class="title">type</span> <span class="title">T</span> <span class="title">&lt;</span></span>: <span class="type">Seq</span>[<span class="type">U</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。</p><p>含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSeqBuffer</span> <span class="keyword">extends</span> <span class="title">SeqBuffer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">U</span> </span>= <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(elem1: <span class="type">Int</span>, elem2: <span class="type">Int</span>): <span class="type">IntSeqBuffer</span> =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">IntSeqBuffer</span> &#123;</span><br><span class="line">       <span class="class"><span class="keyword">type</span> <span class="title">T</span> </span>= <span class="type">List</span>[<span class="type">U</span>]</span><br><span class="line">       <span class="keyword">val</span> element = <span class="type">List</span>(elem1, elem2)</span><br><span class="line">     &#125;</span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p><p>这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。</p><p>把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span>[+<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> element: <span class="type">T</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SeqBuffer</span>[<span class="type">U</span>, +<span class="type">T</span> &lt;: <span class="type">Seq</span>[<span class="type">U</span>]] <span class="keyword">extends</span> <span class="title">Buffer</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">length</span> </span>= element.length</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newIntSeqBuf</span></span>(e1: <span class="type">Int</span>, e2: <span class="type">Int</span>): <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">SeqBuffer</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]] &#123;</span><br><span class="line">    <span class="keyword">val</span> element = <span class="type">List</span>(e1, e2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> buf = newIntSeqBuf(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">println(<span class="string">"length = "</span> + buf.length)</span><br><span class="line">println(<span class="string">"content = "</span> + buf.element)</span><br></pre></td></tr></table></figure></p><p>需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。</p><h1 id="复合类型"><a href="#复合类型" class="headerlink" title="复合类型"></a>复合类型</h1><p><strong>需求</strong>:<br>有时需要表明一个对象的类型是其他几种类型的子类型。 在 Scala 中，这可以表示成 复合类型，即多个类型的交集。</p><p><strong>案例</strong>：<br>假设我们有两个特质 Cloneable 和 Resetable：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Cloneable</span> <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Cloneable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clone</span></span>(): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.clone().asInstanceOf[<span class="type">Cloneable</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Resetable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>: <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>现在假设我们要编写一个方法 cloneAndReset，此方法接受一个对象，克隆它并重置原始对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: ?): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> cloned = obj.clone()</span><br><span class="line">  obj.reset</span><br><span class="line">  cloned</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里出现一个问题，参数 obj 的类型是什么。 如果类型是 Cloneable 那么参数对象可以被克隆 clone，但不能重置 reset; 如果类型是 Resetable 我们可以重置 reset 它，但却没有克隆 clone 操作。 为了避免在这种情况下进行类型转换，我们可以将 obj 的类型同时指定为 Cloneable 和 Resetable。 这种复合类型在 Scala 中写成：Cloneable with Resetable。</p><p>以下是更新后的方法：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cloneAndReset</span></span>(obj: <span class="type">Cloneable</span> <span class="keyword">with</span> <span class="type">Resetable</span>): <span class="type">Cloneable</span> = &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>复合类型可以由多个对象类型构成，这些对象类型可以有单个细化，用于缩短已有对象成员的签名。 格式为：A with B with C … { refinement }</p><p>关于使用细化的例子参考 通过混入（mixin）来组合类。</p><h1 id="自类型"><a href="#自类型" class="headerlink" title="自类型"></a>自类型</h1><p>自类型用于声明一个特质必须混入其他特质，尽管该特质没有直接扩展其他特质。 这使得所依赖的成员可以在没有导入的情况下使用。</p><p>自类型是一种细化 this 或 this 别名之类型的方法。 语法看起来像普通函数语法，但是意义完全不一样。</p><p>要在特质中使用自类型，写一个标识符，跟上要混入的另一个特质，以及 =&gt;（例如 someIdentifier: SomeOtherTrait =&gt;）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">username</span></span>: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Tweeter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>: <span class="type">User</span> =&gt;  <span class="comment">// 重新赋予 this 的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tweet</span></span>(tweetText: <span class="type">String</span>) = println(<span class="string">s"<span class="subst">$username</span>: <span class="subst">$tweetText</span>"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VerifiedTweeter</span>(<span class="params">val username_ : <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Tweeter</span> <span class="keyword">with</span> <span class="title">User</span> </span>&#123;  <span class="comment">// 我们混入特质 User 因为 Tweeter 需要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">username</span> </span>= <span class="string">s"real <span class="subst">$username_</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> realBeyoncé = <span class="keyword">new</span> <span class="type">VerifiedTweeter</span>(<span class="string">"Beyoncé"</span>)</span><br><span class="line">realBeyoncé.tweet(<span class="string">"Just spilled my glass of lemonade"</span>)  <span class="comment">// 打印出 "real Beyoncé: Just spilled my glass of lemonade"</span></span><br></pre></td></tr></table></figure></p><p>因为我们在特质 trait Tweeter 中定义了 this: User =&gt;，现在变量 username 可以在 tweet 方法内使用。 这也意味着，由于 VerifiedTweeter 继承了 Tweeter，它还必须混入 User（使用 with User）。<br><strong>自类型别名</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  self =&gt;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(num1: <span class="type">Int</span>, num2: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    num1 + num2</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(self.sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h1><p><strong>定义</strong>：指的是那种以implicit关键字声明的带有单个参数的函数。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="给File类增加read方法"><a href="#给File类增加read方法" class="headerlink" title="给File类增加read方法"></a>给File类增加read方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">val f: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>() = <span class="type">Source</span>.fromFile(f).mkString</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFilePredef</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">fileToRichFile</span></span>(f: <span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile</span>(f)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>使用</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"><span class="keyword">import</span> com.tm.scala.implic.<span class="type">RichFilePredef</span>._</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFile</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">     <span class="keyword">val</span> f = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"/home/hadoop/sample_movielens_data.txt"</span>)</span><br><span class="line">    <span class="comment">// 注意：要使用隐式转换，必须在当前object之前导入隐式转换的门面object</span></span><br><span class="line">    <span class="keyword">val</span> contents = f.read()</span><br><span class="line">    println(contents)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="排序中的使用案例"><a href="#排序中的使用案例" class="headerlink" title="排序中的使用案例"></a>排序中的使用案例</h3><h4 id="Ordered方式"><a href="#Ordered方式" class="headerlink" title="Ordered方式"></a>Ordered方式</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Girl</span>(<span class="params">val name: <span class="type">String</span>, var faceValue: <span class="type">Int</span>, var age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p><strong>定义比较规则</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 视图定界</span></span><br><span class="line"><span class="comment">  * 使用视图定界,视图定界其实就是隐式转换,将T转换成Ordered</span></span><br><span class="line"><span class="comment">  * 有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。</span></span><br><span class="line"><span class="comment">  * 一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。</span></span><br><span class="line"><span class="comment">  * 更多知识：https://twitter.github.io/scala_school/zh_cn/advanced-types.html</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderedChooser</span>[<span class="type">T</span> &lt;% <span class="type">Ordered</span>[<span class="type">T</span>]] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (first &gt; second) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderedPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 方式一</span></span><br><span class="line"> <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">gilrToOrdered</span></span>(girl: <span class="type">Girl</span>):<span class="type">Ordered</span>[<span class="type">Girl</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">            girl.age - that.age</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            girl.faceValue - that.faceValue</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdered = (girl: <span class="type">Girl</span>) =&gt; <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (girl.faceValue == that.faceValue) &#123;</span><br><span class="line">        girl.age - that.age</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        girl.faceValue - that.faceValue</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdered</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> girl1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"spark"</span>, <span class="number">100</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> girl2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"mxnet"</span>, <span class="number">90</span>,<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderedPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> chooser = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = chooser.choose(girl1, girl2)</span><br><span class="line">    println(g.faceValue)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="Ordering方式"><a href="#Ordering方式" class="headerlink" title="Ordering方式"></a>Ordering方式</h4><p><strong>定义比较规则</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 文本定界</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderingChoose</span>[<span class="type">T</span>: <span class="type">Ordering</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(first: <span class="type">T</span>, second: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ord = implicitly[<span class="type">Ordering</span>[<span class="type">T</span>]]</span><br><span class="line">    <span class="keyword">if</span> (ord.gt(first, second)) first <span class="keyword">else</span> second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>门面类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderingPredef</span> </span>&#123;</span><br><span class="line">  <span class="comment">//  方式一</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> gilrToOrdering = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">Girl</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//  方式二</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">GilrToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 方式三</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 参考：Ordering中的下面方法</span></span><br><span class="line"><span class="comment">    * trait IntOrdering extends Ordering[Int] &#123;</span></span><br><span class="line"><span class="comment">    * def compare(x: Int, y: Int) =</span></span><br><span class="line"><span class="comment">    * if (x &lt; y) -1</span></span><br><span class="line"><span class="comment">    * else if (x == y) 0</span></span><br><span class="line"><span class="comment">    * else 1</span></span><br><span class="line"><span class="comment">    * &#125;</span></span><br><span class="line"><span class="comment">    * implicit object Int extends IntOrdering</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">trait</span> <span class="title">GirlToOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Girl</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Girl</span>, y: <span class="type">Girl</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      x.faceValue - y.faceValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">Girl</span> <span class="keyword">extends</span> <span class="title">GirlToOrdering</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure></p><p><strong>测试类</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestOrdering</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> g1 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"zhangsan"</span>, <span class="number">50</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">val</span> g2 = <span class="keyword">new</span> <span class="type">Girl</span>(<span class="string">"lisi"</span>, <span class="number">500</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">import</span> <span class="type">OrderingPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> choose = <span class="keyword">new</span> <span class="type">OrderingChoose</span>[<span class="type">Girl</span>]</span><br><span class="line">    <span class="keyword">val</span> g = choose.choose(g1, g2)</span><br><span class="line">    println(g.name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;高级函数&quot;&gt;&lt;a href=&quot;#高级函数&quot; class=&quot;headerlink&quot; title=&quot;高级函数&quot;&gt;&lt;/a&gt;高级函数&lt;/h1&gt;&lt;p&gt;高阶函数是指使用其他函数作为参数、或者返回一个函数作为结果的函数。    &lt;/p&gt;
&lt;h2 id=&quot;匿名函数&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala特殊符号使用</title>
    <link href="https://tgluon.github.io/2019/01/04/scala%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E4%BD%BF%E7%94%A8/"/>
    <id>https://tgluon.github.io/2019/01/04/scala特殊符号使用/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-01-09T03:40:26.019Z</updated>
    
    <content type="html"><![CDATA[<h1 id="符号"><a href="#符号" class="headerlink" title="_*符号"></a>_*符号</h1><p>作用：它告诉编译器将序列类型的单个参数视为可变参数序列。<br>举例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">ProcessBuilder</span>(<span class="type">String</span>... command) &#123;</span><br><span class="line">       <span class="keyword">this</span>.command = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(command.length);</span><br><span class="line">       <span class="keyword">for</span> (<span class="type">String</span> arg : command)</span><br><span class="line">           <span class="keyword">this</span>.command.add(arg);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> commandSeq = <span class="type">Seq</span>(command.mainClass) ++ command.arguments</span><br><span class="line"><span class="keyword">val</span> builder = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(commandSeq: _*)</span><br></pre></td></tr></table></figure><h1 id=""><a href="#" class="headerlink" title="::"></a>::</h1><p>该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    args.toList <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> workerUrl :: userJar :: mainClass :: extraArgs =&gt;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">"Driver"</span>,</span><br><span class="line">          <span class="type">Utils</span>.localHostName(), <span class="number">0</span>, conf, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf))</span><br><span class="line">        rpcEnv.setupEndpoint(<span class="string">"workerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(rpcEnv, workerUrl))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> currentLoader = <span class="type">Thread</span>.currentThread.getContextClassLoader</span><br><span class="line">        <span class="keyword">val</span> userJarUrl = <span class="keyword">new</span> <span class="type">File</span>(userJar).toURI().toURL()</span><br><span class="line">        <span class="keyword">val</span> loader =</span><br><span class="line">          <span class="keyword">if</span> (sys.props.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delegate to supplied main class</span></span><br><span class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(mainClass)</span><br><span class="line">        <span class="keyword">val</span> mainMethod = clazz.getMethod(<span class="string">"main"</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line">        mainMethod.invoke(<span class="literal">null</span>, extraArgs.toArray[<span class="type">String</span>])</span><br><span class="line"></span><br><span class="line">        rpcEnv.shutdown()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="comment">// scalastyle:off println</span></span><br><span class="line">        <span class="type">System</span>.err.println(<span class="string">"Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]"</span>)</span><br><span class="line">        <span class="comment">// scalastyle:on println</span></span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;符号&quot;&gt;&lt;a href=&quot;#符号&quot; class=&quot;headerlink&quot; title=&quot;_*符号&quot;&gt;&lt;/a&gt;_*符号&lt;/h1&gt;&lt;p&gt;作用：它告诉编译器将序列类型的单个参数视为可变参数序列。&lt;br&gt;举例：&lt;br&gt;&lt;figure class=&quot;highlight sc
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala自定注解</title>
    <link href="https://tgluon.github.io/2019/01/04/scala%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/01/04/scala自定义注解/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-01-09T05:25:45.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么要注解"><a href="#为什么要注解" class="headerlink" title="为什么要注解"></a>为什么要注解</h1><p>除了编译和允许之外，还可以对程序做：<br>1、使用Scaladoc 自动产生文档;<br>2、漂亮的打印印出符合你偏爱分割的代码;<br>3、代码常见错误检查，如：打开了文件却没(在全部逻辑分支中)关闭。<br>4、实验类型检查，例如副作用管理或所有权属性确认。<br>这类工具（程序）被称为元编程工具，<code>它们把其它程序当做输入程序</code>。  </p><h1 id="scala注解所在包和基本语法格式"><a href="#scala注解所在包和基本语法格式" class="headerlink" title="scala注解所在包和基本语法格式"></a>scala注解所在包和基本语法格式</h1><p>注解所在包： 标准库定义的注解相关内容在包scala.annotation中。<br>基本语法： @注解名称(注解参数)  </p><h1 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h1><p>自定义注解需要从注解特质继承，scala提供两种注解：<br> 1、基本语法： @注解名称(注解参数)<br> scala中的自定义注解不是接口/特质，而是类。<br>自定义注解需要从注解特质中继承，Scala中提供了两类注解特质：</p><p>scala.annotation.ClassfileAnnotation 由Java编译器生成注解<br>scala.annotation.StaticAnnotation 由Scala编译器生成注解</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;为什么要注解&quot;&gt;&lt;a href=&quot;#为什么要注解&quot; class=&quot;headerlink&quot; title=&quot;为什么要注解&quot;&gt;&lt;/a&gt;为什么要注解&lt;/h1&gt;&lt;p&gt;除了编译和允许之外，还可以对程序做：&lt;br&gt;1、使用Scaladoc 自动产生文档;&lt;br&gt;2、漂亮的打印印
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>scala通过反射获取当前类类名</title>
    <link href="https://tgluon.github.io/2019/01/04/scala%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E7%B1%BB%E7%B1%BB%E5%90%8D/"/>
    <id>https://tgluon.github.io/2019/01/04/scala通过反射获取当前类类名/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-01-04T12:38:21.814Z</updated>
    
    <content type="html"><![CDATA[<h1 id="先编写HelloWord-scala文件"><a href="#先编写HelloWord-scala文件" class="headerlink" title="先编写HelloWord.scala文件"></a>先编写HelloWord.scala文件</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 获取当前类类名</span></span><br><span class="line">    println(<span class="keyword">this</span>.getClass.getName.stripSuffix(<span class="string">"$"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令行使用scalac HelloWorld.scala编译后产生两个文件分别为HelloWorld.class和HelloWorld$.class </p><h1 id="spark源码中的案例"><a href="#spark源码中的案例" class="headerlink" title="spark源码中的案例"></a>spark源码中的案例</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"> * this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"> * (the "License"); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"> * the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.spark.internal</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">LogManager</span>, <span class="type">PropertyConfigurator</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.&#123;<span class="type">Logger</span>, <span class="type">LoggerFactory</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.impl.<span class="type">StaticLoggerBinder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">Utils</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows</span></span><br><span class="line"><span class="comment"> * logging messages at different levels using methods that only evaluate parameters lazily if the</span></span><br><span class="line"><span class="comment"> * log level is enabled.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make the log field transient so that objects with Logging can</span></span><br><span class="line">  <span class="comment">// be serialized and used on another machine</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> log_ : <span class="type">Logger</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method to get the logger name for this object</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= &#123;</span><br><span class="line">    <span class="comment">// Ignore trailing $'s in the class names for Scala objects</span></span><br><span class="line">    <span class="keyword">this</span>.getClass.getName.stripSuffix(<span class="string">"$"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method to get or create the logger for this object</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>: <span class="type">Logger</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (log_ == <span class="literal">null</span>) &#123;</span><br><span class="line">      initializeLogIfNecessary(<span class="literal">false</span>)</span><br><span class="line">      log_ = <span class="type">LoggerFactory</span>.getLogger(logName)</span><br><span class="line">    &#125;</span><br><span class="line">    log_</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Log methods that take only a String</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isInfoEnabled) log.info(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logDebug</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled) log.debug(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logTrace</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isTraceEnabled) log.trace(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isWarnEnabled) log.warn(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isErrorEnabled) log.error(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Log methods that take Throwables (Exceptions/Errors) too</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isInfoEnabled) log.info(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logDebug</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled) log.debug(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logTrace</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isTraceEnabled) log.trace(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isWarnEnabled) log.warn(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>, throwable: <span class="type">Throwable</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log.isErrorEnabled) log.error(msg, throwable)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">isTraceEnabled</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    log.isTraceEnabled</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLogIfNecessary</span></span>(isInterpreter: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="type">Logging</span>.initialized) &#123;</span><br><span class="line">      <span class="type">Logging</span>.initLock.synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="type">Logging</span>.initialized) &#123;</span><br><span class="line">          initializeLogging(isInterpreter)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLogging</span></span>(isInterpreter: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Don't use a logger in here, as this is itself occurring during initialization of a logger</span></span><br><span class="line">    <span class="comment">// If Log4j 1.2 is being used, but is not initialized, load a default properties file</span></span><br><span class="line">    <span class="keyword">val</span> binderClass = <span class="type">StaticLoggerBinder</span>.getSingleton.getLoggerFactoryClassStr</span><br><span class="line">    <span class="comment">// This distinguishes the log4j 1.2 binding, currently</span></span><br><span class="line">    <span class="comment">// org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently</span></span><br><span class="line">    <span class="comment">// org.apache.logging.slf4j.Log4jLoggerFactory</span></span><br><span class="line">    <span class="keyword">val</span> usingLog4j12 = <span class="string">"org.slf4j.impl.Log4jLoggerFactory"</span>.equals(binderClass)</span><br><span class="line">    <span class="keyword">if</span> (usingLog4j12) &#123;</span><br><span class="line">      <span class="keyword">val</span> log4j12Initialized = <span class="type">LogManager</span>.getRootLogger.getAllAppenders.hasMoreElements</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      <span class="keyword">if</span> (!log4j12Initialized) &#123;</span><br><span class="line">        <span class="keyword">val</span> defaultLogProps = <span class="string">"org/apache/spark/log4j-defaults.properties"</span></span><br><span class="line">        <span class="type">Option</span>(<span class="type">Utils</span>.getSparkClassLoader.getResource(defaultLogProps)) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(url) =&gt;</span><br><span class="line">            <span class="type">PropertyConfigurator</span>.configure(url)</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">s"Using Spark's default log4j profile: <span class="subst">$defaultLogProps</span>"</span>)</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">s"Spark was unable to load <span class="subst">$defaultLogProps</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isInterpreter) &#123;</span><br><span class="line">        <span class="comment">// Use the repl's main class to define the default log level when running the shell,</span></span><br><span class="line">        <span class="comment">// overriding the root logger's config if they're different.</span></span><br><span class="line">        <span class="keyword">val</span> rootLogger = <span class="type">LogManager</span>.getRootLogger()</span><br><span class="line">        <span class="keyword">val</span> replLogger = <span class="type">LogManager</span>.getLogger(logName)</span><br><span class="line">        <span class="keyword">val</span> replLevel = <span class="type">Option</span>(replLogger.getLevel()).getOrElse(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">        <span class="keyword">if</span> (replLevel != rootLogger.getEffectiveLevel()) &#123;</span><br><span class="line">          <span class="type">System</span>.err.printf(<span class="string">"Setting default log level to \"%s\".\n"</span>, replLevel)</span><br><span class="line">          <span class="type">System</span>.err.println(<span class="string">"To adjust logging level use sc.setLogLevel(newLevel). "</span> +</span><br><span class="line">            <span class="string">"For SparkR, use setLogLevel(newLevel)."</span>)</span><br><span class="line">          rootLogger.setLevel(replLevel)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Logging</span>.initialized = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Force a call into slf4j to initialize it. Avoids this happening from multiple threads</span></span><br><span class="line">    <span class="comment">// and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html</span></span><br><span class="line">    log</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initialized = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">val</span> initLock = <span class="keyword">new</span> <span class="type">Object</span>()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// We use reflection here to handle the case where users remove the</span></span><br><span class="line">    <span class="comment">// slf4j-to-jul bridge order to route their logs to JUL.</span></span><br><span class="line">    <span class="keyword">val</span> bridgeClass = <span class="type">Utils</span>.classForName(<span class="string">"org.slf4j.bridge.SLF4JBridgeHandler"</span>)</span><br><span class="line">    bridgeClass.getMethod(<span class="string">"removeHandlersForRootLogger"</span>).invoke(<span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">val</span> installed = bridgeClass.getMethod(<span class="string">"isInstalled"</span>).invoke(<span class="literal">null</span>).asInstanceOf[<span class="type">Boolean</span>]</span><br><span class="line">    <span class="keyword">if</span> (!installed) &#123;</span><br><span class="line">      bridgeClass.getMethod(<span class="string">"install"</span>).invoke(<span class="literal">null</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt; <span class="comment">// can't log anything yet so just fail silently</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;先编写HelloWord-scala文件&quot;&gt;&lt;a href=&quot;#先编写HelloWord-scala文件&quot; class=&quot;headerlink&quot; title=&quot;先编写HelloWord.scala文件&quot;&gt;&lt;/a&gt;先编写HelloWord.scala文件&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="scala" scheme="https://tgluon.github.io/categories/scala/"/>
    
    
      <category term="scala" scheme="https://tgluon.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>sparksql详解</title>
    <link href="https://tgluon.github.io/2019/01/04/sparksql%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2019/01/04/sparksql详解/</id>
    <published>2019-01-04T11:53:18.000Z</published>
    <updated>2019-02-26T07:20:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-SQL的Dataset基本操作"><a href="#Spark-SQL的Dataset基本操作" class="headerlink" title="Spark SQL的Dataset基本操作"></a>Spark SQL的Dataset基本操作</h1><p><a href="https://www.toutiao.com/i6631318012546793992/" target="_blank" rel="noopener">https://www.toutiao.com/i6631318012546793992/</a><br>版本：Spark 2.3.1，hadoop-2.7.4，jdk1.8为例讲解   </p><h2 id="基本简介和准备工作"><a href="#基本简介和准备工作" class="headerlink" title="基本简介和准备工作"></a>基本简介和准备工作</h2><p>Dataset接口是在spark 1.6引入的，受益于RDD(强类型，可以使用强大的lambda函数)，同时也可以享受Spark SQL优化执行引擎的优点。Dataset的可以从jvm 对象创建，然后就可以使用转换函数(map，flatmap，filter等)。</p><p>1.6版本之前常用的Dataframe是一种特殊的Dataset，也即是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">DataFrame</span> </span>= <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure></p><p>结构化数据文件(json,csv,orc等)，hive表，外部数据库，已有RDD。<br>下面进行测试，大家可能都会说缺少数据，实际上Spark源码里跟我们提供了丰富的测试数据。源码的examples路径下：examples/src/main/resources。</p><p>首先要创建一个SparkSession:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(<span class="keyword">this</span>.getClass.getName)</span><br><span class="line">.setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">.set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"localhost"</span>) </span><br><span class="line"><span class="comment">// executor的实例数</span></span><br><span class="line">.set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>) </span><br><span class="line">.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"4"</span>) </span><br><span class="line"><span class="comment">// sql shuffle的并行度，由于是本地测试，所以设置较小值，避免产生过多空task，实际上要根据生产数据量进行设置。</span></span><br><span class="line">.set(<span class="string">"spark.sql.shuffle.partitions"</span>,<span class="string">"4"</span>)</span><br><span class="line">.setJars(<span class="type">List</span>(<span class="string">"/Users/meitu/Desktop/sparkjar/bigdata.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span> </span><br><span class="line">,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span> </span><br><span class="line"> ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span> </span><br><span class="line">.builder() </span><br><span class="line">.config(sparkConf) </span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure></p><p>创建dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(</span><br><span class="line"><span class="type">Seq</span>( (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>), (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">200</span>), (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>), (<span class="string">"Warsaw"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">50</span>), (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">150</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p><p>使用函数的时候要导入包：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col,expr&#125;</span><br></pre></td></tr></table></figure></p><h2 id="select"><a href="#select" class="headerlink" title="select"></a>select</h2><p>列名称可以是字符串，这种形式无法对列名称使用表达式进行逻辑操作。<br>使用col函数，可以直接对列进行一些逻辑操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.select(<span class="string">"city"</span>,<span class="string">"year"</span>,<span class="string">"amount"</span>).show(<span class="number">1</span>)</span><br><span class="line">sales.select(col(<span class="string">"city"</span>),col(<span class="string">"amount"</span>)+<span class="number">1</span>).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h2 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h2><p>参数是字符串，且直接可以使用表达式。<br>也可以使用select+expr函数来替代。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.selectExpr(<span class="string">"city"</span>,<span class="string">"year as date"</span>,<span class="string">"amount+1"</span>).show(<span class="number">10</span>)</span><br><span class="line">sales.select(expr(<span class="string">"city"</span>),expr(<span class="string">"year as date"</span>),expr(<span class="string">"amount+1"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>参数可以是与col结合的表达式，参数类型为row返回值为boolean的函数，字符串表达式。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.filter(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.filter(row=&gt;&#123; row.getInt(<span class="number">2</span>)&gt;<span class="number">150</span>&#125;).show(<span class="number">10</span>)</span><br><span class="line">sales.filter(<span class="string">"amount &gt; 150 "</span>).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="where"><a href="#where" class="headerlink" title="where"></a>where</h2><p>类似于fliter，参数可以是与col函数结合的表达式也可以是直接使用表达式字符串。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.where(col(<span class="string">"amount"</span>)&gt;<span class="number">150</span>).show()</span><br><span class="line">sales.where(<span class="string">"amount &gt; 150 "</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h2><p>主要是以count和agg聚合函数为例讲解groupby函数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.groupBy(<span class="string">"city"</span>).count().show(<span class="number">10</span>)</span><br><span class="line">sales.groupBy(col(<span class="string">"city"</span>)).agg(sum(<span class="string">"amount"</span>).as(<span class="string">"total"</span>)).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>两个dataset的union操作这个等价于union all操作，所以要实现传统数据库的union操作，需要在其后使用distinct进行去重操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.union(sales).groupBy(<span class="string">"city"</span>).count().show()</span><br></pre></td></tr></table></figure></p><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>join操作相对比较复杂，具体如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 相同的列进行join</span></span><br><span class="line"> sales.join(sales,<span class="string">"city"</span>).show(<span class="number">10</span>)</span><br><span class="line"> <span class="comment">// 多列join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>)).show() </span><br><span class="line"><span class="comment">/* 指定join类型， join 类型可以选择: </span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, </span></span><br><span class="line"><span class="comment">`left`, `left_outer`, `right`, `right_outer`, </span></span><br><span class="line"><span class="comment">`left_semi`, `left_anti`. </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="comment">// 内部join</span></span><br><span class="line">sales.join(sales,<span class="type">Seq</span>(<span class="string">"city"</span>,<span class="string">"year"</span>),<span class="string">"inner"</span>).show() </span><br><span class="line"><span class="comment">/* join条件 ：</span></span><br><span class="line"><span class="comment">可以在join方法里放入join条件，</span></span><br><span class="line"><span class="comment">也可以使用where,这两种情况都要求字段名称不一样。</span></span><br><span class="line"><span class="comment">*/</span> </span><br><span class="line">sales.join(sales, col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() sales.join(sales).where(col(<span class="string">"city"</span>).alias(<span class="string">"city1"</span>) === col(<span class="string">"city"</span>)).show() </span><br><span class="line"> <span class="comment">/* </span></span><br><span class="line"><span class="comment">dataset的self join 此处使用where作为条件，</span></span><br><span class="line"><span class="comment">需要增加配置.set("spark.sql.crossJoin.enabled","true") </span></span><br><span class="line"><span class="comment">也可以加第三个参数，join类型，可以选择如下： </span></span><br><span class="line"><span class="comment">`inner`, `cross`, `outer`, `full`, `full_outer`, `left`,</span></span><br><span class="line"><span class="comment"> `left_outer`, `right`, `right_outer`, `left_semi`, `left_anti` </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.join(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show() sales.join(sales).where(sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>)).show()</span><br><span class="line"><span class="comment">/* joinwith,可以指定第三个参数，join类型，</span></span><br><span class="line"><span class="comment">类型可以选择如下：</span></span><br><span class="line"><span class="comment"> `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, </span></span><br><span class="line"><span class="comment"> `right`, `right_outer`。 </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">sales.joinWith(sales,sales(<span class="string">"city"</span>) === sales(<span class="string">"city"</span>),<span class="string">"inner"</span>).show()</span><br></pre></td></tr></table></figure></p><p>输出结果： </p><h2 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h2><p>orderby 全局有序，其实用的还是sort<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.orderBy(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><p>全局排序，直接替换掉8小结的orderby即可。</p><h2 id="sortwithinpartition"><a href="#sortwithinpartition" class="headerlink" title="sortwithinpartition"></a>sortwithinpartition</h2><p>在分区内部进行排序，局部排序。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sales.sortWithinPartitions(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).show()</span><br><span class="line">sales.sortWithinPartitions(<span class="string">"city"</span>,<span class="string">"year"</span>).show()</span><br></pre></td></tr></table></figure></p><p>可以看到，city为背景的应该是分配到不同的分区，然后每个分区内部year都是有序的。</p><h2 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* withColumn </span></span><br><span class="line"><span class="comment">假如列，存在就替换，不存在新增 </span></span><br><span class="line"><span class="comment">withColumnRenamed </span></span><br><span class="line"><span class="comment">对已有的列进行重命名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//相当于给原来amount列，+1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 对amount列+1，然后将值增加到一个新列 amount1</span></span><br><span class="line">sales.withColumn(<span class="string">"amount1"</span>,col(<span class="string">"amount"</span>)+<span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// 将amount列名，修改为amount1</span></span><br><span class="line">sales.withColumnRenamed(<span class="string">"amount"</span>,<span class="string">"amount1"</span>).show()</span><br></pre></td></tr></table></figure><h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><p>这个跟rdd的foreach一样，元素类型是row。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sales.foreach(row=&gt;&#123; </span><br><span class="line">println(row.getString(<span class="number">0</span>))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p><h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><p>跟RDD的foreachPartition一样，针对分区进行计算，对于输出到数据库，kafka等数据相对于使用foreach可以大量减少连接数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sales.foreachPartition(partition=&gt;&#123; </span><br><span class="line"> <span class="comment">//打开数据库链接等 </span></span><br><span class="line"> partition.foreach(each=&gt;&#123; </span><br><span class="line"> println(each.getString(<span class="number">0</span>))</span><br><span class="line"> <span class="comment">//插入数据库 </span></span><br><span class="line"> &#125;)</span><br><span class="line"> <span class="comment">//关闭数据库链接 </span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><p>针对dataset的行去重，返回的是所有行都不重复的dataset。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.distinct().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><h2 id="dropDuplicates"><a href="#dropDuplicates" class="headerlink" title="dropDuplicates"></a>dropDuplicates</h2><p>这个适用于dataset有唯一的主键，然后对主键进行去重。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> before = sales.count()</span><br><span class="line"><span class="keyword">val</span> after = sales.dropDuplicates(<span class="string">"city"</span>).count()</span><br><span class="line">println(<span class="string">"before ====&gt; "</span> +before)</span><br><span class="line">println(<span class="string">"after ====&gt; "</span>+after)</span><br></pre></td></tr></table></figure></p><h2 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h2><p>删除一列，或者多列，这是一个变参数算子。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sales.drop(<span class="string">"city"</span>).show()</span><br><span class="line">打印出来schema信息如下：</span><br><span class="line">root</span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><h2 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h2><p>输出dataset的schema信息<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sales.printSchema()</span><br><span class="line"></span><br><span class="line">输出结果如下：</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"></span><br><span class="line">|-- city: string (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">|-- year: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">|-- amount: integer (nullable = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><h2 id="explain"><a href="#explain" class="headerlink" title="explain()"></a>explain()</h2><p>打印执行计划，这个便于调试，了解spark sql引擎的优化执行的整个过程<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sales.orderBy(col(<span class="string">"year"</span>).desc,col(<span class="string">"amount"</span>).asc).explain()</span><br><span class="line">执行计划输出如下：</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">1</span>) <span class="type">Sort</span> [year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>], <span class="literal">true</span>, <span class="number">0</span></span><br><span class="line">+- <span class="type">Exchange</span> rangepartitioning(year#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, amount#<span class="number">8</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>, <span class="number">3</span>)</span><br><span class="line">+- <span class="type">LocalTableScan</span> [city#<span class="number">6</span>, year#<span class="number">7</span>, amount#<span class="number">8</span>]</span><br></pre></td></tr></table></figure></p><h1 id="Spark-SQL入门到精通之第二篇Dataset的复杂操作"><a href="#Spark-SQL入门到精通之第二篇Dataset的复杂操作" class="headerlink" title="Spark SQL入门到精通之第二篇Dataset的复杂操作"></a>Spark SQL入门到精通之第二篇Dataset的复杂操作</h1><p>本文是Spark SQL入门到精通系列第二弹，数据仓库常用的操作：<br>cube，rollup，pivot操作。</p><h2 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h2><p>简单的理解就是维度及度量组成的数据体。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sales.cube(<span class="string">"city"</span>,<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>)) </span><br><span class="line">.sort(col(<span class="string">"city"</span>).desc_nulls_first,col(<span class="string">"year"</span>).desc_nulls_first) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>举个简单的例子，上面的纬度city，year两个列是纬度，然后amount是要进行聚合的度量。<br>实际上就相当于，(year,city),(year),(city),() 分别分组然后对amount求sum，最终输出结果，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> city_year = sales</span><br><span class="line">.groupBy(<span class="string">"city"</span>,<span class="string">"year"</span>).</span><br><span class="line">agg(sum(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> city = sales.groupBy(<span class="string">"city"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"city"</span>), lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> year = sales.groupBy(<span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select( lit(<span class="literal">null</span>) as <span class="string">"city"</span>,col(<span class="string">"year"</span>), col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = sales .groupBy()</span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"city"</span>, lit(<span class="literal">null</span>) as <span class="string">"year"</span>, col(<span class="string">"amount"</span>)) </span><br><span class="line">city_year.union(city).union(year).union(none) </span><br><span class="line">.sort(desc_nulls_first(<span class="string">"city"</span>), desc_nulls_first(<span class="string">"year"</span>)) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><h2 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h2><p>这里也是以案例开始，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> expenses = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line">((<span class="number">2012</span>, <span class="type">Month</span>.<span class="type">DECEMBER</span>, <span class="number">12</span>), <span class="number">5</span>), </span><br><span class="line"> ((<span class="number">2016</span>, <span class="type">Month</span>.<span class="type">AUGUST</span>, <span class="number">13</span>), <span class="number">10</span>), </span><br><span class="line"> ((<span class="number">2017</span>, <span class="type">Month</span>.<span class="type">MAY</span>, <span class="number">27</span>), <span class="number">15</span>)) </span><br><span class="line">.map &#123; <span class="keyword">case</span> ((yy, mm, dd), a) =&gt; (<span class="type">LocalDate</span>.of(yy, mm, dd), a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (d.toString, a) &#125; </span><br><span class="line">.map &#123; <span class="keyword">case</span> (d, a) =&gt; (<span class="type">Date</span>.valueOf(d), a) &#125;).toDF(<span class="string">"date"</span>, <span class="string">"amount"</span>)</span><br><span class="line"><span class="comment">// rollup time!</span></span><br><span class="line"><span class="keyword">val</span> res = expenses </span><br><span class="line">.rollup(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>这个等价于分别对(year,month),(year),()进行 groupby 对amount求sum，然后再进行union操作，也即是可以按照下面的实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> year_month = expenses</span><br><span class="line">.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>, month(col(<span class="string">"date"</span>)) as <span class="string">"month"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>)</span><br><span class="line"><span class="keyword">val</span> yearOnly = expenses.groupBy(year(col(<span class="string">"date"</span>)) as <span class="string">"year"</span>) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(col(<span class="string">"year"</span>), lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line"><span class="keyword">val</span> none = expenses.groupBy() </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.select(lit(<span class="literal">null</span>) as <span class="string">"year"</span>, lit(<span class="literal">null</span>) as <span class="string">"month"</span>, col(<span class="string">"amount"</span>))</span><br><span class="line">year_month.union(yearOnly).union(none) </span><br><span class="line">.sort(col(<span class="string">"year"</span>).asc_nulls_last, col(<span class="string">"month"</span>).asc_nulls_last) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><h2 id="pivot"><a href="#pivot" class="headerlink" title="pivot"></a>pivot</h2><p>旋转操作<br><a href="https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Jky2q3FW5wqQ-prpsW2OEw</a><br>Pivot 算子是 spark 1.6 版本开始引入的，在 spark2.4版本中功能做了增强，还是比较强大的，做过数据清洗ETL工作的都知道，行列转换是一个常见的数据整理需求。spark 中的Pivot 可以根据枢轴点(Pivot Point) 把多行的值归并到一行数据的不同列，这个估计不太好理解，我们下面使用例子说明，看看pivot 这个算子在处理复杂数据时候的威力。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>( </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">100</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line">(<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>,<span class="string">"Warsaw"</span>), </span><br><span class="line"> (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">50</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">150</span>,<span class="string">"Boston"</span>), </span><br><span class="line">(<span class="string">"Toronto"</span>, <span class="number">2017</span>, <span class="number">50</span>,<span class="string">"Toronto"</span>)))</span><br><span class="line">.toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>,<span class="string">"test"</span>)</span><br><span class="line">sales.groupBy(<span class="string">"year"</span>)</span><br><span class="line">.pivot(<span class="string">"city"</span>,<span class="type">Seq</span>(<span class="string">"Warsaw"</span>,<span class="string">"Boston"</span>,<span class="string">"Toronto"</span>)) </span><br><span class="line">.agg(sum(<span class="string">"amount"</span>) as <span class="string">"amount"</span>) </span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></p><p>思路就是首先对year分组，然后旋转city字段，只取:<br>“Warsaw”,”Boston”,”Toronto”<br>然后对amount进行聚合操作<br><strong>案例：</strong><br>使用Pivot 来统计天气走势。<br>下面是西雅图的天气数据表，每行代表一天的天气最高值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  <span class="type">Date</span>         <span class="type">Temp</span> (°<span class="type">F</span>)   </span><br><span class="line"><span class="number">07</span><span class="number">-22</span><span class="number">-2018</span><span class="number">86</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-23</span><span class="number">-2018</span><span class="number">90</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-24</span><span class="number">-2018</span><span class="number">91</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-25</span><span class="number">-2018</span><span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-26</span><span class="number">-2018</span><span class="number">92</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-27</span><span class="number">-2018</span><span class="number">88</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-28</span><span class="number">-2018</span><span class="number">85</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-29</span><span class="number">-2018</span><span class="number">94</span>   </span><br><span class="line"><span class="number">07</span><span class="number">-30</span><span class="number">-2018</span><span class="number">89</span></span><br></pre></td></tr></table></figure></p><p>如果我们想看下最近几年的天气走势，如果这样一天一行数据，是很难看出趋势来的，最直观的方式是 按照年来分行，然后每一列代表一个月的平均天气，这样一行数据，就可以看到这一年12个月的一个天气走势，下面我们使用 pivot 来构造这样一个查询结果：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (</span><br><span class="line">      <span class="number">1</span> JAN, <span class="number">2</span> FEB, <span class="number">3</span> MAR, <span class="number">4</span> APR, <span class="number">5</span> MAY, <span class="number">6</span> JUN, <span class="number">7</span> JUL, </span><br><span class="line">      <span class="number">8</span> AUG, <span class="number">9</span> SEP, <span class="number">10</span> <span class="keyword">OCT</span>, <span class="number">11</span> NOV, <span class="number">12</span> <span class="built_in">DEC</span></span><br><span class="line">    )</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult1.png" alt=""><br>是不是很直观，第一行就代表 2018 年，从1月到12月的平均天气，能看出一年的天气走势。<br>我们来看下这个 sql 是怎么玩的，首先是一个 子查询语句，我们最终关心的是 年份，月份，和最高天气值，所以先使用子查询对原始数据进行处理，从日期里面抽取出来年份和月份。</p><p>下面在子查询的结果上使用 pivot 语句，pivot 第一个参数是一个聚合语句，这个代表聚合出来一个月30天的一个平均气温，第二个参数是 FOR month，这个是指定以哪个列为枢轴列，第三个 In 子语句指定我们需要进行行列转换的具体的 枢轴点(Pivot Point)的值，上面的例子中 1到12月份都包含了，而且给了一个别名，如果只指定 1到6月份，结果就如下了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult2.png" alt=""><br>上面sql语句里面有个特别的点需要注意， 就是聚合的时候有个隐含的维度字段，就是 年份，按理来讲，我们没有写  group-by year， 为啥结果表里面不同年份区分在了不同的行，原因是，FORM 子查询出来的每行有 3个列， year，month，tmp，如果一个列既不出现在进行聚合计算的列中（temp 是聚合计算的列）， 也不作为枢轴列 ， 就会作为聚合的时候一个隐含的维度。我们的例子中算平均值聚合操作的维度是 (year, month)，一个是隐含维度，一个是 枢轴列维度， 这一点一定要注意，如果不需要按照 year 来区分，FORM 查询的时候就不要加上这个列。<br><strong><em>指定多个聚合语句</em></strong><br>上文中只有一个聚合语句，就是计算平均天气，其实是可以加多个聚合语句的，比如我们需要看到 7，8，9 月份每个月的最大气温和平均气温，就可以用以下SQL语句。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">      <span class="keyword">year</span>(<span class="built_in">date</span>) <span class="keyword">year</span>, </span><br><span class="line">      <span class="keyword">month</span>(<span class="built_in">date</span>) <span class="keyword">month</span>, </span><br><span class="line">      temp </span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">      high_temps </span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">      <span class="built_in">date</span> <span class="keyword">between</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> </span><br><span class="line">      <span class="keyword">and</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line">  ) <span class="keyword">PIVOT</span> (</span><br><span class="line">    <span class="keyword">CAST</span> (</span><br><span class="line">      <span class="keyword">AVG</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">    ) <span class="keyword">avg</span>, </span><br><span class="line">    <span class="keyword">max</span>(temp) <span class="keyword">max</span> <span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN, <span class="number">7</span> JUL, <span class="number">8</span> AUG, <span class="number">9</span> SEP)</span><br><span class="line">  ) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">  <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>上文中指定了两个聚合语句，查询后， 枢轴点(Pivot Point)  和 聚合语句 的笛卡尔积作为结果的不同列，也就是  <value>_<aggexpr>， 看下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult3.png" alt=""><br><strong><em>聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处</em></strong><br>现在假如我们有西雅图每天的最低温数据，我们需要把最高温和最低温放在同一张表里面看对比着看.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Date</span>   <span class="type">Temp</span> (°<span class="type">F</span>)</span><br><span class="line">…        …</span><br><span class="line"><span class="number">08</span><span class="number">-01</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-02</span><span class="number">-2018</span><span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-03</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-04</span><span class="number">-2018</span><span class="number">58</span></span><br><span class="line"><span class="number">08</span><span class="number">-05</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line"><span class="number">08</span><span class="number">-06</span><span class="number">-2018</span><span class="number">59</span></span><br><span class="line">…         …</span><br></pre></td></tr></table></figure></aggexpr></value></p><p>我们使用  UNION ALL 把两张表做一个合并：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'H'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  high_temps </span><br><span class="line"><span class="keyword">UNION</span> ALL </span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>, temp, <span class="string">'L'</span> <span class="keyword">as</span> flag </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  low_temps;</span><br></pre></td></tr></table></figure></p><p>现在使用 pivot 来进行处理：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> <span class="keyword">month</span> <span class="keyword">in</span> (<span class="number">6</span> JUN,<span class="number">7</span> JUL,<span class="number">8</span> AUG , <span class="number">9</span> SEP)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span> ,<span class="string">`H/L`</span> <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure></p><p>我们统计了 4年中  7，8，9 月份最低温和最高温的平均值，这里要注意的是，我们把 year 和 一个最低最高的标记（H/L）都作为隐含维度，不然算出来的就是最低最高温度在一起的平均值了。结果如下图：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult4.png" alt=""><br>上面的查询中，我们把最低最高的标记（H/L）也作为了一个隐含维度，group-by 的维度就变成了 （year， H/L, month）, 但是year 和 H/L 体现在了不同行上面，month 体现在了不同的列上面，这就是  聚合列（Grouping Columns）和 枢轴列（Pivot Columns）的不同之处。</p><p>再接再厉，我们把 H/L 作为  枢轴列（Pivot Columns） 进行查询：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'H'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> high_temps</span><br><span class="line">  <span class="keyword">UNION</span> ALL </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="built_in">date</span>,temp,<span class="string">'L'</span> <span class="keyword">as</span> flag</span><br><span class="line">  <span class="keyword">FROM</span> low_temps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">date</span> <span class="keyword">BETWEEN</span> <span class="built_in">DATE</span> <span class="string">'2015-01-01'</span> <span class="keyword">AND</span> <span class="built_in">DATE</span> <span class="string">'2018-08-31'</span></span><br><span class="line"><span class="keyword">PIVOT</span>(</span><br><span class="line"><span class="keyword">CAST</span>(<span class="keyword">avg</span>(temp) <span class="keyword">as</span> <span class="built_in">DECIMAL</span>(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">FOR</span> (<span class="keyword">month</span>,flag) <span class="keyword">in</span> (</span><br><span class="line">(<span class="number">6</span>,<span class="string">'H'</span>) JUN_hi,(<span class="number">6</span>,<span class="string">'L'</span>) JUN_lo,</span><br><span class="line">(<span class="number">7</span>,<span class="string">'H'</span>) JUl_hi,(<span class="number">7</span>,<span class="string">'L'</span>) JUl_lo,</span><br><span class="line">(<span class="number">8</span>,<span class="string">'H'</span>) AUG_hi,(<span class="number">8</span>,<span class="string">'L'</span>) AUG_lo,</span><br><span class="line">(<span class="number">9</span>,<span class="string">'H'</span>) SEP_hi,(<span class="number">9</span>,<span class="string">'L'</span>) SEP_lo</span><br><span class="line">)</span><br><span class="line">) <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span> <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><p>结果的展现方式就和上面的不同了，虽然每个单元格值都是相同的，但是把  H/L 和 month 笛卡尔积作为列，H/L 维度体现在了列上面了：<br><img src="/2019/01/04/sparksql详解/./images/PivotReSult5.png" alt="">  </p><h1 id="源码-Spark-SQL-分区特性第一弹"><a href="#源码-Spark-SQL-分区特性第一弹" class="headerlink" title="源码:Spark SQL 分区特性第一弹"></a>源码:Spark SQL 分区特性第一弹</h1><h2 id="常见RDD分区"><a href="#常见RDD分区" class="headerlink" title="常见RDD分区"></a>常见RDD分区</h2><p>Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。<br>举几个浪尖在星球里分享比较多的例子，比如：<br>Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。<br>Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。<br>普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。<br>这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。  </p><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>首先是由Seq数据集合生成一个Dataset<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sales = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2016</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">80</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">130</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2015</span>, <span class="number">160</span>),</span><br><span class="line">     (<span class="string">"Warsaw"</span>, <span class="number">2017</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2017</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2016</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2015</span>, <span class="number">10</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Beijing"</span>, <span class="number">2014</span>, <span class="number">170</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">110</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">150</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2017</span>, <span class="number">180</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2016</span>, <span class="number">30</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2015</span>, <span class="number">200</span>),</span><br><span class="line">     (<span class="string">"Boston"</span>, <span class="number">2014</span>, <span class="number">20</span>)</span><br><span class="line">   )).toDF(<span class="string">"city"</span>, <span class="string">"year"</span>, <span class="string">"amount"</span>)</span><br></pre></td></tr></table></figure></p><p>将Dataset存处为partquet格式的hive表，分两种情况：<br>用city和year字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>,<span class="string">"year"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCityAndYear"</span>)</span><br></pre></td></tr></table></figure></p><p>用city字段分区<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sales.write.partitionBy(<span class="string">"city"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"ParquetTestCity"</span>)</span><br></pre></td></tr></table></figure></p><p>读取数据采用的是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> res = spark.read.parquet(<span class="string">"/user/hive/warehouse/parquettestcity"</span>)</span><br></pre></td></tr></table></figure></p><p>直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。   </p><h3 id="1-spark-default-parallelism-40"><a href="#1-spark-default-parallelism-40" class="headerlink" title="1. spark.default.parallelism =40"></a>1. spark.default.parallelism =40</h3><p>Dataset的分区数是由参数：<br>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行<br>这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。   </p><h3 id="2-spark-default-parallelism-4"><a href="#2-spark-default-parallelism-4" class="headerlink" title="2. spark.default.parallelism =4"></a>2. spark.default.parallelism =4</h3><p>Dataset的分区数是由参数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">"partition size = "</span>+res.rdd.partitions.length)</span><br></pre></td></tr></table></figure></p><p>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。<br><img src="/2019/01/04/sparksql详解/images/dataset分区.jpg" alt=""><br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。</p><p>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br>那么数据源生成的Dataset的分区数到底是如何决定的呢？<br>我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createNonBucketedReadRDD</span></span>(</span><br><span class="line">                                       readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>],</span><br><span class="line">                                       selectedPartitions: <span class="type">Seq</span>[<span class="type">PartitionDirectory</span>],</span><br><span class="line">                                       fsRelation: <span class="type">HadoopFsRelation</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">     selectedPartitions 的大小代表目录数目</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   println(<span class="string">"selectedPartitions.size : "</span>+ selectedPartitions.size)</span><br><span class="line">   <span class="keyword">val</span> defaultMaxSplitBytes =</span><br><span class="line">     fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br><span class="line">   <span class="keyword">val</span> openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br><span class="line"></span><br><span class="line">   <span class="comment">// spark.default.parallelism</span></span><br><span class="line">   <span class="keyword">val</span> defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 计算文件总大小，单位字节数</span></span><br><span class="line">   <span class="keyword">val</span> totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br><span class="line"></span><br><span class="line">   <span class="comment">//计算平均每个并行度读取数据大小</span></span><br><span class="line">   <span class="keyword">val</span> bytesPerCore = totalBytes / defaultParallelism</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值</span></span><br><span class="line">   <span class="comment">// 然后，比较spark.sql.files.maxPartitionBytes 取小者</span></span><br><span class="line">   <span class="keyword">val</span> maxSplitBytes = <span class="type">Math</span>.min(defaultMaxSplitBytes, <span class="type">Math</span>.max(openCostInBytes, bytesPerCore))</span><br><span class="line">   logInfo(<span class="string">s"Planning scan with bin packing, max size: <span class="subst">$maxSplitBytes</span> bytes, "</span> +</span><br><span class="line">     <span class="string">s"open cost is considered as scanning <span class="subst">$openCostInBytes</span> bytes."</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 这对目录遍历</span></span><br><span class="line">   <span class="keyword">val</span> splitFiles = selectedPartitions.flatMap &#123; partition =&gt;</span><br><span class="line">     partition.files.flatMap &#123; file =&gt;</span><br><span class="line">       <span class="keyword">val</span> blockLocations = getBlockLocations(file)</span><br><span class="line"></span><br><span class="line">       <span class="comment">//判断文件类型是否支持分割，以parquet为例，是支持分割的</span></span><br><span class="line">       <span class="keyword">if</span> (fsRelation.fileFormat.isSplitable(</span><br><span class="line">         fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// eg. 0 until 2不包括 2。相当于</span></span><br><span class="line">    <span class="comment">// println(0 until(10) by 3) 输出 Range(0, 3, 6, 9)</span></span><br><span class="line">         (<span class="number">0</span>L until file.getLen by maxSplitBytes).map &#123; offset =&gt;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 计算文件剩余的量</span></span><br><span class="line">           <span class="keyword">val</span> remaining = file.getLen - offset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区</span></span><br><span class="line">           <span class="keyword">val</span> size = <span class="keyword">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="keyword">else</span> remaining</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 位置信息</span></span><br><span class="line">           <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, offset, size)</span><br><span class="line">           <span class="type">PartitionedFile</span>(</span><br><span class="line">             partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// 不可分割的话，那即是一个文件一个分区</span></span><br><span class="line">         <span class="keyword">val</span> hosts = getBlockHosts(blockLocations, <span class="number">0</span>, file.getLen)</span><br><span class="line">         <span class="type">Seq</span>(<span class="type">PartitionedFile</span>(</span><br><span class="line">           partition.values, file.getPath.toUri.toString, <span class="number">0</span>, file.getLen, hosts))</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;.toArray.sortBy(_.length)(implicitly[<span class="type">Ordering</span>[<span class="type">Long</span>]].reverse)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> partitions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">FilePartition</span>]</span><br><span class="line">   <span class="keyword">val</span> currentFiles = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">PartitionedFile</span>]</span><br><span class="line">   <span class="keyword">var</span> currentSize = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Close the current partition and move to the next. */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">closePartition</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">if</span> (currentFiles.nonEmpty) &#123;</span><br><span class="line">       <span class="keyword">val</span> newPartition =</span><br><span class="line">         <span class="type">FilePartition</span>(</span><br><span class="line">           partitions.size,</span><br><span class="line">           currentFiles.toArray.toSeq) <span class="comment">// Copy to a new Array.</span></span><br><span class="line">       partitions += newPartition</span><br><span class="line">     &#125;</span><br><span class="line">     currentFiles.clear()</span><br><span class="line">     currentSize = <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Assign files to partitions using "Next Fit Decreasing"</span></span><br><span class="line">   splitFiles.foreach &#123; file =&gt;</span><br><span class="line">     <span class="keyword">if</span> (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class="line">       closePartition()</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Add the given file to the current partition.</span></span><br><span class="line">     currentSize += file.length + openCostInBytes</span><br><span class="line">     currentFiles += file</span><br><span class="line">   &#125;</span><br><span class="line">   closePartition()</span><br><span class="line"></span><br><span class="line">   println(<span class="string">"FileScanRDD partitions size : "</span>+partitions.size)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">FileScanRDD</span>(fsRelation.sparkSession, readFile, partitions)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><h1 id="找到一列中的中位数"><a href="#找到一列中的中位数" class="headerlink" title="找到一列中的中位数"></a>找到一列中的中位数</h1><p><a href="https://spark.apache.org/docs/latest/api/sql/index.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/api/sql/index.html</a><br>df函数： approxQuantile<br>sql函数： percentile_approx  </p><h1 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h1><p>ServiceLoader<br><a href="https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/QpYvqJpw7TnFAY8rD6Jf3w</a><br>ServiceLoader是SPI的是一种实现，所谓SPI，即Service Provider Interface，用于一些服务提供给第三方实现或者扩展，可以增强框架的扩展或者替换一些组件。<br>要配置在相关项目的固定目录下：<br>resources/META-INF/services/接口全称。<br>这个在大数据的应用中颇为广泛，比如Spark2.3.1 的集群管理器插入：<br>SparkContext类<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getClusterManager</span></span>(url: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">ExternalClusterManager</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="keyword">val</span> serviceLoaders =</span><br><span class="line">      <span class="type">ServiceLoader</span>.load(classOf[<span class="type">ExternalClusterManager</span>], loader).asScala.filter(_.canCreate(url))</span><br><span class="line">    <span class="keyword">if</span> (serviceLoaders.size &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">        <span class="string">s"Multiple external cluster managers registered for the url <span class="subst">$url</span>: <span class="subst">$serviceLoaders</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    serviceLoaders.headOption</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>配置是在<br><img src="/2019/01/04/sparksql详解/images/ExternalClusterManager.png" alt=""><br>spark sql数据源的接入，新增数据源插入的时候可以采用这种方式，要实现的接口是DataSourceRegister。<br>简单测试<br>首先实现一个接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="comment">//可以制定实现类名加载</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后将接口配置在resources/META-INF/services/<br>bigdata.spark.services.DoSomething文件<br>内容：<br>实现该接口<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SayHello</span> <span class="keyword">implements</span> <span class="title">DoSomething</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> String <span class="title">shortName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="string">"SayHello"</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSomeThing</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(<span class="string">"hello !!!"</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>测试</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.services;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceLoader;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line">   <span class="keyword">static</span> ServiceLoader&lt;DoSomething&gt; loader = ServiceLoader.load(DoSomething.class);</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">       <span class="keyword">for</span>(DoSomething sayhello : loader)&#123;</span><br><span class="line">        <span class="comment">//要加载的类名称我们可以制定</span></span><br><span class="line">           <span class="keyword">if</span>(sayhello.shortName().equalsIgnoreCase(<span class="string">"SayHello"</span>))&#123;</span><br><span class="line">               sayhello.doSomeThing();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个主要是为讲自定义数据源作准备。    </p><p><a href="https://articles.zsxq.com/id_702s32f46zet.html" target="_blank" rel="noopener">https://articles.zsxq.com/id_702s32f46zet.html</a><br>首先要搞明白spark是如何支持多数据源的，昨天说了是通过serverloader加载的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Given a provider name, look up the data source class definition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lookupDataSource</span></span>(provider: <span class="type">String</span>): <span class="type">Class</span>[_] = &#123;</span><br><span class="line">    <span class="keyword">val</span> provider1 = backwardCompatibilityMap.getOrElse(provider, provider)</span><br><span class="line">    <span class="comment">// 指定路径加载，默认加载类名是DefaultSource</span></span><br><span class="line">    <span class="keyword">val</span> provider2 = <span class="string">s"<span class="subst">$provider1</span>.DefaultSource"</span></span><br><span class="line">    <span class="keyword">val</span> loader = <span class="type">Utils</span>.getContextOrSparkClassLoader</span><br><span class="line">    <span class="comment">// ServiceLoader加载</span></span><br><span class="line">    <span class="keyword">val</span> serviceLoader = <span class="type">ServiceLoader</span>.load(classOf[<span class="type">DataSourceRegister</span>], loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// the provider format did not match any given registered aliases</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Try</span>(loader.loadClass(provider1)).orElse(<span class="type">Try</span>(loader.loadClass(provider2))) <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Success</span>(dataSource) =&gt;</span><br><span class="line">                <span class="comment">// Found the data source using fully qualified path</span></span><br><span class="line">                dataSource</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Failure</span>(error) =&gt;</span><br><span class="line">                <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"orc"</span> ||</span><br><span class="line">                  provider1.startsWith(<span class="string">"org.apache.spark.sql.hive.orc"</span>)) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">"The ORC data source must be used with Hive support enabled"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (provider1.toLowerCase == <span class="string">"avro"</span> ||</span><br><span class="line">                  provider1 == <span class="string">"com.databricks.spark.avro"</span>) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$&#123;provider1.toLowerCase&#125;</span>. Please find an Avro "</span> +</span><br><span class="line">                      <span class="string">"package at http://spark.apache.org/third-party-projects.html"</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(</span><br><span class="line">                    <span class="string">s"Failed to find data source: <span class="subst">$provider1</span>. Please find packages at "</span> +</span><br><span class="line">                      <span class="string">"http://spark.apache.org/third-party-projects.html"</span>,</span><br><span class="line">                    error)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt; <span class="comment">// This one won't be caught by Scala NonFatal</span></span><br><span class="line">              <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">              <span class="keyword">val</span> className = e.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">              <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"<span class="subst">$className</span> was removed in Spark 2.0. "</span> +</span><br><span class="line">                  <span class="string">"Please check if your library is compatible with Spark 2.0"</span>, e)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> e</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> head :: <span class="type">Nil</span> =&gt;</span><br><span class="line">          <span class="comment">// there is exactly one registered alias</span></span><br><span class="line">          head.getClass</span><br><span class="line">        <span class="keyword">case</span> sources =&gt;</span><br><span class="line">          <span class="comment">// There are multiple registered aliases for the input</span></span><br><span class="line">          sys.error(<span class="string">s"Multiple sources found for <span class="subst">$provider1</span> "</span> +</span><br><span class="line">            <span class="string">s"(<span class="subst">$&#123;sources.map(_.getClass.getName).mkString(", ")&#125;</span>), "</span> +</span><br><span class="line">            <span class="string">"please specify the fully qualified class name."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ServiceConfigurationError</span> <span class="keyword">if</span> e.getCause.isInstanceOf[<span class="type">NoClassDefFoundError</span>] =&gt;</span><br><span class="line">        <span class="comment">// NoClassDefFoundError's class name uses "/" rather than "." for packages</span></span><br><span class="line">        <span class="keyword">val</span> className = e.getCause.getMessage.replaceAll(<span class="string">"/"</span>, <span class="string">"."</span>)</span><br><span class="line">        <span class="keyword">if</span> (spark2RemovedClasses.contains(className)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ClassNotFoundException</span>(<span class="string">s"Detected an incompatible DataSourceRegister. "</span> +</span><br><span class="line">            <span class="string">"Please remove the incompatible library from classpath or upgrade it. "</span> +</span><br><span class="line">            <span class="string">s"Error: <span class="subst">$&#123;e.getMessage&#125;</span>"</span>, e)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">throw</span> e</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p><p>其实，从这个点，你可以思考一下，自己能学到多少东西：类加载，可扩展的编程思路。</p><p>主要思路是：</p><ol><li><p>实现DefaultSource。</p></li><li><p>实现工厂类。</p></li><li><p>实现具体的数据加载类。</p></li></ol><p>首先，主要是有三个实现吧，需要反射加载的类DefaultSource，这个名字很固定的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.&#123;<span class="type">DataSourceOptions</span>, <span class="type">DataSourceV2</span>, <span class="type">ReadSupport</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span>  <span class="keyword">extends</span> <span class="title">DataSourceV2</span> <span class="keyword">with</span> <span class="title">ReadSupport</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(options: <span class="type">DataSourceOptions</span>) = <span class="keyword">new</span> <span class="type">SimpleDataSourceReader</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后是，要实现DataSourceReader，负责创建阅读器工厂：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReaderFactory</span>, <span class="type">DataSourceReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span></span>() = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> factoryList = <span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">DataReaderFactory</span>[<span class="type">Row</span>]]</span><br><span class="line">    factoryList.add(<span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>())</span><br><span class="line">    factoryList</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>数据源的具体实现类：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataReader</span>, <span class="type">DataReaderFactory</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleDataSourceReaderFactory</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">  <span class="title">DataReaderFactory</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReader</span> </span>= <span class="keyword">new</span> <span class="type">SimpleDataSourceReaderFactory</span>()</span><br><span class="line">  <span class="keyword">val</span> values = <span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span> </span>= index &lt; values.length</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> row = <span class="type">Row</span>(values(index))</span><br><span class="line">    index = index + <span class="number">1</span></span><br><span class="line">    row</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() = <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用我们默认的数据源：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bigdata.spark.<span class="type">SparkSQL</span>.<span class="type">DataSources</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getName).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">      .set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"mt-mdh.local"</span>)</span><br><span class="line">      .set(<span class="string">"spark.executor.instances"</span>,<span class="string">"2"</span>)</span><br><span class="line">      .setJars(<span class="type">List</span>(<span class="string">"/opt/sparkjar/bigdata.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka-clients-0.10.2.2.jar"</span></span><br><span class="line">        ,<span class="string">"/opt/jars/kafka_2.11-0.10.2.2.jar"</span>))</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> simpleDf = spark.read</span><br><span class="line">      .format(<span class="string">"bigdata.spark.SparkSQL.DataSources"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    simpleDf.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>format里面指定的是包路径，然后加载的时候会加上默认类名：DefaultSource。</p><h1 id="删除多个字段"><a href="#删除多个字段" class="headerlink" title="删除多个字段"></a>删除多个字段</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropColumns</span></span>(columns: <span class="type">Seq</span>[<span class="type">String</span>]):<span class="type">DataFAME</span>=&#123;</span><br><span class="line">  columns.foldLeft(dataFame)((df,column)=&gt; df.drop(column))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="spark-sql-窗口函数"><a href="#spark-sql-窗口函数" class="headerlink" title="spark sql 窗口函数"></a>spark sql 窗口函数</h1><p><a href="https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/A5CiLWPdg1nkjuNImetaAw</a><br>最近理了下spark sql 中窗口函数的知识，打算开几篇文章讲讲，窗口函数有很多应用场景，比如说炒股的时候有个5日移动均线，或者让你对一个公司所有销售所有部门按照销售业绩进行排名等等，都是要用到窗口函数，在spark sql中，窗口函数和聚合函数的区别，之前文章中也提到过，就是聚合函数是按照你聚合的维度，每个分组中算出来一个聚合值，而窗口函数是对每一行，都根据当前窗口（5日均线就是今日往前的5天组成的窗口），都聚合出一个值。</p><p>1  开胃菜，spark sql 窗口函数的基本概念和使用姿势<br>2  spark sql 中窗口函数深入理解<br>3  不是UDF，也不是UDAF，教你自定义一个窗口函数（UDWF）<br>4  从 spark sql 源码层面理解窗口函数   </p><h2 id="什么是简单移动平均值"><a href="#什么是简单移动平均值" class="headerlink" title="什么是简单移动平均值"></a>什么是简单移动平均值</h2><p>简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。<br><strong>例子</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local"</span>)</span><br><span class="line">    .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">    (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">    (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">    .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 这个 window spec 中，数据根据用户(customer)来分去。</span></span><br><span class="line"><span class="comment">    * 每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行)</span></span><br><span class="line"><span class="comment">    * ，每一个滑动的窗口总用有3行</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  df.withColumn(<span class="string">"movinAvg"</span>, avg(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line">  spark.stop()</span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong><br>2~3~2<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------------------+</span><br><span class="line">|site|     date|user_cnt|          movinAvg|</span><br><span class="line">+----+---------+--------+------------------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|              <span class="number">70.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|              <span class="number">80.0</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|              <span class="number">95.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|              <span class="number">65.0</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|<span class="number">56.666666666666664</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|              <span class="number">50.0</span>|</span><br><span class="line">+----+---------+--------+------------------+</span><br></pre></td></tr></table></figure></p><h2 id="窗口函数和窗口特征定义"><a href="#窗口函数和窗口特征定义" class="headerlink" title="窗口函数和窗口特征定义"></a>窗口函数和窗口特征定义</h2><p>正如上述例子中，窗口函数主要包含两个部分：</p><p> 指定窗口特征（wSpec）：</p><p>“partitionyBY” 定义数据如何分组；在上面的例子中，是用户 site</p><p>“orderBy” 定义分组中的排序</p><p>“rowsBetween” 定义窗口的大小</p><p>指定窗口函数函数：</p><p>指定窗口函数函数，你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数.</p><h2 id="累计汇总"><a href="#累计汇总" class="headerlink" title="累计汇总"></a>累计汇总</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .master(<span class="string">"local"</span>)</span><br><span class="line">  .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">  (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">  (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>))</span><br><span class="line">  .toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line"><span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>).rowsBetween(<span class="type">Long</span>.<span class="type">MinValue</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">"cumsum"</span>, sum(<span class="string">"user_cnt"</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+------+</span><br><span class="line">|site|     date|user_cnt|cumsum|</span><br><span class="line">+----+---------+--------+------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">140</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">240</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">130</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">170</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">170</span>|</span><br><span class="line">+----+---------+--------+------+</span><br></pre></td></tr></table></figure></p><h2 id="前一行数据"><a href="#前一行数据" class="headerlink" title="前一行数据"></a>前一行数据</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行 */</span></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"preUserCnt"</span>, lag(df(<span class="string">"user_cnt"</span>),<span class="number">1</span>).over(wSpce)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----------+</span><br><span class="line">|site|     date|user_cnt|preUserCnt|</span><br><span class="line">+----+---------+--------+----------+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|        <span class="number">50</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|        <span class="number">90</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|      <span class="literal">null</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|        <span class="number">70</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|        <span class="number">60</span>|</span><br><span class="line">+----+---------+--------+----------+</span><br></pre></td></tr></table></figure></p><p>如果计算环比的时候，是不是特别有用啊？！</p><p>在介绍几个常用的行数：</p><p>first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到</p><p>lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件</p><h2 id="排名"><a href="#排名" class="headerlink" title="排名"></a>排名</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local"</span>)</span><br><span class="line">     .appName(<span class="string">"DateFrameFromJsonScala"</span>)</span><br><span class="line">     .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = <span class="type">List</span>(</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902025"</span>, <span class="number">50</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">90</span>),</span><br><span class="line">     (<span class="string">"站点1"</span>, <span class="string">"201902026"</span>, <span class="number">100</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902027"</span>, <span class="number">70</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902028"</span>, <span class="number">60</span>),</span><br><span class="line">     (<span class="string">"站点2"</span>, <span class="string">"201902029"</span>, <span class="number">40</span>)).toDF(<span class="string">"site"</span>, <span class="string">"date"</span>, <span class="string">"user_cnt"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line">   <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> wSpce = <span class="type">Window</span>.partitionBy(<span class="string">"site"</span>).orderBy(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">   df.withColumn(<span class="string">"rank"</span>, rank().over(wSpce)).show()</span><br></pre></td></tr></table></figure><p><strong>结果</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+----+---------+--------+----+</span><br><span class="line">|site|     date|user_cnt|rank|</span><br><span class="line">+----+---------+--------+----+</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902025</span>|      <span class="number">50</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|      <span class="number">90</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">1</span>|<span class="number">201902026</span>|     <span class="number">100</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902027</span>|      <span class="number">70</span>|   <span class="number">1</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902028</span>|      <span class="number">60</span>|   <span class="number">2</span>|</span><br><span class="line">| 站点<span class="number">2</span>|<span class="number">201902029</span>|      <span class="number">40</span>|   <span class="number">3</span>|</span><br><span class="line">+----+---------+--------+----+</span><br></pre></td></tr></table></figure></p><p>这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。</p><h2 id="自定义窗口函数"><a href="#自定义窗口函数" class="headerlink" title="自定义窗口函数"></a>自定义窗口函数</h2><p><a href="https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/SMNX5lVPb0DWRf27QCcjIQ</a><br><a href="https://github.com/zheniantoushipashi/spark-udwf-session" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session</a>  </p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在使用 spark sql 的时候，有时候默认提供的sql 函数可能满足不了需求，这时候可以自定义一些函数，可以自定义 UDF 或者UDAF。</p><p>UDF 在sql中只是简单的处理转换一些字段，类似默认的trim 函数把一个字符串类型的列的头尾空格去掉， UDAF函数不同于UDF，是在sql聚合语句中使用的函数，必须配合 GROUP BY 一同使用，类似默认提供的count，sum函数，但是还有一种自定义函数叫做 UDWF， 这种一般人就不知道了，这种叫做窗口自定义函数，不了解窗口函数的，可以参考 1 开胃菜，spark sql 窗口函数的基本概念和使用姿势 ，或者官方的介绍 <a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a>    </p><p>窗口函数是 SQL 中一类特别的函数。和聚合函数相似，<strong>窗口函数的输入也是多行记录</strong>。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口</p><p>这里怎么理解一个窗口呢，spark君在这里得好好的解释解释，一个窗口是怎么定义的，</p><p>窗口语句中，partition by用来指定分区的列，在同一个分区的行属于同一个窗口</p><p>order by用来指定窗口内的多行，如何排序</p><p>windowing_clause 用来指定开窗方式，在spark sql 中开窗方式有那么几种    </p><ul><li><p>一个分区中的所有行作为一个窗口:UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING（上下都没有边界)，这种情况下，spark sql 会把所有行作为一个输入，进行一次求值</p></li><li><p>Growing frame:UNBOUNDED PRECEDING AND ….（上无边界）, 这种就是不断的把当前行加入的窗口中，而不删除， 例子：.rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行，在数据迭代过程中，不断的把当前行加入的窗口中。</p></li><li><p>Shrinking frame：… AND UNBOUNDED FOLLOWING（下无边界）和Growing frame 相反，窗口不断的把迭代到的当前行从窗口中删除掉。</p></li><li><p>Moving frame：滑动的窗口，举例：.rowsBetween(-1, 1) 就是指窗口定义从 -1(当前行前一行)到 1(当前行后一行) ，每一个滑动的窗口总用有3行</p></li><li><p>Offset frame： 窗口中只有一条数据，就是偏移当前行一定距离的那一行，举例：lag(field, n): 就是取从当前行往前的n个行的字段值</p></li></ul><p>这里就针对窗口函数就介绍这么多，如果不懂请参考相关文档，加强理解，我们在平时使用 spark sql 的过程中，会发现有很多教你自定义 UDF 和 UDAF 的教程，却没有针对UDWF的教程，这是为啥呢，这是因为 UDF 和UDAF 都作为上层API暴露给用户了，使用scala很简单就可以写一个函数出来，但是UDWF没有对上层用户暴露，只能使用 Catalyst expressions. 也就是Catalyst框架底层的表达式语句才可以定义，如果没有对源码有很深入的研究，根本就搞不出来。spark 君在工作中写了一些UDWF的函数，但是都比较复杂，不太好单独抽出来作为一个简明的例子给大家讲解，挑一个网上的例子来进行说明，这个例子 spark君亲测可用。</p><h3 id="窗口函数的使用场景"><a href="#窗口函数的使用场景" class="headerlink" title="窗口函数的使用场景"></a>窗口函数的使用场景</h3><p>我们来举个实际例子来说明 窗口函数的使用场景，在网站的统计指标中，有一个概念叫做用户会话，什么叫做用户会话呢，我来说明一下，我们在网站服务端使用用户session来管理用户状态，过程如下</p><p>1) 服务端session是用户第一次访问应用时，服务器就会创建的对象，代表用户的一次会话过程，可以用来存放数据。服务器为每一个session都分配一个唯一的sessionid，以保证每个用户都有一个不同的session对象。</p><p>2）服务器在创建完session后，会把sessionid通过cookie返回给用户所在的浏览器，这样当用户第二次及以后向服务器发送请求的时候，就会通过cookie把sessionid传回给服务器，以便服务器能够根据sessionid找到与该用户对应的session对象。</p><p>3）session通常有失效时间的设定，比如1个小时。当失效时间到，服务器会销毁之前的session，并创建新的session返回给用户。但是只要用户在失效时间内，有发送新的请求给服务器，通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长1个小时。</p><p>也就是说如果用户在1个超过一个小时不产生用户事件，当前会话就结束了，如果后续再产生用户事件，就当做新的用户会话，我们现在就使用spark sql 来统计用户的会话数，首先我们先加一个列，作为当前列的session，然后再 distinct 这个列就得到用户的会话数了，所以关键是怎么加上这个newSession 这个列，这种场景就很适合使用窗口函数来做统计，因为判断当前是否是一个新会话的依据，需要依赖当前行的前一行的时间戳和当前行的时间戳的间隔来判断，下面的表格可以帮助你理解这个概念，例子中有3列数据，用户，event字段代表用户访问了一个页面产生了一个用户事件，time字段代表访问页面的时间戳：  </p><table><thead><tr><th>user</th><th>event</th><th>time</th><th>session </th></tr></thead><tbody><tr><td>user1</td><td>page1</td><td>10:12</td><td>session1(new session)</td><td></td></tr><tr><td>user1</td><td>page2</td><td>10:20</td><td>session1(same session,8 minutes from last event)</td><td></td></tr><tr><td>user1</td><td>page1</td><td>11:13</td><td>session1(same session,53 minutes from last event)</td><td></td></tr><tr><td>user1</td><td>page3</td><td>14:12</td><td>session1(new session,3 minutes from last event)</td><td></td></tr></tbody></table><p>上面只有一个用户，如果多个用户，可以使用 partition by 来进行分区。</p><h3 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h3><p><strong>构造数据</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActivityData</span>(<span class="params">user:<span class="type">String</span>, ts:<span class="type">Long</span>, session:<span class="type">String</span></span>)   </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">st</span> </span>= <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">  <span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">    <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="string">"a CustomWindowFunction"</span> should <span class="string">"correctly create a session "</span> in &#123;</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.createDataFrame(sc.parallelize(d))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">    <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br></pre></td></tr></table></figure></p><p>怎么使用 spark sql 来统计会话数目呢，因为不同用户产生的是不同的会话，首先使用user字段进行分区，然后按照时间戳进行排序，然后我们需要一个自定义函数来加一个列，这个列的值的逻辑如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IF (no previous event) <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> (<span class="keyword">if</span> cerrent <span class="keyword">event</span> was past <span class="keyword">session</span> window)</span><br><span class="line"><span class="keyword">THEN</span> <span class="keyword">create</span> <span class="keyword">new</span> <span class="keyword">session</span></span><br><span class="line"><span class="keyword">ELSE</span> <span class="keyword">use</span> <span class="keyword">current</span> <span class="keyword">session</span></span><br></pre></td></tr></table></figure></p><p>运行结果如下：<br><img src="/2019/01/04/sparksql详解/./images/UDWFResult.jpg" alt=""><br>我们使用 UUID 来作为会话id， 当后一行的时间戳和前一行的时间戳间隔大于1小时的时候，就创建一个新的会话id作为列值，否则使用老的会话id作为列值。</p><p>这种就涉及到状态，我们在内部需要维护的状态数据</p><ul><li><p>当前的session ID</p></li><li><p>当前session的最后活动事件的时间戳</p></li></ul><p>下面我们就看看这个怎样自定义 caculateSession 这个窗口函数，先自定义一个静态对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Column</span>, <span class="type">Row</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">Add</span>, <span class="type">AggregateWindowFunction</span>, <span class="type">AttributeReference</span>, <span class="type">Expression</span>, <span class="type">If</span>, <span class="type">IsNotNull</span>, <span class="type">LessThanOrEqual</span>, <span class="type">Literal</span>, <span class="type">ScalaUDF</span>, <span class="type">Subtract</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.unsafe.types.<span class="type">UTF8String</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyUDWF</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> defaultMaxSessionLengthms = <span class="number">3600</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SessionUDWF</span>(<span class="params">timestamp:<span class="type">Expression</span>, session:<span class="type">Expression</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                         sessionWindow:<span class="type">Expression</span> = <span class="type">Literal</span>(defaultMaxSessionLengthms</span>)) <span class="keyword">extends</span> <span class="title">AggregateWindowFunction</span> </span>&#123;</span><br><span class="line">    self: <span class="type">Product</span> =&gt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">children</span></span>: <span class="type">Seq</span>[<span class="type">Expression</span>] = <span class="type">Seq</span>(timestamp, session)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> zero = <span class="type">Literal</span>( <span class="number">0</span>L )</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> nullString = <span class="type">Literal</span>(<span class="literal">null</span>:<span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> curentSession = <span class="type">AttributeReference</span>(<span class="string">"currentSession"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>)()</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> previousTs =    <span class="type">AttributeReference</span>(<span class="string">"lastTs"</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>)()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> aggBufferAttributes: <span class="type">Seq</span>[<span class="type">AttributeReference</span>] =  curentSession  :: previousTs :: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> assignSession =  <span class="type">If</span>(<span class="type">LessThanOrEqual</span>(<span class="type">Subtract</span>(timestamp, aggBufferAttributes(<span class="number">1</span>)), sessionWindow),</span><br><span class="line">      aggBufferAttributes(<span class="number">0</span>), <span class="comment">// if</span></span><br><span class="line">      <span class="type">ScalaUDF</span>( createNewSession, <span class="type">StringType</span>, children = <span class="type">Nil</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> initialValues: <span class="type">Seq</span>[<span class="type">Expression</span>] =  nullString :: zero :: <span class="type">Nil</span></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> updateExpressions: <span class="type">Seq</span>[<span class="type">Expression</span>] =</span><br><span class="line">      <span class="type">If</span>(<span class="type">IsNotNull</span>(session), session, assignSession) ::</span><br><span class="line">        timestamp ::</span><br><span class="line">        <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> evaluateExpression: <span class="type">Expression</span> = aggBufferAttributes(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">prettyName</span></span>: <span class="type">String</span> = <span class="string">"makeSession"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">val</span>  createNewSession = () =&gt; org.apache.spark.unsafe.types.<span class="type">UTF8String</span>.fromString(<span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, <span class="type">Literal</span>(defaultMaxSessionLengthms)) &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calculateSession</span></span>(ts:<span class="type">Column</span>,sess:<span class="type">Column</span>, sessionWindow:<span class="type">Column</span>): <span class="type">Column</span> = withExpr &#123; <span class="type">SessionUDWF</span>(ts.expr,sess.expr, sessionWindow.expr) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withExpr</span></span>(expr: <span class="type">Expression</span>): <span class="type">Column</span> = <span class="keyword">new</span> <span class="type">Column</span>(expr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码说明：</p><ul><li><p>状态保存在 Seq[AttributeReference]中</p></li><li><p>重写 initialValues方法进行初始化</p></li><li><p>updateExpressions 函数针对每一行数据都会调用</p></li></ul><blockquote><p>spark sql 在迭代处理每一行数据的时候，都会调用 updateExpressions 函数来处理，根据当后一行的时间戳和前一行的时间戳间隔大于1小时来进行不同的逻辑处理，如果不大于，就使用 aggBufferAttributes(0) 中保存的老的sessionid，如果大于，就把 createNewSession 包装为一个scalaUDF作为一个子表达式来创建一个新的sessionID，并且每次都把当前行的时间戳作为用户活动的最后时间戳。</p></blockquote><p>最后包装为静态对象的方法，就可以在spark sql中使用这个自定义窗口函数了，下面是两个重载的方法，一个最大间隔时间使用默认值，一个可以运行用户自定义，perfect</p><p>现在，我们就可以拿来用在我们的main函数中了。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> st = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> one_minute = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">Array</span>[<span class="type">UserActivityData</span>](</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st, <span class="string">"f237e656-1e53-4a24-9ad5-2b4576a4125d"</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +   <span class="number">5</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">10</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user2"</span>,  st +  <span class="number">15</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">140</span>*one_minute, <span class="literal">null</span>),</span><br><span class="line">  <span class="type">UserActivityData</span>(<span class="string">"user1"</span>,  st + <span class="number">160</span>*one_minute, <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = spark.createDataFrame(sc.parallelize(d))</span><br><span class="line">  <span class="keyword">val</span> specs = <span class="type">Window</span>.partitionBy(f.col(<span class="string">"user"</span>)).orderBy(f.col(<span class="string">"ts"</span>).asc)</span><br><span class="line">  <span class="keyword">val</span> res = df.withColumn( <span class="string">"newsession"</span>, <span class="type">MyUDWF</span>.calculateSession(f.col(<span class="string">"ts"</span>), f.col(<span class="string">"session"</span>)) over specs)</span><br><span class="line">  df.show(<span class="number">20</span>)</span><br><span class="line">  res.show(<span class="number">20</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></p><p>如果我们学会了自定义 spark 窗口函数，原理是就可以处理一切这种对前后数据依赖的统计需求，不过这种自定义毕竟需要对 spark 源码有很深入的研究才可以，这就需要功力了，希望 spark君的读者可以跟着spark君日益精进。文章中的代码可能不太清楚，完整demo参考  <a href="https://github.com/zheniantoushipashi/spark-udwf-session。" target="_blank" rel="noopener">https://github.com/zheniantoushipashi/spark-udwf-session。</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spark-SQL的Dataset基本操作&quot;&gt;&lt;a href=&quot;#Spark-SQL的Dataset基本操作&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL的Dataset基本操作&quot;&gt;&lt;/a&gt;Spark SQL的Dataset基本操作&lt;/
      
    
    </summary>
    
      <category term="sparksql" scheme="https://tgluon.github.io/categories/sparksql/"/>
    
    
      <category term="spark" scheme="https://tgluon.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>java自定义注解</title>
    <link href="https://tgluon.github.io/2018/08/27/%E6%B3%A8%E8%A7%A3%E8%AF%A6%E8%A7%A3/"/>
    <id>https://tgluon.github.io/2018/08/27/注解详解/</id>
    <published>2018-08-27T15:19:23.000Z</published>
    <updated>2018-08-27T15:23:23.148Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注解和注释区别"><a href="#注解和注释区别" class="headerlink" title="注解和注释区别"></a>注解和注释区别</h1><p><code>注释</code>：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程序代码。<br><code>注解</code>：属于Java代码，主要是在程序运行的时候，进行其他的参数配置的，主要是在程序运行过程中，通过反射技术获取参数。注解是给程序使用的。  </p><h1 id="注解的定义和使用模板"><a href="#注解的定义和使用模板" class="headerlink" title="注解的定义和使用模板"></a>注解的定义和使用模板</h1><p>注解：它是Java中的类。可以由开发者自己定义。编译之后，也会生成对应的class文件。</p><blockquote><p>定义格式：<br>修饰符  @interface 注解名{<br>}</p></blockquote><blockquote><p>使用格式：<br>    @注解名;<br>例如：junit测试中的 @Test<br>      复写方法中的@Override</p></blockquote><h1 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h1><h2 id="注解可以书写的位置"><a href="#注解可以书写的位置" class="headerlink" title="注解可以书写的位置"></a>注解可以书写的位置</h2><p>定义的注解，可以书写在类的不同位置：<br>类上、成员变量上、成员方法、构造方法等位置。<br>在自定义注解上使用@Target 声明自定义的注解可以出现的位置，具体的位置需要使用JDK中提供的类（ElementType）来指定。 </p><blockquote><p>ElementType中提供的静态的成员变量，这些变量表示注解可以存在的位置：<br> ElementType.CONSTRUCTOR:当前的注解可以书写在构造方法上<br> ElementType.METHOD:当前的注解可以书写在方法上<br> ElementType.FIELD:当前的注解可以书写在成员变量（字段）上<br> ElementType.TYPE:当前的注解可以书写在类或接口上   </p></blockquote><h2 id="注解生命周期"><a href="#注解生命周期" class="headerlink" title="注解生命周期"></a>注解生命周期</h2><blockquote><p>由@Retention 声明自己的注解可以存活的时间<br>具体的存活的时间需要通过RetentionPolicy 中提供的值。<br>RetentionPolicy.SOURCE：注解只能存在于源代码中，编译之后生成了class文件中就没有注解。<br>RetentionPolicy.RUNTIME：注解一直存在到程序运行过程中，可以通过反射获取注解信息。<br>RetentionPolicy.CLASS：注解在编译之后，可以被保存到class文件中，但是当JVM加载这个class文件的时候注解就被丢弃。<br> 一般我们自己定义注解，都是希望在程序运行过程中获取注解中的数据信息，因此我们需要将注解声明为 运行时的注解。</p></blockquote><h2 id="注解中的成员变量"><a href="#注解中的成员变量" class="headerlink" title="注解中的成员变量"></a>注解中的成员变量</h2><p>注解定义的变量格式：数据类型  变量名();</p><h2 id="基本案例"><a href="#基本案例" class="headerlink" title="基本案例"></a>基本案例</h2><h3 id="自定义注解-1"><a href="#自定义注解-1" class="headerlink" title="自定义注解"></a>自定义注解</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.annotation.ElementType;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Retention;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.RetentionPolicy;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Target;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Target</span>(&#123;ElementType.CONSTRUCTOR,ElementType.METHOD ,  ElementType.FIELD&#125;)</span><br><span class="line"><span class="keyword">public</span>  <span class="meta">@interface</span> MyAnnotation &#123;</span><br><span class="line">    <span class="comment">// 定义注解的变量</span></span><br><span class="line">    <span class="function">String <span class="title">name</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">age</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">sex</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义注解的使用"><a href="#自定义注解的使用" class="headerlink" title="自定义注解的使用"></a>自定义注解的使用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UseAnnotation</span> </span>&#123;</span><br><span class="line">    <span class="meta">@MyAnnotation</span>(name=<span class="string">"zhangsan"</span>,age=<span class="number">23</span>,sex=<span class="string">"nv"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UseAnnotation</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@MyAnnotation</span>(name=<span class="string">"zhangsan"</span>,age=<span class="number">23</span>,sex=<span class="string">"nv"</span>)</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="meta">@MyAnnotation</span>(name=<span class="string">"zhangsan"</span>,age=<span class="number">23</span>,sex=<span class="string">"nv"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="经典使用案例（mysql连接驱动）"><a href="#经典使用案例（mysql连接驱动）" class="headerlink" title="经典使用案例（mysql连接驱动）"></a>经典使用案例（mysql连接驱动）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.annotation.ElementType;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Retention;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.RetentionPolicy;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Target;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 自定义注解，封装数据库连接的四个参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Target</span>(ElementType.METHOD)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> MyDriver &#123;</span><br><span class="line"><span class="function">String <span class="title">driver</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">String <span class="title">url</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">String <span class="title">user</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">String <span class="title">pwd</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 专门负责获取数据库的连接工具类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JDBCUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@MyDriver</span>(driver=<span class="string">"com.mysql.jdbc.Driver"</span>,</span><br><span class="line">url=<span class="string">"jdbc:mysql://localhost:3306/estore"</span>,user=<span class="string">"root"</span>,pwd=<span class="string">"abc"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取当前类的class文件</span></span><br><span class="line">Class clazz = JDBCUtils.class;</span><br><span class="line">Method method = clazz.getMethod(<span class="string">"getConnection"</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="comment">//获取当前方法上的注解信息</span></span><br><span class="line"><span class="keyword">if</span>( method.isAnnotationPresent(MyDriver.class) )&#123;</span><br><span class="line"></span><br><span class="line">MyDriver an = method.getAnnotation(MyDriver.class);</span><br><span class="line">String driver = an.driver();</span><br><span class="line">String url = an.url();</span><br><span class="line">String user = an.user();</span><br><span class="line">String pwd = an.pwd();</span><br><span class="line"><span class="comment">//获取数据库的连接，加载驱动，获取连接</span></span><br><span class="line">Class.forName(driver);</span><br><span class="line"><span class="comment">//获取连接</span></span><br><span class="line">Connection conn = DriverManager.getConnection(url, user, pwd);</span><br><span class="line"><span class="keyword">return</span> conn;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Connection conn = JDBCUtils.getConnection();</span><br><span class="line">System.out.println(conn);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;注解和注释区别&quot;&gt;&lt;a href=&quot;#注解和注释区别&quot; class=&quot;headerlink&quot; title=&quot;注解和注释区别&quot;&gt;&lt;/a&gt;注解和注释区别&lt;/h1&gt;&lt;p&gt;&lt;code&gt;注释&lt;/code&gt;：对程序中的代码解释说明的信息。它主要是给开发者看的,帮助开发者阅读程
      
    
    </summary>
    
      <category term="java" scheme="https://tgluon.github.io/categories/java/"/>
    
    
      <category term="java" scheme="https://tgluon.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>maven删除本地仓库lastUpdated文件</title>
    <link href="https://tgluon.github.io/2018/08/13/maven%E5%88%A0%E9%99%A4lastupdated%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4/"/>
    <id>https://tgluon.github.io/2018/08/13/maven删除lastupdated文件命令/</id>
    <published>2018-08-13T15:43:23.000Z</published>
    <updated>2018-08-13T15:40:00.748Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">windows:</span><br><span class="line">for /r %i in (*.lastUpdated) do del %i</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linux:</span><br><span class="line">find ~/ . -name &quot;*.lastUpdated&quot; -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;
      
    
    </summary>
    
      <category term="maven" scheme="https://tgluon.github.io/categories/maven/"/>
    
    
      <category term="maven" scheme="https://tgluon.github.io/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>TEXTFILE格式存储到hive表</title>
    <link href="https://tgluon.github.io/2018/07/28/TEXTFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8/"/>
    <id>https://tgluon.github.io/2018/07/28/TEXTFILE格式存储到hive表/</id>
    <published>2018-07-27T23:43:23.000Z</published>
    <updated>2018-07-28T00:40:59.562Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向该表中装载以逗号、tab、空格作为分隔符的数据，也可以导入json格式文件。<br>TEXTFILE格式的输出包是：   </p><blockquote></blockquote><p>org.apache.hadoop.mapred.TextfileInputFormat<br>org.apache.hadoop.mapred.TextfileOutputFormat</p><p><code>注意:</code>本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><h2 id="建立TEXTFILE格式表"><a href="#建立TEXTFILE格式表" class="headerlink" title="建立TEXTFILE格式表"></a>建立TEXTFILE格式表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_textfile(   </span><br><span class="line">    c1 <span class="keyword">string</span>,  </span><br><span class="line">    c2 <span class="built_in">int</span>,   </span><br><span class="line">    c3 <span class="keyword">string</span>,   </span><br><span class="line">    c4 <span class="keyword">string</span>)  </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h2 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/textfile/a.txt'</span> <span class="keyword">into</span>/overwrite t_textfile;</span><br></pre></td></tr></table></figure><h2 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_textfile</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;TEXTFILE就是普通的文本型文件，是hadoop里面最常用的输入输出格式，也是hive默认文件格式，如果表定义为TEXFILE,则可以向
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>ORCFILE格式存储到hive表</title>
    <link href="https://tgluon.github.io/2018/07/28/ORCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8/"/>
    <id>https://tgluon.github.io/2018/07/28/ORCFILE格式存储到hive表/</id>
    <published>2018-07-27T23:43:23.000Z</published>
    <updated>2018-07-28T00:40:59.546Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减75%，从而提升数据处理速度。ORC比Text,Squence和RC文件格式有更好的性能，而且ORC是目前是hive唯一支持事物的文件格式。<br>ORCFILE格式的输出包是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.io.orc</span><br></pre></td></tr></table></figure></p><p><code>注意:</code>本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><h2 id="建立ORCFILE格式表"><a href="#建立ORCFILE格式表" class="headerlink" title="建立ORCFILE格式表"></a>建立ORCFILE格式表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_orcfile(   </span><br><span class="line">    c1 <span class="keyword">string</span>,  </span><br><span class="line">    c2 <span class="built_in">int</span>,   </span><br><span class="line">    c3 <span class="keyword">string</span>,   </span><br><span class="line">    c4 <span class="keyword">string</span>)  </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">as</span> orcfile;</span><br></pre></td></tr></table></figure><h2 id="向表中导入数据"><a href="#向表中导入数据" class="headerlink" title="向表中导入数据"></a>向表中导入数据</h2><p><code>注意</code>：不能直接向ORCFILE表插入数据，需要从其他表向ORCFILE表插入数据。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_orcfile  <span class="keyword">select</span> * <span class="keyword">from</span> t_textfile</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;ORC指的是Optimized Record Columnar,就是说相对于其他文件格式，它已更优化方式存储数据.ORC能将原始的大小缩减7
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>RCFILE格式存储到hive表</title>
    <link href="https://tgluon.github.io/2018/07/28/RCFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8/"/>
    <id>https://tgluon.github.io/2018/07/28/RCFILE格式存储到hive表/</id>
    <published>2018-07-27T23:43:23.000Z</published>
    <updated>2018-07-28T00:40:59.557Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，<strong>被用于在一个时间点操作多行的场景</strong>。RCFILEs是由二进制键/值对组成的平面文件，这点于SEQUENCEFILE非常相似。RCFILE以记录的形式存储表中的列，即列存储方式。它先分割行做水平分区，然后分割列做垂直分区。RCFILE把一行的元数据作为键，把行数据作为值，这种面向列的存储在执行数据分析时更高效。<br>RCFILE格式的输入输出包是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.io.RCFileInputFormat</span><br><span class="line">org.apache.hadoop.hive.ql.io.RCFileOutputFormat</span><br></pre></td></tr></table></figure></p><p><code>注意:</code>本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><h2 id="建立RCFILE格式表"><a href="#建立RCFILE格式表" class="headerlink" title="建立RCFILE格式表"></a>建立RCFILE格式表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_rcfile(   </span><br><span class="line">    c1 <span class="keyword">string</span>,  </span><br><span class="line">    c2 <span class="built_in">int</span>,   </span><br><span class="line">    c3 <span class="keyword">string</span>,   </span><br><span class="line">    c4 <span class="keyword">string</span>)  </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">as</span> rcfile;</span><br></pre></td></tr></table></figure><h2 id="向表中导入数据"><a href="#向表中导入数据" class="headerlink" title="向表中导入数据"></a>向表中导入数据</h2><p><code>注意</code>：不能直接向RCFILE表插入数据，需要从其他表向ORCFILE表插入数据。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_rcfile  <span class="keyword">select</span> * <span class="keyword">from</span> t_textfile</span><br></pre></td></tr></table></figure></p><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_rcfile</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;RCFILE指的是 Record Columnar File,是一种高效压缩率的二进制文件格式，&lt;strong&gt;被用于在一个时间点操作多行的
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>sequencefile方式存储到hive表</title>
    <link href="https://tgluon.github.io/2018/07/28/SEQUENCEFILE%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8%E5%88%B0hive%E8%A1%A8/"/>
    <id>https://tgluon.github.io/2018/07/28/SEQUENCEFILE格式存储到hive表/</id>
    <published>2018-07-27T17:43:23.000Z</published>
    <updated>2018-07-28T00:40:59.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为是小文件.元数据的增长将转化为Namenode的开销。如果有大量小文件,Namenode会成为瓶颈。为了解决这个问题，hadoop引入了sequence文件，将sequence作为存储小文件的容器。<br>Sequnce文件是有二进制键值对组成的平面文件。Hive将查询转换成MapReduce作业时，决定一个给定记录的哪些键/值对被使用。Sequence文件是可分割的二进制格式，主要的用途是联合多个小文件。<br>SEQUENCEFILE格式的输入输入包是：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">org.apache.hadoop.hive.ql.id.HiveSequenceFileOutputFormat</span><br></pre></td></tr></table></figure></p><p><code>注意:</code>本段文字来自《Hadoop构建数据仓库实践》书籍6.2.1章节</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><h2 id="建立Sequencefile格式表"><a href="#建立Sequencefile格式表" class="headerlink" title="建立Sequencefile格式表"></a>建立Sequencefile格式表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_sequencefile(   </span><br><span class="line">    c1 <span class="keyword">string</span>,  </span><br><span class="line">    c2 <span class="built_in">int</span>,   </span><br><span class="line">    c3 <span class="keyword">string</span>,   </span><br><span class="line">    c4 <span class="keyword">string</span>)  </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">as</span> sequencefile;</span><br></pre></td></tr></table></figure><h2 id="向表中导入数据"><a href="#向表中导入数据" class="headerlink" title="向表中导入数据"></a>向表中导入数据</h2><p><code>注意</code>：与TEXTFILE有些不同，应为SEQUENCEFILE是二进制格式，所以需要从其他表向SEQUENCEFILE表插入数据。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_sequencefile  <span class="keyword">select</span> * <span class="keyword">from</span> t_textfile</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;我们知道hadoop处理少量大文件比大量小文件的性能要好。如果文件小于hadoop定义的块尺寸(hadoop2.x默认128MB),可以认为
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>加载json文件到hive表</title>
    <link href="https://tgluon.github.io/2018/07/28/%E5%8A%A0%E8%BD%BDjson%E6%96%87%E4%BB%B6%E5%88%B0hive%E8%A1%A8/"/>
    <id>https://tgluon.github.io/2018/07/28/加载json文件到hive表/</id>
    <published>2018-07-27T16:43:23.000Z</published>
    <updated>2018-07-28T00:40:59.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="struct类型应用"><a href="#struct类型应用" class="headerlink" title="struct类型应用"></a>struct类型应用</h1><h2 id="准备json文件"><a href="#准备json文件" class="headerlink" title="准备json文件"></a>准备json文件</h2><p>simple.json<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"foo"</span>:<span class="string">"abc"</span>,<span class="attr">"bar"</span>:<span class="string">"200901011000000"</span>,<span class="attr">"quux"</span>:&#123;<span class="attr">"quuxid"</span>:<span class="number">1234</span>,<span class="attr">"quuxname"</span>:<span class="string">"sam"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p><h2 id="添加hive-hcatalog-core-jar包"><a href="#添加hive-hcatalog-core-jar包" class="headerlink" title="添加hive-hcatalog-core.jar包"></a>添加hive-hcatalog-core.jar包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar</span><br></pre></td></tr></table></figure><h2 id="建立测试表"><a href="#建立测试表" class="headerlink" title="建立测试表"></a>建立测试表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> mytest;</span><br><span class="line"><span class="keyword">use</span> mytest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>  my_table(</span><br><span class="line">   foo <span class="keyword">string</span>,</span><br><span class="line">   bar <span class="keyword">string</span>,</span><br><span class="line">   quux <span class="keyword">struct</span>&lt;quuxid:<span class="built_in">int</span>,quuxname:<span class="keyword">string</span>&gt;</span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hive.hcatlog.data.JsonSerde'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h2 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/simple.json'</span> <span class="keyword">into</span> <span class="keyword">table</span> my_table;</span><br></pre></td></tr></table></figure><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> foo, bar,quux.quuxid,quux.quuname <span class="keyword">from</span> my_table;</span><br></pre></td></tr></table></figure><h1 id="sstruct结合array类型应用"><a href="#sstruct结合array类型应用" class="headerlink" title="sstruct结合array类型应用"></a>sstruct结合array类型应用</h1><h2 id="准备json文件-1"><a href="#准备json文件-1" class="headerlink" title="准备json文件"></a>准备json文件</h2><p>complex.json<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;"docid":"abc","user":&#123;"id":123,"username":"saml1234","name":"sam","shippingaddress":&#123;"address1":"123mainst","address2:""","city":"durham","state":"nc"&#125;,"orders":[&#123;"itemid":6789,"orderdate":"11/11/2012"&#125;,&#123;"itemid":4352,"orderdate"："12/12/2012"&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure></p><h2 id="添加hive-hcatalog-core-jar包-1"><a href="#添加hive-hcatalog-core-jar包-1" class="headerlink" title="添加hive-hcatalog-core.jar包"></a>添加hive-hcatalog-core.jar包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar</span><br></pre></td></tr></table></figure><h2 id="建立测试表-1"><a href="#建立测试表-1" class="headerlink" title="建立测试表"></a>建立测试表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mytest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> complex_json(</span><br><span class="line"> docid <span class="keyword">string</span>,</span><br><span class="line"> <span class="keyword">user</span> <span class="keyword">struct</span>&lt;<span class="keyword">id</span>: <span class="built_in">int</span>,</span><br><span class="line">      username: <span class="keyword">string</span>,</span><br><span class="line">      shippingaddress:<span class="keyword">struct</span>&lt;address1:<span class="keyword">string</span></span><br><span class="line">           address2: <span class="keyword">string</span>,</span><br><span class="line">           city: <span class="keyword">string</span>,</span><br><span class="line">           state: <span class="keyword">string</span>&gt;,</span><br><span class="line">           orders:<span class="built_in">array</span>&lt;<span class="keyword">struct</span>&lt;itemid:<span class="built_in">int</span>,orderdate:<span class="keyword">String</span>&gt;&gt;&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hive.hcatlog.data.JsonSerde'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h2 id="装载数据-1"><a href="#装载数据-1" class="headerlink" title="装载数据"></a>装载数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/complex.json'</span> overwrite <span class="keyword">table</span> complex_json;</span><br></pre></td></tr></table></figure><h2 id="查询-1"><a href="#查询-1" class="headerlink" title="查询"></a>查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> docid,user.id,user.shippingaddress.city <span class="keyword">as</span> city ,user.orders[<span class="number">0</span>].itemid <span class="keyword">as</span> order0id,user.orders[<span class="number">1</span>].itemid <span class="keyword">as</span> order1ib    </span><br><span class="line"><span class="keyword">from</span> complex_json;</span><br></pre></td></tr></table></figure><h1 id="动态map类型应用"><a href="#动态map类型应用" class="headerlink" title="动态map类型应用"></a>动态map类型应用</h1><h2 id="json文件"><a href="#json文件" class="headerlink" title="json文件"></a>json文件</h2><p>a.json<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"conflict"</span>:&#123;<span class="attr">"liveid"</span>:<span class="number">123</span>,<span class="attr">"zhuboid"</span>:<span class="string">"456"</span>,<span class="attr">"media"</span>:<span class="number">789</span>,<span class="attr">"proxy"</span>:<span class="string">"ac"</span>,<span class="attr">"result"</span>:<span class="number">10000</span>&#125;&#125;</span><br><span class="line">&#123;<span class="attr">"conflict"</span>:&#123;<span class="attr">"liveid"</span>:<span class="number">123</span>,<span class="attr">"zhuboid"</span>:<span class="string">"456"</span>,<span class="attr">"media"</span>:<span class="number">789</span>,<span class="attr">"proxy"</span>:<span class="string">"ac"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="attr">"conflict"</span>:&#123;<span class="attr">"liveid"</span>:<span class="number">123</span>,<span class="attr">"zhuboid"</span>:<span class="string">"456"</span>,<span class="attr">"media"</span>:<span class="number">789</span>&#125;&#125;</span><br><span class="line">&#123;<span class="attr">"conflict"</span>:&#123;<span class="attr">"liveid"</span>:<span class="number">123</span>,<span class="attr">"zhuboid"</span>:<span class="string">"456"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="attr">"conflict"</span>:&#123;<span class="attr">"liveid"</span>:<span class="number">123</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p><h2 id="添加hive-hcatalog-core-jar包-2"><a href="#添加hive-hcatalog-core-jar包-2" class="headerlink" title="添加hive-hcatalog-core.jar包"></a>添加hive-hcatalog-core.jar包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/hadoop/hive/lib/hive-hcatalog-core.jar</span><br></pre></td></tr></table></figure><h2 id="建立测试表-2"><a href="#建立测试表-2" class="headerlink" title="建立测试表"></a>建立测试表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mytest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> json_table(</span><br><span class="line">    conflict <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hive.hcatlog.data.JsonSerde'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h2 id="查询-2"><a href="#查询-2" class="headerlink" title="查询"></a>查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> json_table;   </span><br><span class="line"><span class="keyword">select</span> conflict[<span class="string">'media'</span>] <span class="keyword">from</span> json_table;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;struct类型应用&quot;&gt;&lt;a href=&quot;#struct类型应用&quot; class=&quot;headerlink&quot; title=&quot;struct类型应用&quot;&gt;&lt;/a&gt;struct类型应用&lt;/h1&gt;&lt;h2 id=&quot;准备json文件&quot;&gt;&lt;a href=&quot;#准备json文件&quot; cla
      
    
    </summary>
    
      <category term="hive" scheme="https://tgluon.github.io/categories/hive/"/>
    
    
      <category term="hive" scheme="https://tgluon.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台监控工具</title>
    <link href="https://tgluon.github.io/2018/07/24/%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7/"/>
    <id>https://tgluon.github.io/2018/07/24/监控工具/</id>
    <published>2018-07-24T10:19:59.000Z</published>
    <updated>2018-07-24T10:39:00.771Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据平台监控工具"><a href="#大数据平台监控工具" class="headerlink" title="大数据平台监控工具"></a>大数据平台监控工具</h1><h2 id="Prometheus-grafana"><a href="#Prometheus-grafana" class="headerlink" title="Prometheus+grafana"></a>Prometheus+grafana</h2><p><code>环境搭建参考博客：</code></p><blockquote><p><a href="http://blog.51cto.com/youerning/2050543" target="_blank" rel="noopener">http://blog.51cto.com/youerning/2050543</a>   </p></blockquote><h2 id="cerebro"><a href="#cerebro" class="headerlink" title="cerebro"></a>cerebro</h2><p><code>作为本地监控，监控elasticsearch</code>     </p><blockquote><p><a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">https://github.com/lmenezes/cerebro</a></p></blockquote><h2 id="openTSDB-grafana"><a href="#openTSDB-grafana" class="headerlink" title="openTSDB+grafana"></a>openTSDB+grafana</h2><p><code>参考</code></p><blockquote><p><a href="https://blog.csdn.net/u011537073/article/details/54565742" target="_blank" rel="noopener">https://blog.csdn.net/u011537073/article/details/54565742</a></p></blockquote><h2 id="tableau"><a href="#tableau" class="headerlink" title="tableau"></a>tableau</h2><p><code>付费</code> </p><blockquote><p><a href="https://www.tableau.com/support/help" target="_blank" rel="noopener">https://www.tableau.com/support/help</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;大数据平台监控工具&quot;&gt;&lt;a href=&quot;#大数据平台监控工具&quot; class=&quot;headerlink&quot; title=&quot;大数据平台监控工具&quot;&gt;&lt;/a&gt;大数据平台监控工具&lt;/h1&gt;&lt;h2 id=&quot;Prometheus-grafana&quot;&gt;&lt;a href=&quot;#Prometh
      
    
    </summary>
    
      <category term="大数据平台监控" scheme="https://tgluon.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="大数据平台监控" scheme="https://tgluon.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>环境变量配置</title>
    <link href="https://tgluon.github.io/2018/07/22/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/"/>
    <id>https://tgluon.github.io/2018/07/22/环境变量配置/</id>
    <published>2018-07-22T15:10:10.000Z</published>
    <updated>2018-07-24T10:18:23.016Z</updated>
    
    <content type="html"><![CDATA[<h1 id="java环境变量配置"><a href="#java环境变量配置" class="headerlink" title="java环境变量配置"></a>java环境变量配置</h1><h2 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h2><h3 id="方式一-常用"><a href="#方式一-常用" class="headerlink" title="方式一(常用):"></a>方式一(常用):</h3><p>变量名：JAVA_HOME<br>变量值：D:\Program Files\Java\jdk1.8.0_73<br>变量名：PATH<br>变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin<br>变量名：classpath<br>变量值：.,%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;  %JAVA_HOME%\lib\tools.jar  </p><h3 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h3><p>path:<br>D:\Program Files\Java\jdk1.8.0_60\bin<br>classpath:<br>D:\Program Files\Java\jdk1.8.0_60\lib<br>javac</p><h2 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h2><p>JAVA_HOME=/home/hadoop/java/jdk1.8.0_73<br>CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>PATH=$PATH:$JAVA_HOME/bin<br>export JAVA_HOME CLASSPATH   </p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>在命令行输入一下命令进行验证：<br>1.java<br>2.javac   </p><h2 id="查看版本号"><a href="#查看版本号" class="headerlink" title="查看版本号"></a>查看版本号</h2><h1 id="maven环境变量配置"><a href="#maven环境变量配置" class="headerlink" title="maven环境变量配置"></a>maven环境变量配置</h1><h2 id="windows-1"><a href="#windows-1" class="headerlink" title="windows"></a>windows</h2><p><code>MAVEM_HOME</code><br>D:\Program Files\maven<br><code>path</code><br>%MAVEN_HOME%\bin<br>注意：%MAVEN_HOME%\bin 应该放在path的最前面。   </p><h2 id="linux-1"><a href="#linux-1" class="headerlink" title="linux"></a>linux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAVEN_HOME=/home/hadoop/maven     </span><br><span class="line">PATH=$PATH:$MAVEN_HOME/bin</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;java环境变量配置&quot;&gt;&lt;a href=&quot;#java环境变量配置&quot; class=&quot;headerlink&quot; title=&quot;java环境变量配置&quot;&gt;&lt;/a&gt;java环境变量配置&lt;/h1&gt;&lt;h2 id=&quot;windows&quot;&gt;&lt;a href=&quot;#windows&quot; class
      
    
    </summary>
    
      <category term="环境变量配置" scheme="https://tgluon.github.io/categories/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="java" scheme="https://tgluon.github.io/tags/java/"/>
    
  </entry>
  
</feed>
